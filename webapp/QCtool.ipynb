{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import re\n",
    "import traceback\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "pn.extension(\"plotly\")\n",
    "import io\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "from bokeh.models.widgets.tables import NumberFormatter\n",
    "import plotly.express as px\n",
    "import json\n",
    "import os\n",
    "from importlib import reload\n",
    "import pkg_resources\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import domaps.gui as gui\n",
    "\n",
    "try:\n",
    "    type(domaps)\n",
    "    print(\"reloading library\")\n",
    "    domaps = reload(domaps)\n",
    "except Exception as ex:\n",
    "    print(\"loading library first time\")\n",
    "    import domaps\n",
    "    \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## panel and plotly settings and customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".detail_menu .bk-headers-wrapper{\n",
    "  border-bottom: 2px solid #0a5da8 !important;\n",
    "  margin-bottom: 10px;\n",
    "  min-width: 1300px;\n",
    "}\n",
    ".detail_menu .bk-tab{\n",
    "  color: #0a5da8;\n",
    "}\n",
    ".detail_menu .bk-active{\n",
    "  border-color: #0a5da8 !important;\n",
    "  border-width: 2px 1px 0px 1px !important;\n",
    "  color: #111111 !important;\n",
    "}\n",
    "\n",
    ".main_menu .bk-headers-wrapper{\n",
    "  border-bottom: 2px solid #0a5da8 !important;\n",
    "  margin-bottom: 10px;\n",
    "  min-width: 1300px;\n",
    "}\n",
    ".main_menu .bk-tab{\n",
    "  color: #0a5da8;\n",
    "  font-size: 120%;\n",
    "  font-weight: bold;\n",
    "}\n",
    ".main_menu .bk-active{\n",
    "  border-color: #0a5da8 !important;\n",
    "  border-width: 2px 1px 0px 1px !important;\n",
    "  color: #111111 !important;\n",
    "}\n",
    ".content-width{\n",
    "  min-width: 1300px;\n",
    "}\n",
    "\n",
    ".bk-tabs-header{\n",
    "  min-width: 1300px;\n",
    "}\n",
    "\n",
    ".card-title:first-child{\n",
    "  font-size: 13px;\n",
    "}\n",
    "\n",
    ".button-main .bk-btn{\n",
    "  font-size: 120%;\n",
    "  font-weight: bold;\n",
    "}\n",
    "\"\"\"\n",
    "pn.extension(raw_css=[css])\n",
    "\n",
    "plotly_config={\n",
    "      'toImageButtonOptions': {\n",
    "            'format': 'svg', # one of png, svg, jpeg, webp\n",
    "            'filename': 'figure'\n",
    "      }\n",
    "}\n",
    "\n",
    "def resize(el):\n",
    "    try:\n",
    "        el.append(None)\n",
    "        el.pop(-1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data that will be stored in memory during each session\n",
    "## dataset analysed and displayed in the single analysis tab\n",
    "mem_single_analysis = None\n",
    "i_class = None\n",
    "## dataset collection analysed and displayed in the benchmarking tab\n",
    "mem_benchmark = None\n",
    "i_class_comp = None\n",
    "## currently available datasets to select for benchmarking\n",
    "mem_available_datasets = dict()\n",
    "\n",
    "with open(\"textfragments.json\", \"r\") as file:\n",
    "    textfragments = json.load(file)\n",
    "\n",
    "DEBUG = True\n",
    "MAX_SIZE_MB = 80\n",
    "CONTENT_WIDTH = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Served panel object:\n",
    "app = pn.GridSpec()#sizing_mode=\"stretch_both\", margin=0)\n",
    "app[0,0] = pn.Spacer(background=\"white\", margin=0) #\"#DDDDDD\"\n",
    "app[0,9] = pn.Spacer(background=\"white\", margin=0) #\"#DDDDDD\"\n",
    "\n",
    "#### Insert main content container\n",
    "app_center = pn.Column(pn.Row(pn.Pane(\"# QC tool for Spatial Proteomics\", width = 600),\n",
    "                              pn.layout.HSpacer(),\n",
    "                              margin=10),\n",
    "                       pn.Row(name=\"main_content\"),\n",
    "                       pn.Spacer(background=\"#DDDDDD\", height=100, margin=0)\n",
    "                      )\n",
    "app[0,1:8] = app_center\n",
    "\n",
    "#### Insert main menu tab object\n",
    "app_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\", \"main_menu\"], dynamic=False)\n",
    "app_center.objects[[i.name for i in app_center].index(\"main_content\")] = app_tabs\n",
    "\n",
    "#### Append individual dashboards\n",
    "## Home\n",
    "dashboard_home = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"home\", css_classes=[\"content-width\"])\n",
    "app_tabs.append((\"Home\", dashboard_home))\n",
    "\n",
    "## Single analysis\n",
    "dashboard_analysis = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"analysis\", css_classes=[\"content-width\"])\n",
    "app_tabs.append((\"Analysis\", dashboard_analysis))\n",
    "analysis_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\", \"detail_menu\"], dynamic=False)\n",
    "\n",
    "## Benchmark\n",
    "dashboard_benchmark = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"benchmark\", css_classes=[\"content-width\"])\n",
    "app_tabs.append((\"Benchmark\", dashboard_benchmark))\n",
    "comparison_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\", \"detail_menu\"], dynamic=False)\n",
    "\n",
    "## Manage datasets\n",
    "#dashboard_MissclassificationMatrix = pn.Column(\n",
    "#    \"Please, upload a file first and press 'Analyse clusters'\",\n",
    "#    name=\"SVM Analysis\", css_classes=[\"content-width\"])\n",
    "#dashboard_amendment = pn.Column(\n",
    "#    \"Please, upload a json file first\",\n",
    "#    name=\"Renaming\", css_classes=[\"content-width\"])\n",
    "dashboard_manageDatasets = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"Manage datasets\", css_classes=[\"content-width\"])\n",
    "#app_tabs.append((\"Manage Datasets\", dashboard_manageDatasets))\n",
    "\n",
    "#amendment_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\", \"detail_menu\"], dynamic=True)\n",
    "#amendment_tabs.append((\"Change Experiment name and comment\", dashboard_amendment))\n",
    "#amendment_tabs.append((\"SVM Upload\", dashboard_MissclassificationMatrix))\n",
    "\n",
    "## About\n",
    "app_tabs.append((\"About\", pn.Row(pn.Pane(textfragments[\"about_intro\"], width=1000))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App serving\n",
    "Switch cells below between markup and code to set up for server hosting from the command line (app.servable) vs. local hosting from python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    server.stop()\n",
    "except Exception:\n",
    "    print(\"First server startup\")\n",
    "server = app.show(port=5067, websocket_max_message_size=MAX_SIZE_MB*1024*1024, admin=True,\n",
    "                  http_server_kwargs={'max_buffer_size': MAX_SIZE_MB*1024*1024})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell structuring\n",
    "All cells below contain one or several sets of these points:\n",
    "- (Dashboard structure)\n",
    "- Layout and widget elements (outside in)\n",
    "- Layout assembly and appending\n",
    "- Callback definitions\n",
    "- Positioning of callback outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "########################\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# list\n",
    "# of\n",
    "# callbacks\n",
    "\n",
    "#### Callback output positioning\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "########################\n",
    "# already defined as single column\n",
    "dashboard_home.objects = []\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "lo_home_intro = pn.Pane(textfragments[\"home_intro\"], width=CONTENT_WIDTH)\n",
    "btn_home_analysesingle = pn.widgets.Button(name=\"Format and analyse single experiment\",\n",
    "                                           button_type=\"success\", width=400, height=50,\n",
    "                                           css_classes=[\"button-main\"])\n",
    "lo_home_singleinstructions = pn.Column(\n",
    "    pn.Pane(textfragments[\"home_single_shortinstructions\"], width=CONTENT_WIDTH),\n",
    "    pn.Card(textfragments[\"quick_start_guide\"], header=\"DOM-QC 1-min Quick Start Guide\", width=CONTENT_WIDTH,\n",
    "            name=\"tutorial_single\", collapsed=True)\n",
    ")\n",
    "btn_home_benchmark = pn.widgets.Button(name=\"Benchmark multiple experiments\",\n",
    "                                       button_type=\"success\", width=400, height=50,\n",
    "                                       css_classes=[\"button-main\"])\n",
    "lo_home_benchmarkinstructions = pn.Column(\n",
    "    pn.Pane(textfragments[\"home_benchmark_shortinstructions\"], width=CONTENT_WIDTH),\n",
    "#    pn.Card(\"Add screenshot tutorial here.\", header=\"Tutorial\", width=CONTENT_WIDTH,\n",
    "#            name=\"tutorial_benchmark\", collapsed=True)\n",
    ")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "for el in [lo_home_intro,\n",
    "           btn_home_analysesingle, lo_home_singleinstructions,\n",
    "           btn_home_benchmark, lo_home_benchmarkinstructions]:\n",
    "    dashboard_home.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# home_gotosingleanalysis\n",
    "# home_gotobenchmark\n",
    "\n",
    "def home_gotosingleanalysis(event):\n",
    "    app_tabs.active = [el.name for el in app_tabs].index(\"analysis\")\n",
    "btn_home_analysesingle.on_click(home_gotosingleanalysis)\n",
    "\n",
    "def home_gotobenchmark(event):\n",
    "    app_tabs.active = [el.name for el in app_tabs].index(\"benchmark\")\n",
    "btn_home_benchmark.on_click(home_gotobenchmark)\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis tab\n",
    "- File config\n",
    "- Analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "########################\n",
    "dashboard_analysis.objects = [\n",
    "    pn.Row(name=\"file_config\"),\n",
    "    pn.Column(name=\"analysis_output\")\n",
    "]\n",
    "\n",
    "#### Layout elements\n",
    "#### File config\n",
    "####################\n",
    "#lo_read_file = pn.Card(header=\"### Upload configuration\", min_width=400)\n",
    "lo_config_instructions = pn.Card(\n",
    "    pn.Pane(textfragments[\"upload_instructions\"]), header=\"### Instructions\", width=400)\n",
    "lo_config_details = pn.Card(\n",
    "    pn.Pane(textfragments[\"upload_details\"]), header=\"### Details on configuring your data\", width=400, collapsed = True)\n",
    "lo_config_error_messages = pn.Card(\n",
    "    pn.Pane(textfragments[\"upload_error_messages\"]), header=\"### Common error messages\", width=400, collapsed = True)\n",
    "#\n",
    "#i_file = pn.widgets.FileInput(name=\"Upload file\")\n",
    "#lo_read_file.append(i_file)\n",
    "loading_status = pn.Row()\n",
    "idle = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "\n",
    "analysis_status = pn.Pane(\"\", width=300)\n",
    "filereading_status = pn.Pane(\"No data import yet\", width=300)\n",
    "\n",
    "\n",
    "i_FileConfig = gui.ConfigureSingleFile(width=540)\n",
    "#### Append layout to dashboard\n",
    "#### File config\n",
    "###############################\n",
    "dashboard_analysis.objects[[el.name for el in dashboard_analysis].index(\"file_config\")].objects = []\n",
    "for el in [\n",
    "    pn.Column(i_FileConfig, analysis_status, loading_status, width=600),\n",
    "    pn.Column(lo_config_instructions, lo_config_details, lo_config_error_messages)\n",
    "]:\n",
    "    dashboard_analysis.objects[[el.name for el in dashboard_analysis].index(\"file_config\")].append(el)\n",
    "\n",
    "#### Callbacks\n",
    "#### File config\n",
    "################\n",
    "# future_execution\n",
    "# response_pattern\n",
    "# response_acquisition\n",
    "# response_reannotation\n",
    "# read_file\n",
    "# execution\n",
    "\n",
    "cache_uploaded = pn.widgets.Checkbox(value=False)\n",
    "cache_run = pn.widgets.Checkbox(value=False)\n",
    "#define widgets that should be disbled after run==True\n",
    "#wdgts = [i_acquisition,i_name_pattern,i_expname, i_pattern_examples, button_analysis, i_expname, i_organism,\n",
    "#         i_consecutiveLFQi, i_summed_MSMS_counts, i_RatioHLcount, i_RatioVariability, i_comment,\n",
    "#         i_custom_cids, i_custom_cgenes, i_custom_cdata, i_custom_normalized,\n",
    "#         i_reannotate_genes, i_reannotate_genes_file, i_reannotate_genes_source]\n",
    "\n",
    "def future_execution(event):\n",
    "    loading_status.objects = [loading]\n",
    "    analysis_status.object = \"Analysis in progress\"\n",
    "    lo_config_instructions.collapsed = True\n",
    "    lo_config_details.collapsed = True\n",
    "    lo_config_error_messages.collapsed = True\n",
    "    #lo_read_file.collapsed = True\n",
    "    output_layoutpos = [el.name for el in dashboard_analysis].index(\"analysis_output\")\n",
    "    dashboard_analysis.objects[output_layoutpos].objects = []\n",
    "    cache_run.value = False\n",
    "    try:\n",
    "        \n",
    "        global i_class\n",
    "        i_class = domaps.SpatialDataSet.from_settings(i_FileConfig.get_settings(), legacy=False)\n",
    "        i_class.run_pipeline(content=BytesIO(i_FileConfig._content.file.value), progressbar=analysis_status)\n",
    "        \n",
    "        analysis_status.object = \"Analysis finished!\"\n",
    "        update_object_selector(i_mapwidget, i_clusterwidget)\n",
    "        loading_status.objects = []\n",
    "        dashboard_analysis.objects[output_layoutpos].append(i_clusterwidget)\n",
    "        dashboard_analysis.objects[output_layoutpos].append(i_mapwidget)\n",
    "        dashboard_analysis.objects[output_layoutpos].append(analysis_tabs)\n",
    "        mem_available_datasets[i_class.expname] = i_class.analysed_datasets_dict[i_class.expname]\n",
    "        try:\n",
    "            i_dfs_available.options = i_dfs_available.options + [i_class.expname]\n",
    "            i_dfs_available.value = i_dfs_available.value + [i_class.expname]\n",
    "            coll_activatebuttons(i_dfs_available.value)\n",
    "            resize(lo_dfs_available)\n",
    "        except:\n",
    "            pass\n",
    "        cache_run.value = True\n",
    "    except:\n",
    "        loading_status.objects = [\"\"]\n",
    "        analysis_status.object = traceback.format_exc()\n",
    "        cache_run.value = False\n",
    "\n",
    "i_FileConfig._btn_run.on_click(future_execution)\n",
    "\n",
    "\n",
    "#### Analysis output\n",
    "\n",
    "i_logOR01_selection = pn.widgets.Select(options=[\"0/1 normalized data\", \"log transformed data\",\n",
    "                                                 \"stringency filtered raw data\", \"Overview - cluster distances\"],\n",
    "                                        name=\"Select type of data for download\", width=300)\n",
    "\n",
    "i_clusterwidget = pn.widgets.Select(options=[\"Proteasome\", \"Lysosome\"], name=\"Cluster of interest\", width=300)\n",
    "i_mapwidget = pn.widgets.Select(options=[\"Map1\", \"Map2\"], name=\"Map of interest\", width=300)\n",
    "\n",
    "i_collapse_maps_PCA = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "i_pca_ncomp = pn.widgets.IntSlider(value=3, start=2, end=5)\n",
    "btn_pca_ncomp = pn.widgets.Button(name=\"Recalculate PCA with different number of components.\")\n",
    "\n",
    "\n",
    "## Analysis tab\n",
    "\n",
    "def update_object_selector(i_mapwidget, i_clusterwidget):\n",
    "    i_mapwidget.options = list(i_class.map_names)\n",
    "    i_clusterwidget.options = list(i_class.markerproteins.keys())\n",
    "    i_pca_ncomp.end = len(i_class.fractions)\n",
    "\n",
    "\n",
    "def recalculate_pca(event):\n",
    "    cache_run.value=False\n",
    "    i_class.perform_pca(n=i_pca_ncomp.value)\n",
    "    cache_run.value=True\n",
    "btn_pca_ncomp.on_click(recalculate_pca)\n",
    "    \n",
    "@pn.depends(cache_run.param.value,\n",
    "            i_collapse_maps_PCA.param.value)\n",
    "def update_data_overview(run, collapse_maps_PCA):\n",
    "    try:\n",
    "        if run == True:\n",
    "            \n",
    "            compartments = i_class.df_organellarMarkerSet[\"Compartment\"].unique()\n",
    "            compartment_color = dict(zip(compartments, i_class.css_color))\n",
    "            compartment_color[\"undefined\"] = \"lightgrey\"\n",
    "            \n",
    "            pca_plot = gui.pca_plot(\n",
    "                df_pca=i_class.df_pca if not collapse_maps_PCA else i_class.df_pca_combined,\n",
    "                df_var=i_class.df_pca_var if not collapse_maps_PCA else i_class.df_pca_combined_var,\n",
    "                df_loadings=i_class.df_pca_loadings if not collapse_maps_PCA else i_class.df_pca_combined_loadings,\n",
    "                color=\"Compartment\", color_map=compartment_color,\n",
    "                facet_col=\"Map\" if not collapse_maps_PCA else None,\n",
    "                title=\"PCA plot\"\n",
    "            )\n",
    "            pca_plot.highlight_dict=i_class.markerproteins\n",
    "            \n",
    "            log_histogram = i_class.plot_log_data()\n",
    "            visualization_map = pn.Column(\n",
    "                pn.Row(i_collapse_maps_PCA, i_pca_ncomp, btn_pca_ncomp),\n",
    "                pca_plot,\n",
    "                pn.Row(pn.Pane(log_histogram, width=1000, config=plotly_config))\n",
    "            )\n",
    "            app_tabs.active = 1\n",
    "            return visualization_map\n",
    "        else:\n",
    "            visualization_map = \"Run analysis first!\"\n",
    "            return visualization_map\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return pn.Column(\n",
    "            pn.Row(i_collapse_maps_PCA),          \n",
    "            update_status\n",
    "        )\n",
    "            \n",
    "\n",
    "@pn.depends(i_clusterwidget.param.value,i_mapwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_overview(clusterwidget, mapwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            list_genes = [goi for goi in i_class.genenames_sortedout_list if goi in i_class.markerproteins[clusterwidget]]\n",
    "            i_class.cache_cluster_quantified = True\n",
    "            distance_boxplot = i_class.distance_boxplot(cluster_of_interest=clusterwidget)\n",
    "            if i_class.cache_cluster_quantified == False:\n",
    "                return \"This protein cluster was not quantified\"\n",
    "            \n",
    "            else:\n",
    "                df_quantification_overview = i_class.quantification_overview(cluster_of_interest=clusterwidget)\n",
    "                profiles_plot = i_class.profiles_plot(map_of_interest = mapwidget, cluster_of_interest=clusterwidget)\n",
    "                pca_plot = gui.pca_plot(\n",
    "                    df_pca=i_class.df_pca_all_marker_cluster_maps.xs(clusterwidget, level=\"Cluster\", axis=0),\n",
    "                    show_variability=False, show_loadings=False, enable_highlight=False,\n",
    "                    facet_col=None, color=\"Map\", color_map=dict(),\n",
    "                    title=f\"PCA plot of {clusterwidget}\"\n",
    "                )\n",
    "                pca_plot._dimensions.value=\"3D\"\n",
    "                cluster_overview = pn.Column(\n",
    "                        pn.Row(pca_plot,\n",
    "                               pn.Pane(profiles_plot, width=500, config=plotly_config),\n",
    "                               pn.Pane(distance_boxplot, width=500, config=plotly_config),\n",
    "                              ),\n",
    "                        pn.Row(pn.Pane(\"In total {} proteins across all maps were quantified, whereas the following proteins were not consistently quantified throughout all maps: {}\".format(\n",
    "                                i_class.proteins_quantified_across_all_maps, \", \".join(list_genes)) if len(list_genes) != 0 else\n",
    "                            \"All genes from this cluster are quantified in all maps.\"), width=1000),\n",
    "                        pn.Row(pn.widgets.DataFrame(df_quantification_overview, height=200, width=500, disabled=True))  \n",
    "                        )\n",
    "                return cluster_overview\n",
    "        \n",
    "        else:\n",
    "            cluster_overview = \"Run analysis first!\"\n",
    "            return cluster_overview\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(i_clusterwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_details(clusterwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            cluster_details = i_class.distance_to_median_boxplot(cluster_of_interest = clusterwidget)\n",
    "            return pn.Pane(cluster_details, width=1000, config=plotly_config)\n",
    "        else:\n",
    "            cluster_details = \"Run analysis first!\"\n",
    "            return pn.Pane(cluster_details, width=1000)\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_quantity(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            fig_npg, fig_npr, fig_npr_dc, fig_npg_F, fig_npgf_F, fig_npg_F_dc = i_class.plot_quantity_profiles_proteinGroups()\n",
    "            return pn.Column(\n",
    "                    pn.Row(pn.Pane(fig_npg, config=plotly_config), pn.Pane(fig_npr, config=plotly_config), pn.Pane(fig_npr_dc, config=plotly_config)) ,\n",
    "                pn.Row(pn.Pane(fig_npg_F, config=plotly_config), pn.Pane(fig_npgf_F, config=plotly_config), pn.Pane(fig_npg_F_dc, config=plotly_config)))\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def show_tabular_overview(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            content = pn.Column(\n",
    "                pn.widgets.DataFrame(pd.read_json(i_class.analysed_datasets_dict[i_class.expname][\"Overview table\"]), height=200, width=600, disabled=True),\n",
    "\n",
    "                i_logOR01_selection,\n",
    "                df01_download_widget,\n",
    "                pn.widgets.FileDownload(callback=json_download, filename=\"AnalysedDatasets.json\")\n",
    "            )\n",
    "            return pn.Row(content, width=1000)\n",
    "        else:\n",
    "            content = \"Please, upload a file first and press ‘Analyse clusters’\"\n",
    "            return pn.Row(content, width=1000)\n",
    "    except Exception:\n",
    "        content = traceback.format_exc()\n",
    "        return pn.Row(content, width=1000)\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def json_download(run):\n",
    "    sio = StringIO()\n",
    "    json.dump(\n",
    "        i_class.analysed_datasets_dict, \n",
    "        sio, \n",
    "        indent=4, \n",
    "        sort_keys=True\n",
    "    )\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value, i_logOR01_selection.param.value)\n",
    "def df01_download_widget(run, logOR01_selection):\n",
    "    if logOR01_selection == \"0/1 normalized data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df01_download, filename = \"01_normalized_data.csv\"), width=650) \n",
    "    if logOR01_selection == \"log transformed data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=dflog_download, filename = \"log_transformed_data.csv\"), width=650)\n",
    "    if logOR01_selection == \"Overview - cluster distances\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=table_download, filename = \"cluster_distances.csv\"), width=650)  \n",
    "    else:\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df_filteredRawData_download, filename = \"stringency_filtered_raw_data.csv\"), width=650)\n",
    "\n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def df01_download(run):\n",
    "    df_01 = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_01_stacked)\n",
    "    sio = StringIO()\n",
    "    df_01.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "    \n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def dflog_download(run):\n",
    "    df_log = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_log_stacked)\n",
    "    sio = StringIO()\n",
    "    df_log.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def df_filteredRawData_download(run):\n",
    "    df = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_filtered.stack([\"Map\", \"Fraction\"]))\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def table_download(run):\n",
    "    df = i_class.results_overview_table()\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "analysis_tabs.clear()\n",
    "analysis_tabs.append((\"Data overview\", update_data_overview))\n",
    "analysis_tabs.append((\"Depth and Coverage\", update_quantity))\n",
    "analysis_tabs.append((\"Cluster Overview\", update_cluster_overview))\n",
    "analysis_tabs.append((\"Cluster Details\", update_cluster_details))\n",
    "analysis_tabs.append((\"Download\", show_tabular_overview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison tab\n",
    "\n",
    "loading_status_comparison = pn.Row()\n",
    "idle_comparison = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading_comparison = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "\n",
    "cache_uploaded_json = pn.widgets.Checkbox(value=False)\n",
    "cache_run_json = pn.widgets.Checkbox(value=False)\n",
    "#button_comparison = pn.widgets.Button(name=\"Compare experiments\", width=50)\n",
    "#i_jsonFile = pn.widgets.FileInput(name=\"Upload JSON file for comparison\")\n",
    "i_multi_choice = pn.widgets.CrossSelector(name=\"Select experiments for comparison\", value=[\"a\", \"b\"],\n",
    "                                          options=[\"a\", \"b\", \"c\"], definition_order=False)\n",
    "m_diverget_fractions = pn.Pane(\"\")\n",
    "i_ref_exp = pn.widgets.Select(name=\"Select experiments as reference\", options=[\"a\", \"b\", \"c\"])\n",
    "i_collapse_cluster = pn.widgets.Checkbox(value=True, name=\"Collapse cluster\")\n",
    "comparison_status = pn.Pane(\"No datasets were compared yet\")\n",
    "i_markerset_or_cluster = pn.widgets.Checkbox(value=False, name=\"Display only protein clusters\")\n",
    "i_pca_comp_ncomp = pn.widgets.IntSlider(value=3, start=2, end=5)\n",
    "btn_pca_comp_ncomp = pn.widgets.Button(name=\"Recalculate PCA with different number of components.\")\n",
    "#i_ranking_boxPlot = pn.widgets.Checkbox(value=False, name=\"Display box plot\")\n",
    "i_ranking_boxPlot = pn.widgets.RadioBoxGroup(name=\"Types of ranking\", options=[\"Box plot\", \"Bar plot - median\", \"Bar plot - sum\"], inline=True)\n",
    "#i_toggle_sumORmedian = pn.widgets.Toggle(name=\"Sum or Median\", button_type=\"primary\")\n",
    "i_clusterwidget_comparison = pn.widgets.Select(options=[], name=\"Cluster of interest\", width=300)\n",
    "i_ExpOverview = pn.Row(pn.Pane(\"\", width=1000))\n",
    "i_include_dataset = pn.widgets.Checkbox(value=False, name=\"Include data analysed under 'Analysis' tab\")\n",
    "i_compare_gene = pn.widgets.TextInput(value=\"PLEC\", name=\"Enter gene name or protein ID to see profile.\")\n",
    "i_compare_profile_style = pn.widgets.Select(options=[\n",
    "    #\"all profiles\",\n",
    "    \"mean +- stdev\", \"mean +- SEM\"])\n",
    "i_compare_compartment = pn.widgets.MultiSelect(options=[], name=\"Select compartments for which to show summary profiles.\")\n",
    "#wdgts_comparison = [button_comparison]#,i_organism_comparison]#, i_include_dataset]\n",
    "json_exp_name_cache = []\n",
    "\n",
    "def recalculate_comp_pca(event):\n",
    "    cache_run_json.value=False\n",
    "    i_class_comp.perform_pca_comparison(n=i_pca_comp_ncomp.value)\n",
    "    cache_run_json.value=True\n",
    "btn_pca_comp_ncomp.on_click(recalculate_comp_pca)\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value,\n",
    "            i_markerset_or_cluster.param.value)\n",
    "def update_visualization_map_comparison(multi_choice, run_json, markerset_or_cluster):\n",
    "    try:\n",
    "        if run_json == True:\n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\")\n",
    "                                )\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            df_pca = i_class_comp.df_pca.loc[i_class_comp.df_pca[\"Experiment\"].isin(multi_choice)]\\\n",
    "                .set_index([col for col in i_class_comp.df_pca.columns if not col.startswith(\"PC\")])\n",
    "            \n",
    "            pca_global_comparison = gui.pca_plot(\n",
    "                df_pca=df_pca,\n",
    "                df_var=i_class_comp.df_pca_var,\n",
    "                df_loadings=i_class_comp.df_pca_loadings,\n",
    "                show_variability=True, show_loadings=True,\n",
    "                color=\"Cluster\" if markerset_or_cluster else \"Compartment\",\n",
    "                color_map=i_class_comp.color_maps[\"Clusters\"] if markerset_or_cluster else i_class_comp.color_maps[\"Compartments\"],\n",
    "                enable_highlight = False if markerset_or_cluster else True,\n",
    "                facet_col=\"Experiment\",\n",
    "                title=\"PCA plot\"\n",
    "            )\n",
    "            pca_global_comparison.highlight_dict = i_class_comp.markerproteins\n",
    "            return pn.Column(\n",
    "                pn.Row(i_markerset_or_cluster, i_pca_comp_ncomp, btn_pca_comp_ncomp),\n",
    "                pca_global_comparison\n",
    "            )\n",
    "        else:\n",
    "            pca_global_comparison = \"Run analysis first!\"\n",
    "            return pca_global_comparison\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return pn.Column(\n",
    "            pn.Row(i_markerset_or_cluster, i_pca_comp_ncomp, btn_pca_comp_ncomp),\n",
    "            update_status\n",
    "        )\n",
    "\n",
    "#### Biological precision tab\n",
    "## Widgets\n",
    "i_clusters_for_ranking = pn.widgets.CrossSelector(name=\"Select clusters to be considered for ranking calculation\",\n",
    "                                                  options=[], size=8)\n",
    "i_minn_proteins = pn.widgets.IntSlider(name=\"Minimum number of proteins per complex\", start=3, end=13, step=1, value=5)\n",
    "i_collapse_maps = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "i_reference_map = pn.widgets.Select(options=[], value=\"\", name=\"Select reference map\")\n",
    "\n",
    "## Callbacks\n",
    "@pn.depends(i_multi_choice.param.options, i_minn_proteins.param.value, cache_run_json.param.value)\n",
    "def update_comp_cluster_coverage(exp_names, min_n, run_json):\n",
    "    try:\n",
    "        if not run_json:\n",
    "            return \"\"\n",
    "        [f,p,n] = i_class_comp.get_complex_coverage(min_n)\n",
    "        i_clusters_for_ranking.options = [el for el in i_class_comp.markerproteins.keys() if el not in n.keys()]\n",
    "        i_clusterwidget_comparison.options = [el for el in i_class_comp.markerproteins.keys() if el not in n.keys()]\n",
    "        i_clusters_for_ranking.value = [el for el in i_class_comp.markerproteins.keys() if el in f.keys()]\n",
    "        return pn.Row(\n",
    "            \"Coverage in all experiments \\[>= n proteins]:<br>\"+\"<br>\".join([\"- {} ({})\".format(k,v) for k,v in f.items()]),\n",
    "            \"Coverage in some experiments \\[proteins/experiment]:<br>\"+\"<br>\".join([\"- {} \\{}\".format(k,str(v)) for k,v in p.items()]),\n",
    "            \"No sufficient coverage in any experiment \\[proteins/experiment]:<br>\"+\"<br>\".join([\"- {} \\{}\".format(k,str(v)) for k,v in n.items()])\n",
    "        )\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusters_for_ranking.param.value, i_reference_map.param.value,\n",
    "            i_minn_proteins.param.value, cache_run_json.param.value)\n",
    "def update_comp_bp_global(multi_choice, clusters_for_ranking, reference_map, min_n, run_json):\n",
    "    try:\n",
    "        if not run_json:\n",
    "            return \"\"\n",
    "        if set(multi_choice) != set(i_class_comp.df_distance_comp.Experiment.values):\n",
    "            i_class_comp.calc_biological_precision(multi_choice)\n",
    "            i_reference_map.options = multi_choice\n",
    "            if reference_map not in multi_choice:\n",
    "                i_reference_map.value = multi_choice[0]\n",
    "                reference_map = multi_choice[0]\n",
    "        if clusters_for_ranking == []:\n",
    "            return \"Select at least one cluster\"\n",
    "        else:\n",
    "            bp_bargraph, bp_boxplot_abs, bp_boxplot_rel = i_class_comp.plot_biological_precision(\n",
    "                multi_choice, clusters_for_ranking, min_members=min_n, reference = reference_map)\n",
    "            return pn.Column(\n",
    "                pn.Row(i_reference_map),\n",
    "                pn.Row(pn.Pane(bp_bargraph, config=plotly_config),\n",
    "                       pn.Pane(bp_boxplot_abs, config=plotly_config),\n",
    "                       pn.Pane(bp_boxplot_rel, config=plotly_config)))\n",
    "        \n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return pn.Column(\n",
    "            pn.Row(i_reference_map),\n",
    "            update_status\n",
    "        )\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusterwidget_comparison.param.value, i_collapse_maps.param.value,\n",
    "            cache_run_json.param.value)\n",
    "def update_comp_bp_single(multi_choice, clusterwidget_comparison, collapse_maps, run_json):\n",
    "    try:\n",
    "        i_class_comp.cache_cluster_quantified = True\n",
    "        distance_comparison = i_class_comp.distance_boxplot_comparison(collapse_maps=collapse_maps, cluster_of_interest_comparison=clusterwidget_comparison, multi_choice=multi_choice)\n",
    "        if i_class_comp.cache_cluster_quantified == False:\n",
    "            return \"Cluster was not quantified in any experiment\"\n",
    "        else:\n",
    "            df_pca = i_class_comp.df_cluster_pca.xs(clusterwidget_comparison, level=\"Cluster\", axis=0)\n",
    "            df_pca = df_pca.query(f'Experiment in {str(multi_choice)}')\n",
    "            if collapse_maps:\n",
    "                df_pca = df_pca.groupby([el for el in df_pca.index.names if \"Map\" not in el]).mean()\n",
    "            pca_comparison = gui.pca_plot(\n",
    "                df_pca=df_pca,\n",
    "                color=\"Experiment\", color_map=dict(), facet_col=None,\n",
    "                enable_highlight=False, show_variability=False, show_loadings=False,\n",
    "                title=\"PCA plot for {}\".format(clusterwidget_comparison)\n",
    "            )\n",
    "            return pn.Row(pca_comparison,\n",
    "                          pn.Pane(distance_comparison, config=plotly_config))\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "## Tab assembly\n",
    "comparison_tab_bp = pn.Column(\n",
    "    pn.Row(pn.Column(i_clusters_for_ranking,i_minn_proteins),\n",
    "           update_comp_cluster_coverage),\n",
    "    update_comp_bp_global,\n",
    "    pn.Row(i_clusterwidget_comparison,i_collapse_maps),\n",
    "    update_comp_bp_single\n",
    ")\n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_npr_ngg_nprDc(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                fig_quantity_pg, fig_quantity_pr = i_class_comp.quantity_pr_pg_barplot_comparison(multi_choice=multi_choice)\n",
    "                coverage_barplot = i_class_comp.coverage_comparison(multi_choice=multi_choice)\n",
    "                return pn.Row(pn.Column(\n",
    "                                 pn.Pane(fig_quantity_pg, config=plotly_config), \n",
    "                                 #pn.Pane(fig_quantity_pr, config=plotly_config), #removed for now, as the added information is not a lot\n",
    "                                 pn.Pane(coverage_barplot, config=plotly_config)\n",
    "                                ))\n",
    "        else:\n",
    "            completeness_barplot = \"Run analysis first!\"\n",
    "            return completeness_barplot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status \n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_venn(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            venn_plot = []\n",
    "            if len(multi_choice)<=1:\n",
    "                return pn.Column(\n",
    "                    pn.Row(pn.Pane(\"Please select 2 or more experiments for comparison\"), width=1000))\n",
    "            else:\n",
    "                venn_plot_total, venn_plot_int, figure_UpSetPlot_total, figure_UpSetPlot_int = i_class_comp.venn_sections(multi_choice_venn = multi_choice)\n",
    "                return pn.Row(\n",
    "                    pn.Column(\n",
    "                        \"Proteins quantified in at least one map\",\n",
    "                        pn.Pane(venn_plot_total),\n",
    "                        pn.Row(figure_UpSetPlot_total,width=1000)\n",
    "                    ),\n",
    "                    pn.Column(\n",
    "                        \"Proteins quantified in all maps\",\n",
    "                        pn.Pane(venn_plot_int),\n",
    "                        pn.Row(figure_UpSetPlot_int,width=1000)\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            venn_plot = \"Run analysis first!\"\n",
    "            return venn_plot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status    \n",
    "\n",
    "\n",
    "#### Dashboard structure\n",
    "#### SMV analysis\n",
    "########################\n",
    "lo_benchmark_SVMs = pn.Column(\n",
    "    pn.Card(header=\"###Add misclassification matrix\", name=\"add_mcmatrix\"),\n",
    "    pn.Row(name=\"svm_output\")\n",
    ")\n",
    "\n",
    "#### Layout elements\n",
    "#### SVM analysis\n",
    "####################\n",
    "SVM_status = pn.pane.Markdown(width=400)\n",
    "lo_SVM_heatmap = pn.Column(SVM_status)\n",
    "i_SVMmatrix = pn.widgets.input.TextAreaInput(name=\"Misclassification matrix\", placeholder=\"Copy matrix here...\")\n",
    "i_SVMsource = pn.widgets.Select(options=[\"Perseus\", \"MetaMass\", \"direct\"], value=\"Perseus\", name=\"Select source of misclassification matrix\")\n",
    "lo_SVM_source = pn.Row(i_SVMsource, gui.help_icon(\"For Perseus, copy whole matrix via Ctrl-A, Ctrl-C. For MetaMass copy matrix including column headings, but without row labels. Direct assumes true classes in rows and predicted classes in columns, with column headings only.\"))\n",
    "i_SVMcomment = pn.widgets.TextInput(name=\"Comment\", placeholder=\"Add comments here...\")\n",
    "i_SVMexp = pn.widgets.Select(name=\"Select experiments for the assignment of a misclassification matrix\", options=[\"a\", \"b\", \"c\"])\n",
    "btn_SVM_addmatrix = pn.widgets.Button(name=\"Update misclassification matrix\",\n",
    "                                      button_type=\"success\", width=400, height=50,\n",
    "                                      css_classes=[\"button-main\"])\n",
    "#### Append layout to dashboard\n",
    "#### SVM analysis\n",
    "###############################\n",
    "for el in [i_SVMexp, lo_SVM_source, i_SVMcomment, i_SVMmatrix, lo_SVM_heatmap, btn_SVM_addmatrix]:\n",
    "    lo_benchmark_SVMs.objects[[el.name for el in lo_benchmark_SVMs.objects].index(\"add_mcmatrix\")].append(el)\n",
    "\n",
    "#### Callbacks\n",
    "# update_SVMexp\n",
    "# add_SVM_result\n",
    "# update_SVM_Analysis # needs update, including the backend function in domaps.py\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_SVMexp(multi_choice):\n",
    "    i_SVMexp.options = multi_choice\n",
    "\n",
    "\n",
    "def add_SVM_result(event):\n",
    "    \"\"\"\n",
    "    Adds SVM matrix uploaded in the tool to memory and to the current analysis\n",
    "    \"\"\"\n",
    "    # 1. Get input from interface\n",
    "    \n",
    "    experiment, SVMsource, SVMcomment = i_SVMexp.value, i_SVMsource.value, i_SVMcomment.value\n",
    "    \n",
    "    # change comment of a stored dataset\n",
    "    try:\n",
    "        if i_SVMmatrix.value == \"\" and i_class_comp.svm_results[experiment][\"default\"][\"misclassification\"] is not None:\n",
    "            SVMmatrix = i_class_comp.svm_results[experiment][\"default\"][\"misclassification\"]\n",
    "        # return error if no misclassification  is uploaded or no comment is changed\n",
    "        #if i_SVMmatrix.value == \"\":\n",
    "        #    SVM_status.object = \"No misclassification matrix is uploaded\"\n",
    "        else:\n",
    "            SVMmatrix = pd.read_table(StringIO(i_SVMmatrix.value), sep=\"\\t\")\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        SVM_status.object = update_status\n",
    "# \n",
    "    # 2. Add to i_class_comp\n",
    "    # defaults to name=\"default\" and overwrite=True\n",
    "    i_class_comp.add_svm_result(experiment, SVMmatrix, source=SVMsource, comment=SVMcomment)\n",
    "    # 3. Add to mem_available_datasets so it can be downloaded together with the data\n",
    "    mem_available_datasets[experiment][\"SVM results\"] = copy.deepcopy(i_class_comp.svm_results[experiment])\n",
    "    for k in i_class_comp.svm_results[experiment].keys():\n",
    "        mc_json = mem_available_datasets[experiment][\"SVM results\"][k][\"misclassification\"].to_json()\n",
    "        mem_available_datasets[experiment][\"SVM results\"][k][\"misclassification\"] = mc_json\n",
    "        pr_json = mem_available_datasets[experiment][\"SVM results\"][k][\"prediction\"].to_json()\n",
    "        mem_available_datasets[experiment][\"SVM results\"][k][\"prediction\"] = pr_json\n",
    "        \n",
    "    #empty misclassification matrix of the textarea-input widget\n",
    "    i_SVMmatrix.value = \"\"\n",
    "    # 4. Trigger update of figure\n",
    "    update_SVM_Analysis(i_multi_choice.value)\n",
    "\n",
    "\n",
    "btn_SVM_addmatrix.on_click(add_SVM_result)\n",
    "    \n",
    "@pn.depends(i_SVMexp.param.value, i_SVMmatrix.param.value, watch=True)\n",
    "def fill_svm_comment(SVMexp, SVMmatrix):\n",
    "    \"\"\"\n",
    "    Acess stored comment if possible and no new matrix was uploaded.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        comment = i_class_comp.svm_results[SVMexp][\"default\"][\"comment\"]\n",
    "        if SVMmatrix != \"\" or comment == \"\":\n",
    "            timestampStr = datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\")\n",
    "            comment = \"Matrix added on {date}\".format(date=timestampStr)\n",
    "        else:\n",
    "            pass\n",
    "    except Exception:\n",
    "            timestampStr = datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\")\n",
    "            comment = \"Matrix added on {date}\".format(date=timestampStr)\n",
    "    i_SVMcomment.value = comment\n",
    "    #except Exception:\n",
    "    #    update_status = traceback.format_exc()\n",
    "    #    SVM_status.object = update_status\n",
    "        \n",
    "@pn.depends(i_SVMexp.param.value, i_SVMmatrix.param.value)\n",
    "def show_misclassification(SVMexp, SVMmatrix):\n",
    "    \"\"\"\n",
    "    Display heatmap of freshly uploaded or reloaded SVM misclassification matrix\n",
    "    \"\"\"\n",
    "    if SVMmatrix == \"\":\n",
    "        try: \n",
    "            df_SVM = i_class_comp.svm_results[SVMexp][\"default\"][\"misclassification\"]\n",
    "            SVMheatmap = domaps.svm_heatmap(df_SVM)\n",
    "        except: \n",
    "            SVMheatmap = \"No misclassification matrix is uploaded\"\n",
    "    else: \n",
    "        df_SVM = pd.read_table(StringIO(SVMmatrix), sep=\"\\t\")\n",
    "        SVMheatmap = domaps.svm_heatmap(df_SVM)\n",
    "    return SVMheatmap\n",
    "\n",
    "@pn.depends(i_SVMexp.param.value)\n",
    "def load_matrix(SVMexp):\n",
    "    \"\"\"\n",
    "    Load data if new experiment is selected \n",
    "    \"\"\"\n",
    "    try:\n",
    "        SVMmatrix = i_class_comp.svm_results[SVMexp][\"default\"][\"misclassification\"]\n",
    "        SVM_status.object = \"Misclassification Matrix from class\"\n",
    "    except:\n",
    "        SVM_status.object = \"Upload missclassificationmatrix first\"\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)        \n",
    "def update_SVM_Analysis(multi_choice):     \n",
    "    #empty lo if available\n",
    "    lo_benchmark_SVMs.objects[[i.name for i in lo_benchmark_SVMs].index(\"svm_output\")].objects = []\n",
    "    try:\n",
    "        if multi_choice == []:\n",
    "            lo = pn.Column(pn.Row(\"Please select experiments for comparison\"))\n",
    "        else:\n",
    "            for exp in multi_choice:\n",
    "                # catch exception, if misclassification matrix is missing\n",
    "                try:\n",
    "                    i_class_comp.svm_results[exp]\n",
    "                    i_class_comp.svm_processing()\n",
    "                    fig_markerPredictionAccuracy, fig_clusterPerformance = i_class_comp.svm_plotting(multi_choice)\n",
    "                    lo = pn.Column(pn.Pane(fig_markerPredictionAccuracy, config=plotly_config),\n",
    "                                pn.Pane(fig_clusterPerformance, config=plotly_config),\n",
    "                                pn.Row(pn.Pane(\"\", width=1000))\n",
    "                               )\n",
    "                except KeyError:\n",
    "                #if i_class_comp.svm_results[exp][\"default\"][\"misclassification\"] is None:\n",
    "                    lo = pn.Column(pn.Row(\"Please upload misclassfication matrix for {}\".format(exp)))\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        lo = update_status\n",
    "    lo_benchmark_SVMs.objects[[el.name for el in lo_benchmark_SVMs.objects].index(\"svm_output\")].append(lo)\n",
    "    \n",
    "#### Callback output positioning\n",
    "#### SVM analysis\n",
    "################################\n",
    "lo_SVM_heatmap.append(show_misclassification)\n",
    "#lo_benchmark_SVMs.objects[[el.name for el in lo_benchmark_SVMs.objects].index(\"svm_output\")].append(update_SVM_Analysis())\n",
    "\n",
    "i_scatter_metric = pn.widgets.Select(name=\"Distance metric\",\n",
    "                                     options=[\"manhattan distance to average profile\",\n",
    "                                              \"manhattan distance to median profile\",\n",
    "                                              \"euclidean distance\", \"manhattan distance\",\n",
    "                                              \"1 - cosine correlation\", \"1 - pearson correlation\",])\n",
    "i_scatter_consolidation = pn.widgets.Select(name=\"Consolidation of replicate distances\",\n",
    "                                            options=[\"average\",\"median\",\"sum\"])\n",
    "i_scatter_quantile = pn.widgets.FloatInput(name=\"Highlight quantile\", value=0.5)\n",
    "i_scatter_showrug = pn.widgets.Checkbox(name=\"Show rugplot in bottom margin\", value=False)\n",
    "i_scatter_showfull = pn.widgets.Checkbox(name=\"Show additional traces for full datasets\", value=False)\n",
    "i_scatter_x_cut = pn.widgets.FloatInput(name=\"Cut x-axis at\", value=1)\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_scatter_metric.param.value, i_scatter_consolidation.param.value,\n",
    "            cache_run_json.param.value, i_scatter_showfull.param.value,\n",
    "            i_scatter_quantile.param.value, i_scatter_showrug.param.value, i_scatter_x_cut.param.value)\n",
    "def update_global_scatter_comparison(multi_choice, metric, consolidation, run_json,\n",
    "                                     show_full, quantile, show_rug, x_cut):\n",
    "    try:\n",
    "        if run_json == False:\n",
    "            return \"Run analysis first!\"\n",
    "        if multi_choice == []:\n",
    "            return pn.Column(pn.Row(\"Please select experiments for comparison\"))\n",
    "        if \"reproducibility consolidation\" not in i_class_comp.parameters.keys() or \\\n",
    "        i_class_comp.parameters[\"reproducibility metric\"] != metric or \\\n",
    "        i_class_comp.parameters[\"reproducibility consolidation\"] != consolidation:\n",
    "            i_class_comp.calculate_global_scatter(metric, consolidation)\n",
    "        reproducibility_plot = i_class_comp.plot_reproducibility_distribution(\n",
    "            multi_choice=multi_choice,\n",
    "            x_cut=x_cut, q=quantile,\n",
    "            show_rug=show_rug, show_full=show_full)\n",
    "        return pn.Column(pn.Row(pn.Column(i_scatter_metric,i_scatter_consolidation),\n",
    "                                pn.Column(\"Customize:\",i_scatter_showrug,i_scatter_showfull,\n",
    "                                          i_scatter_quantile,i_scatter_x_cut)),\n",
    "                         pn.Pane(reproducibility_plot, config=plotly_config)\n",
    "                        )\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return pn.Column(pn.Row(pn.Column(i_scatter_metric,i_scatter_consolidation),\n",
    "                                pn.Column(\"Customize:\",i_scatter_showrug,i_scatter_showfull,\n",
    "                                          i_scatter_quantile,i_scatter_x_cut)),\n",
    "                         update_status\n",
    "                        )\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_compare_gene.param.value, i_compare_compartment.param.value,\n",
    "            i_compare_profile_style.param.value, cache_run_json.param.value)\n",
    "def update_profile_comparison(multi_choice, compare_gene, compare_compartment, compare_profile_style, run_json):\n",
    "    if not run_json:\n",
    "        return \"Run analysis first!\"\n",
    "    try:\n",
    "        try:\n",
    "            plotdata = i_class_comp.df_01_filtered_combined.xs(compare_gene,level=\"Gene names\",\n",
    "                                                               axis=0, drop_level=False)\\\n",
    "                                   .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "        except:\n",
    "            try:\n",
    "                plotdata = i_class_comp.df_01_filtered_combined.xs(compare_gene,level=\"Protein IDs\",\n",
    "                                                                   axis=0, drop_level=False)\\\n",
    "                                       .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "            except:\n",
    "                try:\n",
    "                    plotdata = i_class_comp.df_01_filtered_combined.loc[[\n",
    "                        compare_gene in \" \".join(el) for el in i_class_comp.df_01_filtered_combined.index.values],:]\\\n",
    "                                           .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "                except:\n",
    "                    plotdata = pd.DataFrame()\n",
    "        if len(plotdata) > 0:\n",
    "            plotdata.drop(\"Exp_Map\", axis=1, inplace=True)\n",
    "            plotdata.sort_values(\"Fraction\", key=domaps.natsort_list_keys, inplace=True)\n",
    "            experiments = [el for el in multi_choice if el in plotdata[\"Experiment\"].values]\n",
    "            plotdata = plotdata.set_index(\"Experiment\").loc[experiments,:].reset_index()\n",
    "            plotprofile = px.line(plotdata, x=\"Fraction\", y=\"Profile [% total signal]\", line_group=\"Map\",\n",
    "                                  facet_col=\"Protein IDs\", template=\"simple_white\",\n",
    "                                  line_dash = \"Sequence\" if \"Sequence\" in plotdata.columns else None,\n",
    "                                  color=\"Experiment\", hover_data=list(plotdata.columns))\n",
    "        else:\n",
    "            plotprofile = \"No gene or protein ID matching {} found.\".format(compare_gene)\n",
    "        \n",
    "        plotdata = pd.DataFrame()\n",
    "        if compare_profile_style == \"all profiles\":\n",
    "            for el in compare_compartment:\n",
    "                el_df = i_class_comp.df_01_filtered_combined.xs(el, level=\"Compartment\", axis=0, drop_level=False)\\\n",
    "                .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "                plotdata = plotdata.append(el_df)\n",
    "            if len(plotdata) > 0:\n",
    "                plotdata.sort_values(\"Fraction\", key=domaps.natsort_list_keys, inplace=True)\n",
    "                plotdata = plotdata.set_index(\"Experiment\").loc[multi_choice,:].reset_index()\n",
    "                plotdata.insert(0, \"PG_Map\", [str(p)+\"_\"+str(m) for p,m in zip(plotdata[\"Protein IDs\"], plotdata[\"Map\"])])\n",
    "                plotcompartments = px.box(plotdata, x=\"Fraction\", y=\"Profile [% total signal]\", color=\"Experiment\",\n",
    "                                          facet_row=\"Compartment\", template=\"simple_white\")\n",
    "            else:\n",
    "                plotcompartments = \"Please select at least one compartment\"\n",
    "        else:\n",
    "            for el in compare_compartment:\n",
    "                el_df = i_class_comp.df_01_filtered_combined.xs(el, level=\"Compartment\", axis=0, drop_level=False)\n",
    "                plotdata = plotdata.append(el_df.stack(\"Fraction\").groupby([\"Compartment\", \"Map\", \"Experiment\", \"Fraction\"])\\\n",
    "                    .apply(lambda x: pd.Series({\"Profile [% total signal]\": np.nanmean(x), \"std\":np.nanstd(x),\n",
    "                                                \"sem\":np.nanstd(x)/np.sqrt(sum(np.isfinite(x)))}))\\\n",
    "                    .reset_index().rename(columns={\"level_4\": \"measure\", 0: \"value\"})\\\n",
    "                    .set_index([\"Compartment\", \"Map\", \"Experiment\", \"Fraction\", \"measure\"]).unstack(\"measure\")\\\n",
    "                    .droplevel(0, axis=1).reset_index())\n",
    "            if len(plotdata) > 0:\n",
    "                plotdata.sort_values(\"Fraction\", key=domaps.natsort_list_keys, inplace=True)\n",
    "                plotdata = plotdata.set_index(\"Experiment\").loc[multi_choice,:].reset_index()\n",
    "                plotcompartments = px.line(plotdata, x=\"Fraction\", y=\"Profile [% total signal]\", color=\"Experiment\",\n",
    "                                           line_group=\"Map\", line_dash=\"Compartment\", template=\"simple_white\",\n",
    "                                           error_y=\"std\" if \"stdev\" in compare_profile_style else \"sem\")\n",
    "            else:\n",
    "                plotcompartments = \"Please select at least one compartment\"\n",
    "        \n",
    "        return pn.Row(pn.Column(i_compare_gene,\n",
    "                                pn.Pane(plotprofile, config=plotly_config)),\n",
    "                      pn.Column(pn.Row(i_compare_compartment, i_compare_profile_style),\n",
    "                                pn.Pane(plotcompartments, config=plotly_config)))\n",
    "    except Exception:\n",
    "        return pn.Row(pn.Column(i_compare_gene,\n",
    "                                traceback.format_exc()),\n",
    "                      pn.Column(pn.Row(i_compare_compartment, i_compare_profile_style)))\n",
    "\n",
    "\n",
    "def update_multi_choice():\n",
    "    i_multi_choice.options = i_class_comp.exp_names\n",
    "    i_reference_map.options = i_class_comp.exp_names\n",
    "    i_clusterwidget.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.value = list(i_class_comp.markerproteins.keys())\n",
    "    i_multi_choice.value = i_class_comp.exp_names\n",
    "    i_reference_map.value = i_class_comp.exp_names[0]\n",
    "\n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ref_exp(multi_choice):\n",
    "    i_ref_exp.options = i_multi_choice.value\n",
    "    #return i_ref_exp\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ExpOverview(multi_choice):\n",
    "    dict_analysis_parameters={}\n",
    "    for exp_name in multi_choice:\n",
    "        dict_analysis_parameters[exp_name] = i_class_comp.json_dict[exp_name][\"Analysis parameters\"]\n",
    "    i_ExpOverview[0] = pn.widgets.DataFrame(pd.DataFrame.from_dict(dict_analysis_parameters), height=300, disabled=True)\n",
    "\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Download data\n",
    "########################\n",
    "lo_benchmark_download = pn.Column()\n",
    "\n",
    "#### Layout elements\n",
    "#### Download data\n",
    "####################\n",
    "i_benchmark_downloadselector = pn.widgets.Select(options=[\n",
    "    \"0-1 normalized data.csv\",\n",
    "    \"pca coordinates.csv\",\n",
    "    \"complex scatter.csv\",\n",
    "    \"reproducibility.csv\",\n",
    "    \"protein id alignment.csv\",\n",
    "    \"benchmarking results collection.xlsx\"\n",
    "], value=\"0-1 normalized data.csv\", width=200)\n",
    "i_benchmark_download = pn.widgets.FileDownload(label=\"Download file\", width=200, button_type=\"success\")\n",
    "o_benchmark_downloadpreview = pn.Row()\n",
    "\n",
    "#### Append layout to dashboard\n",
    "#### Download data\n",
    "###############################\n",
    "for el in [i_benchmark_downloadselector,i_benchmark_download,o_benchmark_downloadpreview]:\n",
    "    lo_benchmark_download.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "#### Download data\n",
    "##################\n",
    "# benchmark_download_getsheet\n",
    "# benchmark_download_preview\n",
    "# benchmark_download\n",
    "\n",
    "def benchmark_download_getsheet(sheet, mode=\"csv\"):\n",
    "    if sheet == \"0-1 normalized data\":\n",
    "        out = i_class_comp.df_01_filtered_combined.copy()\n",
    "        out.index = out.index.droplevel(\"Exp_Map\")\n",
    "        out = out.unstack([\"Experiment\", \"Map\"])\n",
    "        out.columns = [\"_\".join(el) for el in out.columns.reorder_levels([\"Experiment\", \"Map\", \"Fraction\"]).values]\n",
    "    elif sheet == \"pca coordinates\":\n",
    "        out = i_class_comp.df_pca.copy()\n",
    "        out.index = out.index.droplevel(\"merge type\")\n",
    "    elif sheet == \"complex scatter\":\n",
    "        out = i_class_comp.df_distance_comp.copy()\n",
    "        if mode==\"csv\":\n",
    "            out[\"Cluster\"] = ['\"'+el+'\"' for el in out[\"Cluster\"]]\n",
    "        out = out.set_index(\n",
    "            [\"Cluster\", \"Gene names\", \"Protein IDs\", \"Compartment\", \"Experiment\", \"Map\"]).drop(\n",
    "            [\"Exp_Map\", \"merge type\"], axis=1).unstack([\"Experiment\", \"Map\"]).copy()\n",
    "        out.columns = [\"_\".join(el) for el in out.columns.values]\n",
    "    elif sheet == \"reproducibility\":\n",
    "        out = i_class_comp.distances.copy()\n",
    "    elif sheet == \"protein id alignment\":\n",
    "        out = i_class_comp.id_alignment.copy()\n",
    "        out.columns = [\" \".join(el) for el in out.columns.values]\n",
    "    else:\n",
    "        raise KeyError(sheet)\n",
    "    return out\n",
    "\n",
    "@pn.depends(i_benchmark_downloadselector.param.value, cache_run_json.param.value)\n",
    "def benchmark_download_preview(file_selection, run_json):\n",
    "    if not run_json:\n",
    "        return \"Run analysis first.\"\n",
    "    if file_selection.endswith(\".csv\"):\n",
    "        out = benchmark_download_getsheet(file_selection.split(\".csv\")[0], mode=\"csv\")\n",
    "        return pn.Column(f\"{out.shape[0]} rows x {out.head().reset_index().shape[1]} columns\",\n",
    "                          pn.widgets.DataFrame(\n",
    "            out.head(10),\n",
    "            editable=False)\n",
    "                         )\n",
    "    else:\n",
    "        return \"This will download a .xlsx file with all tables as individual sheets.\"\n",
    "\n",
    "@pn.depends(i_benchmark_downloadselector.param.value)\n",
    "def benchmark_download(file_selection):\n",
    "    i_benchmark_downloadselector.disabled=True\n",
    "    i_benchmark_download.loading=True\n",
    "    # set up list of files to format\n",
    "    if file_selection.endswith(\".csv\"):\n",
    "        sheets = [file_selection.split(\".csv\")[0]]\n",
    "        mode=\"csv\"\n",
    "    elif file_selection.endswith(\".xlsx\"):\n",
    "        sheets = [\n",
    "            \"0-1 normalized data\",\n",
    "            \"pca coordinates\",\n",
    "            \"complex scatter\",\n",
    "            \"reproducibility\",\n",
    "            \"protein id alignment\",\n",
    "        ]\n",
    "        mode=\"xlsx\"\n",
    "    \n",
    "    # format outputs\n",
    "    out_dict = dict()\n",
    "    for sheet in sheets:\n",
    "        out_dict[sheet] = benchmark_download_getsheet(sheet, mode=mode)\n",
    "    \n",
    "    # return file object\n",
    "    i_benchmark_download.filename = file_selection\n",
    "    if file_selection.endswith(\".csv\"):\n",
    "        sio = StringIO()\n",
    "        out_dict[sheets[0]].to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        i_benchmark_download.loading=False\n",
    "        i_benchmark_downloadselector.disabled=False\n",
    "        return sio\n",
    "    elif file_selection.endswith(\".xlsx\"):\n",
    "        bio = BytesIO()\n",
    "        excel = pd.ExcelWriter(bio, engine_kwargs = {\"data_only\": True})\n",
    "        for sheet, df in out_dict.items():\n",
    "            df.reset_index().T.reset_index().T.to_excel(\n",
    "                excel, sheet_name=sheet,\n",
    "                merge_cells=False, index=False, header=False)\n",
    "        excel.save()\n",
    "        bio.seek(0)\n",
    "        i_benchmark_download.loading=False\n",
    "        i_benchmark_downloadselector.disabled=False\n",
    "        return bio\n",
    "i_benchmark_download.callback = benchmark_download\n",
    "\n",
    "#### Callback output positioning\n",
    "#### Download data\n",
    "################################\n",
    "o_benchmark_downloadpreview.append(benchmark_download_preview)\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusters_for_ranking.param.value,\n",
    "            i_scatter_metric.param.value, i_scatter_consolidation.param.value, i_scatter_quantile.param.value,\n",
    "            cache_run_json.param.value)\n",
    "def update_benchmark_overview(multi_choice, clusters,\n",
    "                              metric, consolidation, quantile,\n",
    "                              run_json):\n",
    "    try:\n",
    "        if run_json == False:\n",
    "            return \"Run analysis first!\"\n",
    "        if multi_choice == []:\n",
    "            return \"Please select experiments for comparison\"\n",
    "        if \"reproducibility consolidation\" not in i_class_comp.parameters.keys() or \\\n",
    "        i_class_comp.parameters[\"reproducibility metric\"] != metric or \\\n",
    "        i_class_comp.parameters[\"reproducibility consolidation\"] != consolidation:\n",
    "            i_class_comp.calculate_global_scatter(metric, consolidation)\n",
    "        if len(clusters) == 0:\n",
    "            return \"Please select clusters for comparison in the intramap scatter tab\"\n",
    "        \n",
    "        fig = i_class_comp.plot_overview(multi_choice, clusters, quantile)\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Pane(fig, config=plotly_config),\n",
    "            \"Note that this overview figure is affected by the settings you choose in the intra- and inter-map scatter tabs.\"\n",
    "        )\n",
    "    except:\n",
    "        return traceback.format_exc()\n",
    "\n",
    "\n",
    "comparison_tabs.clear()\n",
    "comparison_tabs.append((\"Overview\", pn.Column(\n",
    "    \"Once you have run the analysis you can find different benchmarking outputs here and dive into the data.\",\n",
    "    update_benchmark_overview\n",
    ")))\n",
    "comparison_tabs.append((\"PCA maps\", update_visualization_map_comparison))\n",
    "comparison_tabs.append((\"Depth & Coverage\", pn.Column(update_npr_ngg_nprDc, update_venn)))\n",
    "comparison_tabs.append((\"Intermap scatter\", update_global_scatter_comparison))\n",
    "comparison_tabs.append((\"Intramap scatter\", comparison_tab_bp))\n",
    "comparison_tabs.append((\"SVM Analysis\", lo_benchmark_SVMs))\n",
    "comparison_tabs.append((\"Compare profiles\", update_profile_comparison))\n",
    "comparison_tabs.append((\"Download data\", lo_benchmark_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_benchmark.objects = [\n",
    "    pn.Card(objects=[], header=\"## Manage data\", name=\"manage_data\", height_policy=\"fit\"),\n",
    "    pn.Row(objects=[], name=\"benchmark_output\", height_policy=\"fit\")\n",
    "]\n",
    "\n",
    "#### Manage data Card layout\n",
    "# This accesses mem_available_datasets and mem_benchmark.\n",
    "####\n",
    "\n",
    "## Adding datasets row\n",
    "i_upload_collection = pn.widgets.FileInput(name=\"Upload collection\")\n",
    "btn_load_reference = pn.widgets.Button(name=\"Load\", width=100)\n",
    "i_load_reference = pn.widgets.Select(options=pkg_resources.resource_listdir(\"domaps\", \"referencedata\"),\n",
    "                                     value=None, width=200)\n",
    "lo_add_datasets = pn.Row(objects=[\n",
    "    \"**Upload collection from file (.json):**\", i_upload_collection,\n",
    "    \"**Add reference set:**\", i_load_reference, btn_load_reference\n",
    "])\n",
    "\n",
    "## Selection checkbox\n",
    "i_dfs_available = pn.widgets.CheckBoxGroup(options=[], value=[],\n",
    "                                           name=\"Datasets available\")\n",
    "lo_dfs_available = pn.WidgetBox(objects=[\"**Datasets available**\", i_dfs_available], width=250)\n",
    "\n",
    "## Management button group\n",
    "i_coll_download = pn.widgets.FileDownload(label=\"Download selected as collection (.json)\", filename=\"AnalysedDatasets.json\", disabled=True)\n",
    "btn_coll_editnames = pn.widgets.Button(name=\"Edit names and comments\", disabled=True) # move from management\n",
    "btn_coll_reannotate = pn.widgets.Button(name=\"Reannotate genes/organelles/complexes\", disabled=True) # new functionality\n",
    "btn_coll_runmain = pn.widgets.Button(name=\"Align and analyse selected datasets\",\n",
    "                                     button_type=\"success\", disabled=True) # change from main comparison\n",
    "btn_coll_dropmem = pn.widgets.Button(name=\"Drop selected datasets from memory\",\n",
    "                                     button_type=\"danger\", disabled=True) # move from top of page\n",
    "lo_coll_buttons = pn.WidgetBox(objects=[\n",
    "    i_coll_download,\n",
    "#    btn_coll_editnames,\n",
    "#    btn_coll_reannotate,\n",
    "    btn_coll_runmain,\n",
    "    btn_coll_dropmem\n",
    "], width=300)\n",
    "\n",
    "## Interaction pane\n",
    "lo_instructions_datamanagement = pn.Card(pn.pane.Markdown(textfragments[\"coll_status_default\"]),\n",
    "                                         header=\"**Explanation**\", width=400, collapsed=True)\n",
    "\n",
    "lo_instructions_error_messages = pn.Card(pn.pane.Markdown(textfragments[\"benchmark_error_messages\"]),\n",
    "                                         header=\"**Common error messages**\", width=400, collapsed=True)\n",
    "\n",
    "\n",
    "o_status_datamanagement = pn.pane.Markdown(width=400)\n",
    "def set_status_datamanagement(x, append=False):\n",
    "    if not append:\n",
    "        o_status_datamanagement.object = x+\"<br><br>\"\n",
    "    else:\n",
    "        o_status_datamanagement.object += x+\"<br><br>\"\n",
    "    if x.startswith(\"Traceback\") or o_status_datamanagement.object.count(\"<br><br>\") > 1:\n",
    "        resize(dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"manage_data\")])\n",
    "    if DEBUG:\n",
    "        time.sleep(0.3)\n",
    "    o_status_datamanagement.object = x\n",
    "\n",
    "set_status_datamanagement(\"Step 1: Add datasets\")\n",
    "o_dynamic_collectionmanagement = pn.Row()\n",
    "lo_coll_interactions = pn.Column(objects=[lo_instructions_datamanagement,\n",
    "                                          lo_instructions_error_messages,\n",
    "                                          o_status_datamanagement,\n",
    "                                          o_dynamic_collectionmanagement])\n",
    "\n",
    "## Assemble collection management row\n",
    "lo_manage_collection = pn.Row(objects=[lo_dfs_available, lo_coll_buttons, lo_coll_interactions])\n",
    "\n",
    "#### Append elements to manage data row\n",
    "dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"manage_data\")].objects = []\n",
    "for el in [lo_add_datasets, lo_manage_collection]:\n",
    "    dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"manage_data\")].append(el)\n",
    "\n",
    "#### Management callbacks\n",
    "# upload_collection #Done\n",
    "# load_reference\n",
    "# coll_activatebuttons # Done\n",
    "# coll_downloadjson\n",
    "# coll_editnames\n",
    "# coll_reannotate\n",
    "# coll_runmain #Button change in place\n",
    "# coll_dropmem #Done\n",
    "####\n",
    "\n",
    "lock_collection_change = [\n",
    "    #btn_coll_editnames,\n",
    "    #btn_coll_reannotate,\n",
    "    i_coll_download,\n",
    "    btn_coll_runmain,\n",
    "    btn_coll_dropmem,\n",
    "    i_dfs_available,\n",
    "    i_upload_collection,\n",
    "    btn_load_reference\n",
    "]\n",
    "\n",
    "@pn.depends(i_upload_collection.param.value, watch=True)\n",
    "def upload_collection(file):\n",
    "    \"\"\"\n",
    "    This callback adds the datasets from a .json collection to the global memory object\n",
    "    and updates interface elements accordingly.\n",
    "    \"\"\"\n",
    "    if file == None:\n",
    "        return\n",
    "    # deactivate interface\n",
    "    for el in lock_collection_change:\n",
    "        el.disabled = True\n",
    "    try:\n",
    "        set_status_datamanagement(\"Loading data ...\")\n",
    "        \n",
    "        status = \"\"\n",
    "        json_loaded = json.load(BytesIO(file))\n",
    "        \n",
    "        # Check if experiment names are still free\n",
    "        renamed_exps = []\n",
    "        n_sets = 0\n",
    "        keys = list(json_loaded.keys())\n",
    "        for exp in keys:\n",
    "            if exp in i_dfs_available.options:\n",
    "                renamed_exps.append(exp)\n",
    "                json_loaded[exp+i_upload_collection.filename] = json_loaded.pop(exp)\n",
    "            n_sets += 1\n",
    "        if len(renamed_exps) != 0:\n",
    "            status += f\"\"\"These datasets were already available:<br><br>{\", \".join(renamed_exps)}\n",
    "            <br><br>Please rename them prior to analysis.<br><br><br><br>\"\"\"\n",
    "        \n",
    "        # Load datasets into memory\n",
    "        keys = list(json_loaded.keys())\n",
    "        for exp in keys:\n",
    "            set_status_datamanagement(f\"Loading dataset {exp} ...\")\n",
    "            mem_available_datasets[exp] = json_loaded[exp]\n",
    "        \n",
    "        # Adjust list of available dataset\n",
    "        i_dfs_available.options = i_dfs_available.options + keys\n",
    "        i_dfs_available.value = i_dfs_available.value + keys\n",
    "        resize(lo_dfs_available)\n",
    "        \n",
    "        status += f\"Loaded **{n_sets}** datasets from file **{i_upload_collection.filename}**\"\n",
    "        set_status_datamanagement(status)\n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "    finally:\n",
    "        # reactivate interface\n",
    "        for el in lock_collection_change:\n",
    "            if type(el) != pn.widgets.button.Button:\n",
    "                el.disabled = False\n",
    "        coll_activatebuttons(i_dfs_available.value)\n",
    "\n",
    "\n",
    "def load_reference(event):\n",
    "    upload_collection(pkg_resources.resource_stream(\"domaps\", \"referencedata/\"+i_load_reference.value).read())\n",
    "btn_load_reference.on_click(load_reference)\n",
    "\n",
    "\n",
    "@pn.depends(i_dfs_available.param.value, watch=True)\n",
    "def coll_activatebuttons(v):\n",
    "    \"\"\"\n",
    "    Activate/Deactivate buttons based on selection of available datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn_load_reference.disabled = False\n",
    "        if len(v) == 0:\n",
    "            i_coll_download.disabled = True\n",
    "            btn_coll_reannotate.disabled = True\n",
    "            btn_coll_runmain.disabled = True\n",
    "            btn_coll_dropmem.disabled = True\n",
    "        else:\n",
    "            btn_coll_reannotate.disabled = False\n",
    "            i_coll_download.disabled = False\n",
    "            btn_coll_runmain.disabled = False\n",
    "            btn_coll_dropmem.disabled = False\n",
    "        if len(i_dfs_available.options) == 0:\n",
    "            btn_coll_editnames.disabled = True\n",
    "        else:\n",
    "            btn_coll_editnames.disabled = False\n",
    "            pass\n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "\n",
    "@pn.depends(i_dfs_available.param.value)\n",
    "def coll_downloadjson(dfs):\n",
    "    sio = StringIO()\n",
    "    json.dump(\n",
    "        {k: mem_available_datasets[k] for k in dfs}, \n",
    "        sio,\n",
    "        indent=4, \n",
    "        sort_keys=True\n",
    "    )\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "i_coll_download.callback = coll_downloadjson\n",
    "\n",
    "\n",
    "def coll_runmain(event):\n",
    "    \"\"\"\n",
    "    Runs main analysis\n",
    "    \"\"\"\n",
    "    # deactivate interface\n",
    "    for el in lock_collection_change:\n",
    "        el.disabled = True\n",
    "    try:\n",
    "        if btn_coll_runmain.button_type == \"success\":\n",
    "            set_status_datamanagement(\"Aligning and analysing data ...\")\n",
    "            cache_run_json.value=False\n",
    "            #### Main execution of the comparison\n",
    "            loading_status_comparison.objects = [loading_comparison]\n",
    "            selection = i_dfs_available.value\n",
    "            global i_class_comp\n",
    "            i_class_comp = domaps.SpatialDataSetComparison(ref_exp=selection[0])#, clusters_for_ranking=protein_cluster, organism=i_organism_comparison.value)\n",
    "            i_class_comp.json_dict = {k: mem_available_datasets[k] for k in selection}\n",
    "            set_status_datamanagement(\"Aligning data ...\", append=True)\n",
    "            i_class_comp.read_jsonFile()\n",
    "            set_status_datamanagement(\"Analysing intra-map scatter ...\", append=True)\n",
    "            i_class_comp.calc_biological_precision()\n",
    "            i_class_comp.get_complex_coverage()\n",
    "            update_multi_choice()\n",
    "            set_status_datamanagement(\"Running PCA ...\", append=True)\n",
    "            i_class_comp.perform_pca_comparison()\n",
    "            i_pca_comp_ncomp.end=len(i_class_comp.df_01_filtered_combined.columns)\n",
    "            i_compare_compartment.options = list(set(\n",
    "                i_class_comp.df_01_filtered_combined.index.get_level_values(\"Compartment\")))\n",
    "            loading_status_comparison.objects = []\n",
    "            m_diverget_fractions.object = \"\" if not i_class_comp.mixed else \"**Caution: You are comparing experiments with differently labelled fractions. This does not affect distance metrics, but the PCA and profile plots.**\"\n",
    "            m_diverget_fractions.background = None if not i_class_comp.mixed else \"salmon\"\n",
    "            cache_run_json.value=True\n",
    "            set_status_datamanagement(\"Comparison finished!\", append=True)\n",
    "            \n",
    "            #### Switch button mode\n",
    "            btn_coll_runmain.button_type = \"danger\"\n",
    "            btn_coll_runmain.name = \"Reset analysis to make new selection\"\n",
    "            set_status_datamanagement(\n",
    "                \"Next step: Use interface below to evaluate and download benchmark results.\", append=True)\n",
    "        elif btn_coll_runmain.button_type == \"danger\":\n",
    "            set_status_datamanagement(\"Resetting data analysis ...\")\n",
    "            cache_run_json.value=False\n",
    "            \n",
    "            btn_coll_runmain.button_type = \"success\"\n",
    "            btn_coll_runmain.name = \"Align and analyse selected datasets\"\n",
    "            set_status_datamanagement(\"Analysis results have been reset.\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "        cache_run_json.value=False\n",
    "        \n",
    "    finally:\n",
    "        # reactivate interface\n",
    "        if btn_coll_runmain.button_type == \"danger\":\n",
    "            for el in [\n",
    "                i_coll_download,\n",
    "                btn_coll_runmain,\n",
    "                btn_coll_editnames\n",
    "            ]:\n",
    "                el.disabled = False\n",
    "        elif btn_coll_runmain.button_type == \"success\":\n",
    "            for el in lock_collection_change:\n",
    "                if type(el) != pn.widgets.button.Button:\n",
    "                    el.disabled = False\n",
    "            coll_activatebuttons(i_dfs_available.value)\n",
    "        else:\n",
    "            for el in lock_collection_change:\n",
    "                if type(el) != pn.widgets.button.Button:\n",
    "                    el.disabled = False\n",
    "            coll_activatebuttons(i_dfs_available.value)\n",
    "            \n",
    "btn_coll_runmain.on_click(coll_runmain)\n",
    "        \n",
    "def coll_dropmem(event):\n",
    "    \"\"\"\n",
    "    Drops selected datasets from collection stored in RAM.\n",
    "    \"\"\"\n",
    "    # deactivate interface\n",
    "    for el in lock_collection_change:\n",
    "        el.disabled = True\n",
    "    try:\n",
    "        set_status_datamanagement(\"Deleting data ...\")\n",
    "        \n",
    "        keys = list(i_dfs_available.value)\n",
    "        for exp in keys:\n",
    "            set_status_datamanagement(f\"Deleting dataset {exp} ...\")\n",
    "            del mem_available_datasets[exp]\n",
    "            \n",
    "        # Adjust list of available dataset\n",
    "        i_dfs_available.options = [el for el in i_dfs_available.options if el not in keys]\n",
    "        i_dfs_available.value = [el for el in i_dfs_available.value if el not in keys]\n",
    "        resize(lo_dfs_available)\n",
    "        i_dfs_available.disabled = True\n",
    "        \n",
    "        set_status_datamanagement(f\"Deleted **{len(keys)}** datasets **{', '.join(keys)}**\")\n",
    "        \n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "    finally:\n",
    "        # reactivate interface\n",
    "        for el in lock_collection_change:\n",
    "            if type(el) != pn.widgets.button.Button:\n",
    "                el.disabled = False\n",
    "        coll_activatebuttons(i_dfs_available.value)\n",
    "btn_coll_dropmem.on_click(coll_dropmem)\n",
    "\n",
    "#### Benchmark output\n",
    "@pn.depends(cache_run_json.param.value)\n",
    "def display_benchmark_output(run_json):\n",
    "    if run_json:\n",
    "        return pn.Column(\n",
    "            \"## Benchmark results\",\n",
    "            \"Select dataset overlap to plot:\",\n",
    "            i_multi_choice,\n",
    "            comparison_tabs\n",
    "        )\n",
    "    else:\n",
    "        return \"Select data and run analysis.\"\n",
    "\n",
    "#### Append callback output to benchmark output\n",
    "dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"benchmark_output\")].objects = []\n",
    "for el in [display_benchmark_output]:\n",
    "    dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"benchmark_output\")].append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case of loading a json comparison larger than 80 MB\n",
    "#with open(\"G:\\_DIA manuscript\\Figure panes and data\\Figure 1\\Figure1_v2_peptides.json\", \"br\") as file:\n",
    "#    i_jsonFile.value = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case of loading a dingle file larger than 80 MB\n",
    "#i_FileConfig._content.file.filename = \"proteinGroups.txt\"\n",
    "#with open(r\"path\\proteinGroups.txt\", \"br\") as file:\n",
    "#    i_FileConfig._content.file.value = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the order of the multi choice widget manually\n",
    "#i_multi_choice.value=[\"21 min\", \"44 min\", \"100 min\", \"DDA\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
