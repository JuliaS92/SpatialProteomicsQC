{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents<a id=\"TOC\"></a>\n",
    "1. [Library imports](#libraries)\n",
    "2. [panel and plotly settings and customization](#styling)\n",
    "3. [Global variables](#globals)\n",
    "4. [App interface skeleton](#skeleton)\n",
    "5. [App serving](#serving)\n",
    "6. [Cell structuring](#structuring)\n",
    "7. [Home tab](#home)\n",
    "8. [Analysis tab](#analysis)\n",
    "9. [Comparison output tabs](#comparison)\n",
    "10. [Benchmark tab upload section](#benchmarkupload)\n",
    "11. [Code interactions](#interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Library imports<a id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import re\n",
    "import traceback\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "pn.extension(\"plotly\")\n",
    "import io\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "from bokeh.models.widgets.tables import NumberFormatter\n",
    "import plotly.express as px\n",
    "import json\n",
    "import os\n",
    "from importlib import reload\n",
    "import pkg_resources\n",
    "import time\n",
    "import copy\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import domaps.gui as gui\n",
    "\n",
    "try:\n",
    "    type(domaps)\n",
    "    print(\"reloading library\")\n",
    "    domaps = reload(domaps)\n",
    "except Exception as ex:\n",
    "    print(\"loading library first time\")\n",
    "    import domaps\n",
    "    \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## panel and plotly settings and customization<a id=\"styling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".detail_menu .bk-headers-wrapper{\n",
    "  border-bottom: 2px solid #0a5da8 !important;\n",
    "  margin-bottom: 10px;\n",
    "  min-width: 800px;\n",
    "  max-width: 2000px !important;\n",
    "}\n",
    ".detail_menu .bk-tab{\n",
    "  color: #0a5da8;\n",
    "}\n",
    ".detail_menu .bk-active{\n",
    "  border-color: #0a5da8 !important;\n",
    "  border-width: 2px 1px 0px 1px !important;\n",
    "  color: #111111 !important;\n",
    "}\n",
    "\n",
    ".main_menu .bk-headers-wrapper{\n",
    "  border-bottom: 2px solid #0a5da8 !important;\n",
    "  margin-bottom: 10px;\n",
    "  min-width: 800px;\n",
    "  max-width: 2000px !important;\n",
    "}\n",
    ".main_menu .bk-tab{\n",
    "  color: #0a5da8;\n",
    "  font-size: 120%;\n",
    "  font-weight: bold;\n",
    "}\n",
    ".main_menu .bk-active{\n",
    "  border-color: #0a5da8 !important;\n",
    "  border-width: 2px 1px 0px 1px !important;\n",
    "  color: #111111 !important;\n",
    "}\n",
    "\n",
    ".bk-tabs-header{\n",
    "  min-width: 1000px;\n",
    "}\n",
    "\n",
    ".card-title:first-child{\n",
    "  font-size: 13px;\n",
    "}\n",
    "\n",
    ".button-main .bk-btn{\n",
    "  font-size: 120%;\n",
    "  font-weight: bold;\n",
    "}\n",
    "\n",
    "body{\n",
    "  margin:0;\n",
    "}\n",
    "\"\"\"\n",
    "pn.extension(raw_css=[css])\n",
    "\n",
    "plotly_config={\n",
    "      'toImageButtonOptions': {\n",
    "            'format': 'svg', # one of png, svg, jpeg, webp\n",
    "            'filename': 'figure'\n",
    "      }\n",
    "}\n",
    "\n",
    "def resize(el):\n",
    "    try:\n",
    "        el.append(None)\n",
    "        el.pop(-1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Global variables<a id=\"globals\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data that will be stored in memory during each session\n",
    "## dataset analysed and displayed in the single analysis tab\n",
    "mem_single_analysis = None\n",
    "i_class = None\n",
    "## dataset collection analysed and displayed in the benchmarking tab\n",
    "mem_benchmark = None\n",
    "i_class_comp = None\n",
    "## currently available datasets to select for benchmarking\n",
    "mem_available_datasets = dict()\n",
    "\n",
    "with open(\"textfragments.json\", \"r\") as file:\n",
    "    textfragments = json.load(file)\n",
    "\n",
    "DEBUG = False\n",
    "MAX_SIZE_MB = 80\n",
    "CONTENT_WIDTH = 1000\n",
    "HOSTING = \"online\" #switch to local to remove limits on iterations and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## App interface skeleton<a id=\"skeleton\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Served panel object:\n",
    "app = pn.GridSpec(sizing_mode=\"stretch_both\", margin=0)\n",
    "app[0,0] = pn.Spacer(background=\"#DDDDDD\", margin=0) #\n",
    "app[0,16] = pn.Spacer(background=\"#DDDDDD\", margin=0) #\n",
    "\n",
    "#### Insert main content container\n",
    "app_center = pn.Column(pn.Row(pn.Pane(\"# QC tool for Spatial Proteomics\", width = 600),\n",
    "                              pn.layout.HSpacer(),\n",
    "                              margin=10),\n",
    "                       pn.Row(name=\"main_content\", margin=50)\n",
    "                      )\n",
    "app[0,1:15] = app_center\n",
    "\n",
    "#### Insert main menu tab object\n",
    "app_tabs = pn.Tabs(margin=10, css_classes=[\"main_menu\"], dynamic=True, sizing_mode=\"stretch_width\")\n",
    "app_center.objects[[i.name for i in app_center].index(\"main_content\")] = app_tabs\n",
    "\n",
    "#### Append individual dashboards\n",
    "## Home\n",
    "dashboard_home = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"home\", sizing_mode=\"stretch_width\"\n",
    ")\n",
    "app_tabs.append((\"Home\", dashboard_home))\n",
    "\n",
    "## Single analysis\n",
    "dashboard_analysis = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"analysis\", sizing_mode=\"stretch_width\"\n",
    ")\n",
    "app_tabs.append((\"Analysis\", dashboard_analysis))\n",
    "analysis_tabs = pn.Tabs(margin=10, css_classes=[\"detail_menu\"], dynamic=True, sizing_mode=\"stretch_width\")\n",
    "\n",
    "## Benchmark\n",
    "dashboard_benchmark = pn.Column(\n",
    "    \"Interface loading ...\",\n",
    "    name=\"benchmark\", sizing_mode=\"stretch_width\"\n",
    ")\n",
    "app_tabs.append((\"Benchmark\", dashboard_benchmark))\n",
    "comparison_tabs = pn.Tabs(margin=10, css_classes=[\"detail_menu\"], dynamic=True, sizing_mode=\"stretch_width\")\n",
    "\n",
    "## About\n",
    "app_tabs.append((\"About\", pn.Row(pn.Pane(textfragments[\"about_intro\"], sizing_mode=\"stretch_width\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## App serving<a id=\"serving\"></a>\n",
    "Switch cells below between markup and code to set up for server hosting from the command line (app.servable) vs. local hosting from python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    server.stop()\n",
    "except Exception:\n",
    "    print(\"First server startup\")\n",
    "server = app.show(port=5067, websocket_max_message_size=MAX_SIZE_MB*1024*1024, admin=True,\n",
    "                  http_server_kwargs={'max_buffer_size': MAX_SIZE_MB*1024*1024})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Cell structuring<a id=\"structuring\"></a>\n",
    "All cells below contain one or several sets of these points:\n",
    "- (Dashboard structure)\n",
    "- Layout and widget elements (outside in)\n",
    "- Layout assembly and appending\n",
    "- Callback definitions\n",
    "- Positioning of callback outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "########################\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# list\n",
    "# of\n",
    "# callbacks\n",
    "\n",
    "#### Callback output positioning\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Home tab<a id=\"home\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "########################\n",
    "# already defined as single column\n",
    "dashboard_home.objects = []\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "lo_home_intro = pn.Pane(textfragments[\"home_intro\"], width=CONTENT_WIDTH)\n",
    "btn_home_analysesingle = pn.widgets.Button(name=\"Format and analyse single experiment\",\n",
    "                                           button_type=\"success\", width=400, height=50,\n",
    "                                           css_classes=[\"button-main\"])\n",
    "lo_home_singleinstructions = pn.Column(\n",
    "    pn.Pane(textfragments[\"home_single_shortinstructions\"], width=CONTENT_WIDTH),\n",
    "    pn.Card(textfragments[\"quick_start_guide\"], header=\"DOM-QC 1-min Quick Start Guide\", width=CONTENT_WIDTH,\n",
    "            name=\"tutorial_single\", collapsed=True)\n",
    ")\n",
    "btn_home_benchmark = pn.widgets.Button(name=\"Benchmark multiple experiments\",\n",
    "                                       button_type=\"success\", width=400, height=50,\n",
    "                                       css_classes=[\"button-main\"])\n",
    "lo_home_benchmarkinstructions = pn.Column(\n",
    "    pn.Pane(textfragments[\"home_benchmark_shortinstructions\"], width=CONTENT_WIDTH),\n",
    "#    pn.Card(\"Add screenshot tutorial here.\", header=\"Tutorial\", width=CONTENT_WIDTH,\n",
    "#            name=\"tutorial_benchmark\", collapsed=True)\n",
    ")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "for el in [lo_home_intro,\n",
    "           btn_home_analysesingle, lo_home_singleinstructions,\n",
    "           btn_home_benchmark, lo_home_benchmarkinstructions]:\n",
    "    dashboard_home.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# home_gotosingleanalysis\n",
    "# home_gotobenchmark\n",
    "\n",
    "def home_gotosingleanalysis(event):\n",
    "    app_tabs.active = [el.name for el in app_tabs].index(\"analysis\")\n",
    "btn_home_analysesingle.on_click(home_gotosingleanalysis)\n",
    "\n",
    "def home_gotobenchmark(event):\n",
    "    app_tabs.active = [el.name for el in app_tabs].index(\"benchmark\")\n",
    "btn_home_benchmark.on_click(home_gotobenchmark)\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Analysis tab<a id=\"analysis\"></a>\n",
    "- File config\n",
    "- Analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "########################\n",
    "dashboard_analysis.objects = [\n",
    "    pn.Row(name=\"file_config\"),\n",
    "    pn.Column(name=\"analysis_output\", sizing_mode=\"stretch_width\")\n",
    "]\n",
    "\n",
    "#### Layout elements\n",
    "#### File config\n",
    "####################\n",
    "#lo_read_file = pn.Card(header=\"### Upload configuration\", min_width=400)\n",
    "lo_config_instructions = pn.Card(\n",
    "    pn.Pane(textfragments[\"upload_instructions\"]), header=\"### Instructions\", width=400)\n",
    "lo_config_details = pn.Card(\n",
    "    pn.Pane(textfragments[\"upload_details\"]), header=\"### Details on configuring your data\", width=400, collapsed = True)\n",
    "lo_config_error_messages = pn.Card(\n",
    "    pn.Pane(textfragments[\"upload_error_messages\"]), header=\"### Common error messages\", width=400, collapsed = True)\n",
    "\n",
    "loading_status = pn.Row()\n",
    "idle = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "analysis_status = pn.Pane(\"\", width=300)\n",
    "filereading_status = pn.Pane(\"No data import yet\", width=300)\n",
    "i_FileConfig = gui.ConfigureSingleFile(width=540)\n",
    "#lo_read_file.append(i_FileConfig)\n",
    "\n",
    "#### Append layout to dashboard\n",
    "#### File config\n",
    "###############################\n",
    "dashboard_analysis.objects[[el.name for el in dashboard_analysis].index(\"file_config\")].objects = []\n",
    "for el in [\n",
    "    pn.Column(i_FileConfig, analysis_status, loading_status, width=600),\n",
    "    pn.Column(\n",
    "        lo_config_instructions,\n",
    "        lo_config_details,\n",
    "        # lo_config_error_messages # currently empty\n",
    "    )\n",
    "]:\n",
    "    dashboard_analysis.objects[[el.name for el in dashboard_analysis].index(\"file_config\")].append(el)\n",
    "\n",
    "#### Callbacks\n",
    "#### File config\n",
    "################\n",
    "# future_execution\n",
    "\n",
    "cache_uploaded = pn.widgets.Checkbox(value=False)\n",
    "cache_run = pn.widgets.Checkbox(value=False)\n",
    "#define widgets that should be disbled after run==True\n",
    "#wdgts = [i_FileConfig]\n",
    "\n",
    "def execution(event):\n",
    "    loading_status.objects = [loading]\n",
    "    analysis_status.object = \"Analysis in progress\"\n",
    "    lo_config_instructions.collapsed = True\n",
    "    lo_config_details.collapsed = True\n",
    "    #lo_config_error_messages.collapsed = True\n",
    "    #lo_read_file.collapsed = True\n",
    "    output_layoutpos = [el.name for el in dashboard_analysis].index(\"analysis_output\")\n",
    "    dashboard_analysis.objects[output_layoutpos].objects = []\n",
    "    cache_run.value = False\n",
    "    try:\n",
    "        \n",
    "        global i_class\n",
    "        i_class = domaps.SpatialDataSet.from_settings(i_FileConfig.get_settings(), legacy=False)\n",
    "        i_class.run_pipeline(content=BytesIO(i_FileConfig._content.file.value), progressbar=analysis_status)\n",
    "        \n",
    "        analysis_status.object = \"Analysis finished!\"\n",
    "        update_object_selector()\n",
    "        loading_status.objects = []\n",
    "        dashboard_analysis.objects[output_layoutpos].append(analysis_tabs)\n",
    "        mem_available_datasets[i_class.expname] = i_class.analysed_datasets_dict[i_class.expname]\n",
    "        try:\n",
    "            i_dfs_available.options = i_dfs_available.options + [i_class.expname]\n",
    "            i_dfs_available.value = i_dfs_available.value + [i_class.expname]\n",
    "            coll_activatebuttons(i_dfs_available.value)\n",
    "            resize(lo_dfs_available)\n",
    "        except:\n",
    "            pass\n",
    "        cache_run.value = True\n",
    "    except:\n",
    "        loading_status.objects = [\"\"]\n",
    "        analysis_status.object = traceback.format_exc()\n",
    "        cache_run.value = False\n",
    "\n",
    "i_FileConfig._btn_run.on_click(execution)\n",
    "\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Analysis output\n",
    "########################\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "i_logOR01_selection = pn.widgets.Select(options=[\"0/1 normalized data\", \"log transformed data\",\n",
    "                                                 \"stringency filtered raw data\", \"Overview - cluster distances\"],\n",
    "                                        name=\"Select type of data for download\", width=300)\n",
    "\n",
    "i_clusterwidget = pn.widgets.Select(options=[\"Proteasome\", \"Lysosome\"], name=\"Cluster of interest\", width=300)\n",
    "i_mapwidget = pn.widgets.Select(options=[\"Map1\", \"Map2\"], name=\"Map of interest\", width=300)\n",
    "\n",
    "i_collapse_maps_PCA = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "i_pca_ncomp = pn.widgets.IntSlider(value=3, start=2, end=5)\n",
    "btn_pca_ncomp = pn.widgets.Button(name=\"Recalculate PCA with different number of components.\")\n",
    "\n",
    "#### Correlation plot in single analysis tab\n",
    "i_corr_mode = pn.widgets.Select(options=[\"heatmap\", \"scatter\"], name=\"Show correlations as\")\n",
    "i_corr_measure = pn.widgets.Select(options=[\"Pearson\", \"Spearman\"], name=\"Correlation metric\")\n",
    "i_corr_selection = pn.widgets.Select(options=[\"all samples\"], name=\"Select fraction(s)\")\n",
    "i_corr_level = pn.widgets.Select(options=[\"original data\", \"filtered and processed data\", \"normalized profiles\"],\n",
    "                                 value=\"filtered and processed data\",\n",
    "                                 name=\"Show dataset\")\n",
    "lo_corr_widgets = pn.WidgetBox(i_corr_mode, i_corr_measure, i_corr_selection, i_corr_level)\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# update_corr_selection\n",
    "# show_correlation_single\n",
    "# update_object_selector\n",
    "# recalculate_pca\n",
    "# update_data_overview # tab content\n",
    "# update_cluster_overview # tab content\n",
    "# update_cluster_details\n",
    "# update_quantity # tab content\n",
    "# show_tabular_overview\n",
    "# json_download\n",
    "# df01_download_widget\n",
    "# df01_download\n",
    "# dflog_download\n",
    "# df_filteredRawData_download\n",
    "# table_download # tab content\n",
    "\n",
    "\n",
    "@param.depends(cache_run.param.value, watch=True)\n",
    "def update_corr_selection(run):\n",
    "    if run == True:\n",
    "        i_corr_selection.options = [i_corr_selection.options[0]]+i_class.fractions\n",
    "\n",
    "@param.depends(i_corr_mode.param.value, i_corr_measure.param.value,\n",
    "               i_corr_selection.param.value, i_corr_level.param.value,\n",
    "               cache_run.param.value)\n",
    "def show_correlation_single(mode, measure, selection, level, run):\n",
    "    if run == True:\n",
    "        try:\n",
    "            if mode == \"scatter\" and selection == \"all samples\":\n",
    "                return \"Please select only one fraction in scatter mode.\"\n",
    "            if level == \"original data\":\n",
    "                df = i_class.df_index[i_class.mainset]\n",
    "            elif level == \"filtered and processed data\":\n",
    "                df = i_class.df_log_stacked[\"log profile\"].unstack([\"Map\", \"Fraction\"])\n",
    "            elif level == \"normalized profiles\":\n",
    "                df = i_class.df_01_stacked[\"normalized profile\"].unstack([\"Map\", \"Fraction\"])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown data level {level} for correlation plot.\")\n",
    "            df = df.sort_index(axis=1, level=\"Fraction\")\n",
    "            if selection != \"all samples\":\n",
    "                df = df.xs(selection, level=\"Fraction\", axis=1, drop_level=False)\n",
    "            \n",
    "            df.columns = [\"_\".join(el) for el in df.columns]\n",
    "            \n",
    "            measure_dict = {\n",
    "                \"Spearman\": lambda x: spearmanr(x.values).correlation,\n",
    "                \"Pearson\": lambda x: np.corrcoef(x.T)\n",
    "            }\n",
    "            \n",
    "            plot = domaps.plot_sample_correlations(\n",
    "                df, data_columns=\"(.*)\",\n",
    "                log10=False if level != \"original data\" else True,\n",
    "                binning=10 if level != \"normalized profiles\" else 50,\n",
    "                mode=mode,\n",
    "                correlation_function=measure_dict[measure]\n",
    "            )\n",
    "            return plot\n",
    "    \n",
    "        except:\n",
    "            return traceback.format_exc()\n",
    "lo_corr = pn.Row(lo_corr_widgets, show_correlation_single)\n",
    "\n",
    "def update_object_selector():\n",
    "    i_mapwidget.options = list(i_class.map_names)\n",
    "    i_clusterwidget.options = list(i_class.markerproteins.keys())\n",
    "    i_pca_ncomp.end = len(i_class.fractions)\n",
    "\n",
    "\n",
    "def recalculate_pca(event):\n",
    "    cache_run.value=False\n",
    "    i_class.perform_pca(n=i_pca_ncomp.value)\n",
    "    cache_run.value=True\n",
    "btn_pca_ncomp.on_click(recalculate_pca)\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value,\n",
    "            i_collapse_maps_PCA.param.value)\n",
    "def update_data_overview(run, collapse_maps_PCA):\n",
    "    try:\n",
    "        if run == True:\n",
    "            \n",
    "            compartments = i_class.df_organellarMarkerSet[\"Compartment\"].unique()\n",
    "            compartment_color = dict(zip(compartments, i_class.css_color))\n",
    "            compartment_color[\"undefined\"] = \"lightgrey\"\n",
    "            \n",
    "            pca_plot = gui.pca_plot(\n",
    "                df_pca=i_class.df_pca if not collapse_maps_PCA else i_class.df_pca_combined,\n",
    "                df_var=i_class.df_pca_var if not collapse_maps_PCA else i_class.df_pca_combined_var,\n",
    "                df_loadings=i_class.df_pca_loadings if not collapse_maps_PCA else i_class.df_pca_combined_loadings,\n",
    "                color=\"Compartment\", color_map=compartment_color,\n",
    "                facet_col=\"Map\" if not collapse_maps_PCA else None,\n",
    "                title=\"PCA plot\"\n",
    "            )\n",
    "            pca_plot.highlight_dict=i_class.markerproteins\n",
    "            \n",
    "            log_histogram = i_class.plot_log_data()\n",
    "            visualization_map = pn.Column(\n",
    "                pn.Pane(textfragments[\"analysis_overview_top\"], width=600),\n",
    "                pn.Row(i_collapse_maps_PCA, i_pca_ncomp, btn_pca_ncomp),\n",
    "                pca_plot,\n",
    "                pn.layout.VSpacer(height=3, background=\"#AAAAAA\"),\n",
    "                lo_corr,\n",
    "                pn.layout.VSpacer(height=3, background=\"#AAAAAA\"),\n",
    "                pn.Row(pn.Pane(log_histogram, width=1000, config=plotly_config))\n",
    "            )\n",
    "            app_tabs.active = 1\n",
    "            return visualization_map\n",
    "        else:\n",
    "            visualization_map = \"Run analysis first!\"\n",
    "            return visualization_map\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return pn.Column(\n",
    "            pn.Row(i_collapse_maps_PCA),          \n",
    "            update_status\n",
    "        )\n",
    "\n",
    "\n",
    "@pn.depends(i_clusterwidget.param.value,i_mapwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_overview(clusterwidget, mapwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            list_genes = [goi for goi in i_class.genenames_sortedout_list if goi in i_class.markerproteins[clusterwidget]]\n",
    "            i_class.cache_cluster_quantified = True\n",
    "            distance_boxplot = i_class.distance_boxplot(cluster_of_interest=clusterwidget)\n",
    "            if i_class.cache_cluster_quantified == False:\n",
    "                return pn.Column(\n",
    "                    pn.Row(\n",
    "                        pn.Pane(textfragments[\"analysis_intramap_top\"], width=600),\n",
    "                        pn.Column(width=30),\n",
    "                        pn.Column(i_clusterwidget,i_mapwidget)\n",
    "                    ),\n",
    "                    \"This protein cluster was not quantified\"\n",
    "                )\n",
    "            \n",
    "            else:\n",
    "                df_quantification_overview = i_class.quantification_overview(cluster_of_interest=clusterwidget)\n",
    "                profiles_plot = i_class.profiles_plot(map_of_interest = mapwidget, cluster_of_interest=clusterwidget)\n",
    "                pca_plot = gui.pca_plot(\n",
    "                    df_pca=i_class.df_pca_all_marker_cluster_maps.xs(clusterwidget, level=\"Cluster\", axis=0),\n",
    "                    show_variability=False, show_loadings=False, enable_highlight=False,\n",
    "                    facet_col=None, color=\"Map\", color_map=dict(),\n",
    "                    title=f\"PCA plot of {clusterwidget}\"\n",
    "                )\n",
    "                pca_plot._dimensions.value=\"3D\"\n",
    "                cluster_overview = pn.Column(\n",
    "                    pn.Row(\n",
    "                        pn.Pane(textfragments[\"analysis_intramap_top\"], width=600),\n",
    "                        pn.Column(width=30),\n",
    "                        pn.Column(i_clusterwidget,i_mapwidget)\n",
    "                    ),\n",
    "                    pn.Row(pca_plot,\n",
    "                           pn.Pane(profiles_plot, width=500, config=plotly_config),\n",
    "                           pn.Pane(distance_boxplot, width=500, config=plotly_config),\n",
    "                          ),\n",
    "                    pn.layout.VSpacer(height=3, background=\"#AAAAAA\"),\n",
    "                    pn.Row(pn.Pane(\"In total {} proteins across all maps were quantified, whereas the following proteins were not consistently quantified throughout all maps: {}\".format(\n",
    "                            i_class.proteins_quantified_across_all_maps, \", \".join(list_genes)) if len(list_genes) != 0 else\n",
    "                        \"All genes from this cluster are quantified in all maps.\"), width=1000),\n",
    "                    pn.Row(pn.widgets.DataFrame(df_quantification_overview, height=200, width=500, disabled=True)),\n",
    "                    pn.layout.VSpacer(height=3, background=\"#AAAAAA\"),\n",
    "                    update_cluster_details\n",
    "                )\n",
    "                return cluster_overview\n",
    "        \n",
    "        else:\n",
    "            cluster_overview = \"Run analysis first!\"\n",
    "            return cluster_overview\n",
    "    except Exception:\n",
    "        update_status = pn.Column(\n",
    "            pn.Row(\n",
    "                pn.Pane(textfragments[\"analysis_intramap_top\"], width=600),\n",
    "                pn.Column(width=30),\n",
    "                pn.Column(i_clusterwidget,i_mapwidget)\n",
    "            ),\n",
    "            pn.Pane(traceback.format_exc(), width=600)\n",
    "        )\n",
    "        return update_status\n",
    "\n",
    "\n",
    "@pn.depends(i_clusterwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_details(clusterwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            cluster_details = i_class.distance_to_median_boxplot(cluster_of_interest = clusterwidget)\n",
    "            return pn.Pane(cluster_details, width=1000, config=plotly_config)\n",
    "        else:\n",
    "            cluster_details = \"Run analysis first!\"\n",
    "            return pn.Pane(cluster_details, width=1000)\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_quantity(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            fig_npg, fig_npr, fig_npr_dc, fig_npg_F, fig_npgf_F, fig_npg_F_dc = i_class.plot_quantity_profiles_proteinGroups()\n",
    "            return pn.Column(\n",
    "                pn.Pane(textfragments[\"analysis_depth_top\"], width=600),\n",
    "                pn.Row(\n",
    "                    pn.Pane(fig_npg, config=plotly_config),\n",
    "                    #pn.Pane(fig_npr, config=plotly_config),\n",
    "                    pn.Pane(fig_npr_dc, config=plotly_config)\n",
    "                ),\n",
    "                pn.layout.VSpacer(background=\"#AAAAAA\", height=3),\n",
    "                pn.Row(\n",
    "                    pn.Pane(fig_npg_F, config=plotly_config),\n",
    "                    pn.Pane(fig_npgf_F, config=plotly_config),\n",
    "                    pn.Pane(fig_npg_F_dc, config=plotly_config)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def show_tabular_overview(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            content = pn.Column(\n",
    "                pn.widgets.DataFrame(pd.read_json(i_class.analysed_datasets_dict[i_class.expname][\"Overview table\"]), height=200, width=600, disabled=True),\n",
    "\n",
    "                i_logOR01_selection,\n",
    "                df01_download_widget,\n",
    "                pn.widgets.FileDownload(callback=json_download, filename=\"AnalysedDatasets.json\")\n",
    "            )\n",
    "            return pn.Row(content, width=1000)\n",
    "        else:\n",
    "            content = \"Please, upload a file first and press ‘Analyse clusters’\"\n",
    "            return pn.Row(content, width=1000)\n",
    "    except Exception:\n",
    "        content = traceback.format_exc()\n",
    "        return pn.Row(content, width=1000)\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def json_download(run):\n",
    "    sio = StringIO()\n",
    "    json.dump(\n",
    "        i_class.analysed_datasets_dict, \n",
    "        sio, \n",
    "        indent=4, \n",
    "        sort_keys=True\n",
    "    )\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value, i_logOR01_selection.param.value)\n",
    "def df01_download_widget(run, logOR01_selection):\n",
    "    if logOR01_selection == \"0/1 normalized data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df01_download, filename = \"01_normalized_data.csv\"), width=650) \n",
    "    if logOR01_selection == \"log transformed data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=dflog_download, filename = \"log_transformed_data.csv\"), width=650)\n",
    "    if logOR01_selection == \"Overview - cluster distances\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=table_download, filename = \"cluster_distances.csv\"), width=650)  \n",
    "    else:\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df_filteredRawData_download, filename = \"stringency_filtered_raw_data.csv\"), width=650)\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def df01_download(run):\n",
    "    df_01 = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_01_stacked)\n",
    "    sio = StringIO()\n",
    "    df_01.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def dflog_download(run):\n",
    "    df_log = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_log_stacked)\n",
    "    sio = StringIO()\n",
    "    df_log.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def df_filteredRawData_download(run):\n",
    "    df = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_filtered.stack([\"Map\", \"Fraction\"]))\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def table_download(run):\n",
    "    df = i_class.results_overview_table()\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Movement analysis\n",
    "########################\n",
    "\n",
    "lo_movement_analysis = pn.Column(\n",
    "    pn.panel(textfragments[\"mr_top\"], width=600),\n",
    "    name=\"movement_analysis\", sizing_mode=\"stretch_width\")\n",
    "btn_calc_mr = pn.widgets.Button(name=\"Calculate/display MR-plot\", width=300, button_type=\"success\")\n",
    "\n",
    "def calculate_mr(event):\n",
    "    lo_movement_analysis.objects = lo_movement_analysis.objects[0:3]\n",
    "    MRstatus = pn.Card(title=\"Temporary processing data\")\n",
    "    lo_movement_analysis.append(MRstatus)\n",
    "    try:\n",
    "        mr = i_class.run_outliertest(**i_MRConfig.get_settings(), canvas = MRstatus)\n",
    "    except:\n",
    "        MRstatus.append(traceback.format_exc())\n",
    "    try:\n",
    "        MRstatus.append(\"Loading M-R plot display ...\")\n",
    "        lo_movement_analysis.append(gui.MRPlot(data=mr))\n",
    "        MRstatus.pop(-1)\n",
    "    except:\n",
    "        MRstatus.append(traceback.format_exc())\n",
    "\n",
    "btn_calc_mr.on_click(calculate_mr)\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_MRConfig(run):\n",
    "    if run:\n",
    "        try:\n",
    "            global i_MRConfig\n",
    "            i_MRConfig = gui.ConfigureMR(\n",
    "                conditions=i_class.conditions,\n",
    "                maps = {c:[el.split(\"_\")[1] for el in i_class.map_names if el.startswith(c)]\\\n",
    "                           for c in i_class.conditions})\n",
    "            return i_MRConfig\n",
    "        except:\n",
    "            if i_class.conditions == [\"\"]:\n",
    "                return pn.Column(\"**This analysis does not contain any conditions**\")\n",
    "            else:\n",
    "                return pn.Column(\n",
    "                    i_class.conditions,\n",
    "                    pn.Str(traceback.format_exc(), width=600),\n",
    "                )\n",
    "    else:\n",
    "        lo_movement_analysis.objects = lo_movement_analysis.objects[0:2]\n",
    "        return \"Run analysis first\"\n",
    "\n",
    "lo_movement_analysis.append(update_MRConfig)\n",
    "lo_movement_analysis.append(btn_calc_mr)\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "analysis_tabs.clear()\n",
    "analysis_tabs.append((\"Data overview\", update_data_overview))\n",
    "analysis_tabs.append((\"Depth and Coverage\", update_quantity))\n",
    "analysis_tabs.append((\"Intramap Scatter\", update_cluster_overview))\n",
    "analysis_tabs.append((\"Movement analysis\", lo_movement_analysis))\n",
    "analysis_tabs.append((\"Download\", show_tabular_overview))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Comparison output tabs<a id=\"comparison\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Dashboard structure\n",
    "#### Overview tab\n",
    "########################\n",
    "lo_benchmark_pca = pn.Column()\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "loading_status_comparison = pn.Row()\n",
    "idle_comparison = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading_comparison = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "cache_uploaded_json = pn.widgets.Checkbox(value=False)\n",
    "cache_run_json = pn.widgets.Checkbox(value=False)\n",
    "m_diverget_fractions = pn.Pane(\"\")\n",
    "comparison_status = pn.Pane(\"No datasets were compared yet\")\n",
    "i_ExpOverview = pn.Row(pn.Pane(\"\", width=1000))\n",
    "\n",
    "i_multi_choice = pn.widgets.CrossSelector(name=\"Select experiments for comparison\", value=[\"a\", \"b\"],\n",
    "                                          options=[\"a\", \"b\", \"c\"], definition_order=False)\n",
    "\n",
    "i_markerset_or_cluster = pn.widgets.Checkbox(value=False, name=\"Display only protein clusters\")\n",
    "i_pca_comp_ncomp = pn.widgets.IntSlider(value=3, start=2, end=5)\n",
    "btn_pca_comp_ncomp = pn.widgets.Button(name=\"Recalculate PCA with different number of components.\")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "for el in [pn.Pane(textfragments[\"benchmark_pca_top\"], width=600),\n",
    "           pn.Row(i_markerset_or_cluster, i_pca_comp_ncomp, btn_pca_comp_ncomp)]:\n",
    "    lo_benchmark_pca.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# recalculate_comp_pca\n",
    "# update_visualization_map_comparison # tab content\n",
    "\n",
    "def recalculate_comp_pca(event):\n",
    "    cache_run_json.value=False\n",
    "    i_class_comp.perform_pca_comparison(n=i_pca_comp_ncomp.value)\n",
    "    cache_run_json.value=True\n",
    "btn_pca_comp_ncomp.on_click(recalculate_comp_pca)\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value,\n",
    "            i_markerset_or_cluster.param.value)\n",
    "def update_visualization_map_comparison(multi_choice, run_json, markerset_or_cluster):\n",
    "    try:\n",
    "        if run_json == True:\n",
    "            if multi_choice == []:\n",
    "                return \"Please select experiments for comparison\"\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            df_pca = i_class_comp.df_pca.loc[i_class_comp.df_pca[\"Experiment\"].isin(multi_choice)]\\\n",
    "                .set_index([col for col in i_class_comp.df_pca.columns if not col.startswith(\"PC\")])\n",
    "            \n",
    "            pca_global_comparison = gui.pca_plot(\n",
    "                df_pca=df_pca,\n",
    "                df_var=i_class_comp.df_pca_var,\n",
    "                df_loadings=i_class_comp.df_pca_loadings,\n",
    "                show_variability=True, show_loadings=True,\n",
    "                color=\"Cluster\" if markerset_or_cluster else \"Compartment\",\n",
    "                color_map=i_class_comp.color_maps[\"Clusters\"] if markerset_or_cluster else i_class_comp.color_maps[\"Compartments\"],\n",
    "                enable_highlight = False if markerset_or_cluster else True,\n",
    "                facet_col=\"Experiment\",\n",
    "                title=\"PCA plot\"\n",
    "            )\n",
    "            pca_global_comparison.highlight_dict = i_class_comp.markerproteins\n",
    "            return pca_global_comparison\n",
    "        else:\n",
    "            pca_global_comparison = \"Run analysis first!\"\n",
    "            return pca_global_comparison\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "lo_benchmark_pca.append(update_visualization_map_comparison)\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Depth tab\n",
    "########################\n",
    "lo_benchmark_depth = pn.Column()\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "for el in [\n",
    "    pn.Pane(textfragments[\"benchmark_depth_top\"], width=600)\n",
    "]:\n",
    "    lo_benchmark_depth.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# update_npr_ngg_nprDc\n",
    "# update_venn\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_npr_ngg_nprDc(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                fig_quantity_pg, fig_quantity_pr = i_class_comp.quantity_pr_pg_barplot_comparison(multi_choice=multi_choice)\n",
    "                coverage_barplot = i_class_comp.coverage_comparison(multi_choice=multi_choice)\n",
    "                return pn.Row(\n",
    "                    pn.Pane(fig_quantity_pg, config=plotly_config), \n",
    "                    pn.Pane(coverage_barplot, config=plotly_config)\n",
    "                )\n",
    "        else:\n",
    "            completeness_barplot = \"Run analysis first!\"\n",
    "            return completeness_barplot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status \n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_venn(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            venn_plot = []\n",
    "            if len(multi_choice)<=1:\n",
    "                return pn.Column(\n",
    "                    pn.Row(pn.Pane(\"Please select 2 or more experiments for comparison\"), width=1000))\n",
    "            else:\n",
    "                venn_plot_total, venn_plot_int, figure_UpSetPlot_total, figure_UpSetPlot_int = i_class_comp.venn_sections(multi_choice_venn = multi_choice)\n",
    "                return pn.Row(\n",
    "                    pn.Column(\n",
    "                        \"Proteins quantified in at least one map\",\n",
    "                        pn.Pane(venn_plot_total),\n",
    "                        pn.Row(figure_UpSetPlot_total,width=1000)\n",
    "                    ),\n",
    "                    pn.Column(\n",
    "                        \"Proteins quantified in all maps\",\n",
    "                        pn.Pane(venn_plot_int),\n",
    "                        pn.Row(figure_UpSetPlot_int,width=1000)\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            venn_plot = \"Run analysis first!\"\n",
    "            return venn_plot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "lo_benchmark_depth.append(update_npr_ngg_nprDc)\n",
    "lo_benchmark_depth.append(update_venn)\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Intramap scatter tab\n",
    "########################\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "i_complexes_norm = pn.widgets.Select(name=\"Normalize complexes to\", options=[\"Median across all experiments\"])\n",
    "i_complexes_plot = pn.widgets.Select(name=\"Plot type\", options=[\"stacked\", \"strip\", \"box\", \"violin\", \"histogram\"])\n",
    "i_complexes_sync = pn.widgets.Checkbox(name=\"Highlight selected cluster (summary plots only)\")\n",
    "i_complexes_detail = pn.widgets.Select(options=[], name=\"Cluster of interest\", width=300)\n",
    "\n",
    "lo_complexes = pn.WidgetBox(\"**Compare experiments**\", i_complexes_norm, i_complexes_plot, i_complexes_sync)\n",
    "\n",
    "i_clusters_for_ranking = pn.widgets.CrossSelector(name=\"Select clusters to be considered for ranking calculation\",\n",
    "                                                  options=[], size=8)\n",
    "i_minn_proteins = pn.widgets.IntSlider(name=\"Minimum number of proteins per complex\", start=3, end=13, step=1, value=5)\n",
    "i_collapse_maps = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# update_comp_cluster_coverage\n",
    "# update_comp_bp_global\n",
    "# update_comp_bp_single\n",
    "\n",
    "@pn.depends(i_multi_choice.param.options, i_minn_proteins.param.value, cache_run_json.param.value)\n",
    "def update_comp_cluster_coverage(exp_names, min_n, run_json):\n",
    "    try:\n",
    "        if not run_json:\n",
    "            return \"\"\n",
    "        [f,p,n] = i_class_comp.get_complex_coverage(min_n)\n",
    "        i_clusters_for_ranking.options = [el for el in i_class_comp.markerproteins.keys() if el not in n.keys()]\n",
    "        i_complexes_detail.options = [el for el in i_class_comp.markerproteins.keys() if el not in n.keys()]\n",
    "        i_clusters_for_ranking.value = [el for el in i_class_comp.markerproteins.keys() if el in f.keys()]\n",
    "        return pn.Row(\n",
    "            \"Coverage in all experiments \\[>= n proteins]:<br>\"+\"<br>\".join([\"- {} ({})\".format(k,v) for k,v in f.items()]),\n",
    "            \"Coverage in some experiments \\[proteins/experiment]:<br>\"+\"<br>\".join([\"- {} \\{}\".format(k,str(v)) for k,v in p.items()]),\n",
    "            \"No sufficient coverage in any experiment \\[proteins/experiment]:<br>\"+\"<br>\".join([\"- {} \\{}\".format(k,str(v)) for k,v in n.items()])\n",
    "        )\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusters_for_ranking.param.value,\n",
    "            i_minn_proteins.param.value,\n",
    "            i_complexes_norm.param.value, i_complexes_plot.param.value, i_complexes_sync.param.value, i_complexes_detail.param.value,\n",
    "            cache_run_json.param.value)\n",
    "def update_comp_bp_global(multi_choice, clusters_for_ranking, min_n,\n",
    "                          norm, plot_type, sync, highlight,\n",
    "                          run_json):\n",
    "    try:\n",
    "        if not run_json:\n",
    "            return \"\"\n",
    "        if set(multi_choice) != set(i_class_comp.df_distance_comp.Experiment.values):\n",
    "            i_class_comp.calc_biological_precision(multi_choice)\n",
    "            i_complexes_norm.options = [i_complexes_norm.options[0]]+multi_choice\n",
    "        if clusters_for_ranking == []:\n",
    "            return pn.Row(\n",
    "                lo_complexes,\n",
    "                \"Select at least one cluster\"\n",
    "            )\n",
    "        else:\n",
    "            medians, plot = i_class_comp.plot_intramap_scatter(\n",
    "                normalization=np.median if norm == \"Median across all experiments\" else norm,\n",
    "                aggregate_proteins=True, aggregate_maps=False,\n",
    "                plot_type=plot_type, highlight=None if sync == False else highlight,\n",
    "                min_size=min_n,\n",
    "                multi_choice=multi_choice,\n",
    "                clusters_for_ranking=clusters_for_ranking)\n",
    "            return pn.Row(\n",
    "                lo_complexes,\n",
    "                medians.rename({\"distance\": \"median distance\"}, axis=1),\n",
    "                pn.Pane(plot, config=plotly_config)\n",
    "            )\n",
    "        \n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return pn.Row(\n",
    "            lo_complexes,\n",
    "            update_status\n",
    "        )\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_complexes_detail.param.value, i_collapse_maps.param.value,\n",
    "            cache_run_json.param.value)\n",
    "def update_comp_bp_single(multi_choice, clusterwidget_comparison, collapse_maps, run_json):\n",
    "    try:\n",
    "        i_class_comp.cache_cluster_quantified = True\n",
    "        distance_comparison = i_class_comp.plot_intramap_scatter_cluster(collapse_maps=collapse_maps, cluster_of_interest_comparison=clusterwidget_comparison, multi_choice=multi_choice)\n",
    "        if i_class_comp.cache_cluster_quantified == False:\n",
    "            return \"Cluster was not quantified in any experiment\"\n",
    "        else:\n",
    "            df_pca = i_class_comp.df_cluster_pca.xs(clusterwidget_comparison, level=\"Cluster\", axis=0)\n",
    "            df_pca = df_pca.query(f'Experiment in {str(multi_choice)}')\n",
    "            if collapse_maps:\n",
    "                df_pca = df_pca.groupby([el for el in df_pca.index.names if \"Map\" not in el]).mean()\n",
    "            pca_comparison = gui.pca_plot(\n",
    "                df_pca=df_pca,\n",
    "                color=\"Experiment\", color_map=dict(), facet_col=None,\n",
    "                enable_highlight=False, show_variability=False, show_loadings=False,\n",
    "                title=\"PCA plot for {}\".format(clusterwidget_comparison)\n",
    "            )\n",
    "            return pn.Row(pca_comparison,\n",
    "                          pn.Pane(distance_comparison, config=plotly_config))\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "comparison_tab_bp = pn.Column(\n",
    "    pn.Pane(textfragments[\"benchmark_intra_top\"], width=600),\n",
    "    pn.Row(pn.Column(i_clusters_for_ranking,i_minn_proteins),\n",
    "           update_comp_cluster_coverage),\n",
    "    update_comp_bp_global,\n",
    "    pn.Row(i_complexes_detail,i_collapse_maps),\n",
    "    update_comp_bp_single\n",
    ")\n",
    "\n",
    "#### Dashboard structure\n",
    "#### SMV analysis\n",
    "########################\n",
    "lo_benchmark_SVMs_tabs = pn.Tabs()\n",
    "lo_benchmark_SVMs = pn.Column(\n",
    "    pn.Pane(textfragments[\"benchmark_SVM_top\"], width=600),\n",
    "    lo_benchmark_SVMs_tabs,\n",
    "    pn.Row(name=\"svm_output\")\n",
    ")\n",
    "lo_benchmark_SVMs_addmcm = pn.Column(name=\"add_mcmatrix\")\n",
    "lo_benchmark_SVMs_runsvm = pn.Column(name=\"run_svm\")\n",
    "lo_benchmark_SVMs_tabs.append((\"Upload misclassification\", lo_benchmark_SVMs_addmcm))\n",
    "lo_benchmark_SVMs_tabs.append((\"Run SVMs\", lo_benchmark_SVMs_runsvm))\n",
    "\n",
    "#### Layout elements\n",
    "#### SVM analysis\n",
    "####################\n",
    "SVM_status = pn.pane.Markdown(width=400)\n",
    "lo_SVM_heatmap = pn.Column(SVM_status)\n",
    "i_SVMmatrix = pn.widgets.input.TextAreaInput(name=\"Misclassification matrix\", placeholder=\"Copy matrix here...\")\n",
    "i_SVMsource = pn.widgets.Select(options=[\"Perseus\", \"MetaMass\", \"direct\"], value=\"Perseus\", name=\"Select source of misclassification matrix\")\n",
    "i_SVMname = pn.widgets.TextInput(name=\"Set name\", value=\"default\", placeholder=\"Identify the set of misclassification matrices (e.g. rbf_SVM)\")\n",
    "lo_SVM_source = pn.Row(i_SVMsource, gui.help_icon(\"For Perseus, copy whole matrix via Ctrl-A, Ctrl-C. For MetaMass copy matrix including column headings, but without row labels. Direct assumes true classes in rows and predicted classes in columns, with column headings only.\"))\n",
    "i_SVMcomment = pn.widgets.TextInput(name=\"Comment\", placeholder=\"Add comments here...\")\n",
    "i_SVMexp = pn.widgets.Select(name=\"Select experiments for the assignment of a misclassification matrix\", options=[\"a\", \"b\", \"c\"])\n",
    "btn_SVM_addmatrix = pn.widgets.Button(name=\"Update misclassification matrix\",\n",
    "                                      button_type=\"success\", width=400, height=50,\n",
    "                                      css_classes=[\"button-main\"])\n",
    "i_svm_set = pn.widgets.Select(options=[\"default\"], name=\"Select set\")\n",
    "i_svm_score = pn.widgets.Select(options=[\"F1 score\", \"Precision\", \"Recall\", \"Class size\"], value=\"F1 score\")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "#### SVM analysis\n",
    "###############################\n",
    "for el in [i_SVMexp, lo_SVM_source, i_SVMname, i_SVMcomment, i_SVMmatrix, lo_SVM_heatmap, btn_SVM_addmatrix]:\n",
    "    lo_benchmark_SVMs_addmcm.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "# update_SVMexp\n",
    "# add_SVM_result\n",
    "# update_SVM_Analysis # needs update, including the backend function in domaps.py\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_SVMexp(multi_choice):\n",
    "    i_SVMexp.options = multi_choice\n",
    "\n",
    "\n",
    "def add_SVM_result(event):\n",
    "    \"\"\"\n",
    "    Adds SVM matrix uploaded in the tool to memory and to the current analysis\n",
    "    \"\"\"\n",
    "    # 1. Get input from interface\n",
    "    \n",
    "    experiment, SVMname, SVMsource, SVMcomment = i_SVMexp.value, i_SVMname.value, i_SVMsource.value, i_SVMcomment.value\n",
    "    \n",
    "    # change comment of a stored dataset\n",
    "    try:\n",
    "        if i_SVMmatrix.value == \"\" and i_class_comp.svm_results[experiment][SVMname][\"misclassification\"] is not None:\n",
    "            SVMmatrix = i_class_comp.svm_results[experiment][SVMname][\"misclassification\"]\n",
    "        # return error if no misclassification  is uploaded or no comment is changed\n",
    "        #if i_SVMmatrix.value == \"\":\n",
    "        #    SVM_status.object = \"No misclassification matrix is uploaded\"\n",
    "        else:\n",
    "            SVMmatrix = pd.read_table(StringIO(i_SVMmatrix.value), sep=\"\\t\")\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        SVM_status.object = update_status\n",
    "# \n",
    "    # 2. Add to i_class_comp\n",
    "    # defaults to name=\"default\" and overwrite=True\n",
    "    i_class_comp.add_svm_result(experiment, SVMmatrix, name=SVMname, source=SVMsource, comment=SVMcomment)\n",
    "    if SVMname not in i_svm_set.options:\n",
    "        i_svm_set.options = i_svm_set.options+[SVMname]\n",
    "    # 3. Add to mem_available_datasets so it can be downloaded together with the data\n",
    "    mem_available_datasets[experiment][\"SVM results\"] = copy.deepcopy(i_class_comp.svm_results[experiment])\n",
    "    for k in i_class_comp.svm_results[experiment].keys():\n",
    "        mc_json = mem_available_datasets[experiment][\"SVM results\"][k][\"misclassification\"].to_json()\n",
    "        mem_available_datasets[experiment][\"SVM results\"][k][\"misclassification\"] = mc_json\n",
    "        pr_json = mem_available_datasets[experiment][\"SVM results\"][k][\"prediction\"].to_json()\n",
    "        mem_available_datasets[experiment][\"SVM results\"][k][\"prediction\"] = pr_json\n",
    "        \n",
    "    #empty misclassification matrix of the textarea-input widget\n",
    "    i_SVMmatrix.value = \"\"\n",
    "    # 4. Trigger update of figure\n",
    "    update_SVM_Analysis(i_multi_choice.value)\n",
    "\n",
    "\n",
    "btn_SVM_addmatrix.on_click(add_SVM_result)\n",
    "    \n",
    "@pn.depends(i_SVMexp.param.value, i_SVMname.param.value, i_SVMmatrix.param.value, watch=True)\n",
    "def fill_svm_comment(SVMexp, SVMname, SVMmatrix):\n",
    "    \"\"\"\n",
    "    Acess stored comment if possible and no new matrix was uploaded.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        comment = i_class_comp.svm_results[SVMexp][SVMname][\"comment\"]\n",
    "        if SVMmatrix != \"\" or comment == \"\":\n",
    "            timestampStr = datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\")\n",
    "            comment = \"Matrix added to set {} on {}\".format(SVMname, timestampStr)\n",
    "        else:\n",
    "            pass\n",
    "    except Exception:\n",
    "            timestampStr = datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\")\n",
    "            comment = \"Matrix added to set {} on {}\".format(SVMname, timestampStr)\n",
    "    i_SVMcomment.value = comment\n",
    "        \n",
    "@pn.depends(i_SVMexp.param.value, i_SVMname.param.value, i_SVMmatrix.param.value)\n",
    "def show_misclassification(SVMexp, SVMname, SVMmatrix):\n",
    "    \"\"\"\n",
    "    Display heatmap of freshly uploaded or reloaded SVM misclassification matrix\n",
    "    \"\"\"\n",
    "    if SVMmatrix == \"\":\n",
    "        try: \n",
    "            df_SVM = i_class_comp.svm_results[SVMexp][SVMname][\"misclassification\"]\n",
    "            SVMheatmap = domaps.svm_heatmap(df_SVM)\n",
    "        except: \n",
    "            SVMheatmap = \"No misclassification matrix is uploaded\"\n",
    "    else: \n",
    "        df_SVM = pd.read_table(StringIO(SVMmatrix), sep=\"\\t\")\n",
    "        SVMheatmap = domaps.svm_heatmap(df_SVM)\n",
    "    return SVMheatmap\n",
    "\n",
    "@pn.depends(i_SVMexp.param.value, i_SVMname.param.value)\n",
    "def load_matrix(SVMexp, SVMname):\n",
    "    \"\"\"\n",
    "    Load data if new experiment is selected \n",
    "    \"\"\"\n",
    "    try:\n",
    "        SVMmatrix = i_class_comp.svm_results[SVMexp][SVMname][\"misclassification\"]\n",
    "        SVM_status.object = \"Misclassification Matrix from class\"\n",
    "    except:\n",
    "        SVM_status.object = \"Upload missclassificationmatrix first\"\n",
    "\n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, i_svm_set.param.value, i_svm_score.param.value)\n",
    "def show_svm_results(multi_choice, svmset, score):\n",
    "    try:\n",
    "        if len(i_class_comp.svm_results) == 0:\n",
    "            return \"Please upload SVM results first\"\n",
    "        plot_detail = i_class_comp.plot_svm_detail(multi_choice=multi_choice, score=score, svmset=svmset)\n",
    "        plot_summary = i_class_comp.plot_svm_summary(multi_choice=multi_choice, score=score, svmset=svmset)\n",
    "        return pn.Column(pn.Pane(plot_summary, config=plotly_config), pn.Pane(plot_detail, config=plotly_config))\n",
    "    except:\n",
    "        return traceback.format_exc()\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_SVM_Analysis(multi_choice):\n",
    "    #empty lo if available\n",
    "    lo_benchmark_SVMs.objects[[i.name for i in lo_benchmark_SVMs].index(\"svm_output\")].objects = []\n",
    "    try:\n",
    "        if multi_choice == []:\n",
    "            lo = pn.Column(pn.Row(\"Please select experiments for comparison\"))\n",
    "        else:\n",
    "            lo = pn.Column(i_svm_set, i_svm_score, show_svm_results)\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        lo = update_status\n",
    "    lo_benchmark_SVMs.objects[[el.name for el in lo_benchmark_SVMs.objects].index(\"svm_output\")].append(lo)\n",
    "\n",
    "#### Callback output positioning\n",
    "#### SVM analysis\n",
    "################################\n",
    "lo_SVM_heatmap.append(show_misclassification)\n",
    "#lo_benchmark_SVMs.objects[[el.name for el in lo_benchmark_SVMs.objects].index(\"svm_output\")].append(update_SVM_Analysis())\n",
    "\n",
    "\n",
    "i_svm_svmcomp = domaps.SVMComp(\"exp1;exp2__42__0.2__cl1;cl2\")\n",
    "i_svm_randomstate = pn.widgets.IntInput(value=42, width=150, name=\"Seed for randomization\")\n",
    "i_svm_testsplit = pn.widgets.FloatInput(start=0, end=1, step=0.05, value=0.2, width=150,\n",
    "                                        name=\"Test set proportion\")\n",
    "i_svm_classes = gui.ConfigureSVMClasses()\n",
    "i_svm_Crange = pn.widgets.IntRangeSlider(start=1,end=50,step=1,value=(1,30), name=\"C (% misclassification)\", width=250)\n",
    "i_svm_gammarange = pn.widgets.IntRangeSlider(start=1,end=100,step=1,value=(1,50), name=\"gamma (RBF diameter)\", width=250)\n",
    "i_svm_trainingrounds = pn.widgets.IntSlider(start=3, end=10, value=5, name=\"Training iterations\", width=150)\n",
    "btn_svm_train = pn.widgets.Button(name=\"Run training\", width=150, button_type=\"success\")\n",
    "i_svm_canvas = pn.Column(pn.Row())\n",
    "i_svm_C = pn.widgets.FloatInput(name=\"C\", background=\"salmon\", width=150)\n",
    "i_svm_gamma = pn.widgets.FloatInput(name=\"gamma\", background=\"salmon\", width=150)\n",
    "i_svm_nameprediction = pn.widgets.TextInput(name=\"Name prediction set\", width=150, value=\"SVM run\")\n",
    "i_svm_minp = pn.widgets.FloatInput(start=0.1, end=1.0, step=0.05, value=0.4, width=150, name=\"Minimum SVM probability\")\n",
    "i_svm_minpdiff = pn.widgets.FloatInput(start=0.00, end=0.9, step=0.05, value=0.15, width=150, name=\"Minimum difference to second\")\n",
    "btn_svm_predict = pn.widgets.Button(name=\"Run predictions\", width=150)\n",
    "i_svm_canvas_prediction = pn.Column()\n",
    "\n",
    "for el in [\n",
    "    pn.panel(textfragments[\"benchmark_SVM_internal\"], width=600),\n",
    "    \"**Define classes and training/test split for SVMs**\",\n",
    "    pn.Row(\n",
    "        i_svm_classes,\n",
    "        pn.Column(\n",
    "            i_svm_randomstate,\n",
    "            i_svm_testsplit\n",
    "        ),\n",
    "        gui.help_icon(\"The seed for randomization is used for defining the test-split, as well as the folds for crossvalidation. By setting this the analysis can be exactly repeated at any time.\")\n",
    "    ),\n",
    "    \"**Configure training for hyperparameter optimization**\",\n",
    "    pn.Row(\n",
    "        i_svm_Crange,\n",
    "        i_svm_gammarange,\n",
    "        i_svm_trainingrounds,\n",
    "        btn_svm_train,\n",
    "        gui.help_icon(\"The hyper parameter optimization is done in an iterative grid search. These paramters define where the search starts and how many iterations it should run.\")\n",
    "    ),\n",
    "    pn.Card(i_svm_canvas, header=pn.pane.Markdown(\"**Training output**\", width=860), width=860),\n",
    "    \"**Select hyper parameters and run predictions**\",\n",
    "    pn.Row(\n",
    "        i_svm_C,\n",
    "        i_svm_gamma,\n",
    "    ),\n",
    "    pn.Row(\n",
    "        i_svm_minp,\n",
    "        i_svm_minpdiff,\n",
    "        i_svm_nameprediction,\n",
    "        btn_svm_predict,\n",
    "        gui.help_icon(\"The minimum probability is used to determine whether any organelle matches the protein well. The minimum difference determines the labelling if more than one class matches the minimum probability.\")\n",
    "    ),\n",
    "    i_svm_canvas_prediction\n",
    "]:\n",
    "    lo_benchmark_SVMs_runsvm.append(el)\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_svm_class(multi_choice):\n",
    "    try:\n",
    "        i_svm_svmcomp.experiments = multi_choice\n",
    "        available_classes = list(set(i_class_comp.df_01_filtered_combined.drop(\"undefined\", axis=0, level=\"Compartment\").index.get_level_values(\"Compartment\")))\n",
    "        i_svm_svmcomp.classes = available_classes\n",
    "        i_svm_svmcomp.set_df(i_class_comp.df_01_filtered_combined)\n",
    "        class_counts = i_svm_svmcomp._get_markers(\"shared\").value_counts(\"Compartment\")\n",
    "        i_svm_classes.class_counts = class_counts\n",
    "        i_svm_classes.classes = list(class_counts.index)\n",
    "        train, test = i_svm_svmcomp._train_test_split(test_percent=i_svm_testsplit.value, random_state=i_svm_randomstate.value)\n",
    "        i_svm_classes.train = train\n",
    "        i_svm_classes.test = test\n",
    "        update_canvas_hash()\n",
    "    except:\n",
    "        lo_benchmark_SVMs_runsvm.append(traceback.format_exc())\n",
    "\n",
    "@pn.depends(i_svm_randomstate.param.value, i_svm_testsplit.param.value, i_svm_classes.param.classes, watch=True)\n",
    "def update_test_split(random_state, test_split, classes):\n",
    "    try:\n",
    "        i_svm_svmcomp.random_state = random_state\n",
    "        i_svm_svmcomp.test_split = test_split\n",
    "        if len(classes) > 0:\n",
    "            i_svm_svmcomp.classes = classes\n",
    "        train, test = i_svm_svmcomp._train_test_split(test_percent=test_split, random_state=random_state)\n",
    "        i_svm_classes.train = train\n",
    "        i_svm_classes.test = test\n",
    "        update_canvas_hash()\n",
    "    except:\n",
    "        lo_benchmark_SVMs_runsvm.append(traceback.format_exc())\n",
    "\n",
    "def update_canvas_hash():\n",
    "    hash_data = domaps.SVMComp._construct_hash_data(\n",
    "        experiments=i_svm_svmcomp.experiments,\n",
    "        random_state=i_svm_svmcomp.random_state,\n",
    "        test_split=i_svm_svmcomp.test_split,\n",
    "        classes=i_svm_svmcomp.classes)\n",
    "    if hash_data in i_class_comp.svm_runs.keys():\n",
    "        i_svm_canvas.objects = [pn.Row(domaps.SVMComp._plot_training(i_class_comp.svm_runs[hash_data].accuracies))]\n",
    "        i_svm_C.value = i_class_comp.svm_runs[hash_data].C\n",
    "        i_svm_gamma.value = i_class_comp.svm_runs[hash_data].gamma\n",
    "        sio = StringIO()\n",
    "        i_class_comp.svm_runs[hash_data].accuracies.to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        download_acc = pn.widgets.FileDownload(file=sio, filename=\"SVMtraining_accuracies.csv\")\n",
    "        i_svm_canvas.append(download_acc)\n",
    "    else:\n",
    "        i_svm_canvas.objects = [pn.Row()]\n",
    "        btn_svm_train.button_type=\"success\"\n",
    "\n",
    "def svm_run_training(event):\n",
    "    try:\n",
    "        hash_data = domaps.SVMComp._construct_hash_data(\n",
    "            experiments = i_multi_choice.value,\n",
    "            random_state = i_svm_randomstate.value,\n",
    "            test_split = i_svm_testsplit.value,\n",
    "            classes = i_svm_classes.classes\n",
    "        )\n",
    "        if hash_data not in i_class_comp.svm_runs.keys() or btn_svm_train.button_type != \"success\":\n",
    "            i_svm_canvas.objects = [\"Starting training\"]\n",
    "        C, gamma = i_class_comp.train_svm(\n",
    "            experiments = i_multi_choice.value,\n",
    "            random_state = i_svm_randomstate.value,\n",
    "            test_split = i_svm_testsplit.value,\n",
    "            classes = i_svm_classes.classes,\n",
    "            output = \"canvas\",\n",
    "            canvas = i_svm_canvas,\n",
    "            rounds = i_svm_trainingrounds.value,\n",
    "            C0 = i_svm_Crange.value[0], C1 = i_svm_Crange.value[1],\n",
    "            g0 = i_svm_gammarange.value[0], g1 = i_svm_gammarange.value[1],\n",
    "            overwrite = False if btn_svm_train.button_type==\"success\" else True\n",
    "        )\n",
    "        btn_svm_train.button_type=\"success\"\n",
    "        sio = StringIO()\n",
    "        i_class_comp.svm_runs[hash_data].accuracies.to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        download_acc = pn.widgets.FileDownload(file=sio, filename=\"SVMtraining_accuracies.csv\")\n",
    "        i_svm_canvas.append(download_acc)\n",
    "        i_svm_C.value = C\n",
    "        i_svm_gamma.value = gamma\n",
    "    except RuntimeError:\n",
    "        btn_svm_train.button_type=\"danger\"\n",
    "        i_svm_canvas[0] = \"Please confirm that you want to retrain\"\n",
    "        pass\n",
    "    except:\n",
    "        i_svm_canvas[0] = traceback.format_exc()\n",
    "        pass\n",
    "btn_svm_train.on_click(svm_run_training)\n",
    "\n",
    "def svm_run_prediction(event):\n",
    "    try:\n",
    "        i_svm_canvas_prediction.objects = [\"Running predictions\"]\n",
    "        prediction = i_class_comp.predict_svm(\n",
    "            experiments = i_multi_choice.value,\n",
    "            random_state = i_svm_randomstate.value,\n",
    "            test_split = i_svm_testsplit.value,\n",
    "            classes = i_svm_classes.classes,\n",
    "            C = i_svm_C.value,\n",
    "            gamma = i_svm_gamma.value,\n",
    "            min_p = i_svm_minp.value,\n",
    "            min_diff = i_svm_minpdiff.value,\n",
    "            svmset = i_svm_nameprediction.value)\n",
    "        hash_data = domaps.SVMComp._construct_hash_data(\n",
    "            experiments = i_multi_choice.value,\n",
    "            random_state = i_svm_randomstate.value,\n",
    "            test_split = i_svm_testsplit.value,\n",
    "            classes = i_svm_classes.classes\n",
    "        )\n",
    "        prob = i_class_comp.svm_runs[hash_data].probabilities\n",
    "        out = i_class_comp.id_alignment.drop(i_class_comp.id_alignment.columns, axis=1).join(prediction).join(prob)\n",
    "        sio = StringIO()\n",
    "        out.to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        download_pred = pn.widgets.FileDownload(file=sio, filename=\"SVMprediction.csv\")\n",
    "        i_svm_canvas_prediction.append(download_pred)\n",
    "        \n",
    "        for exp in i_multi_choice.value:\n",
    "            mem_available_datasets[exp][\"SVM results\"] = copy.deepcopy(i_class_comp.svm_results[exp])\n",
    "            for k in i_class_comp.svm_results[exp].keys():\n",
    "                mc_json = mem_available_datasets[exp][\"SVM results\"][k][\"misclassification\"].to_json()\n",
    "                mem_available_datasets[exp][\"SVM results\"][k][\"misclassification\"] = mc_json\n",
    "                pr_json = mem_available_datasets[exp][\"SVM results\"][k][\"prediction\"].to_json()\n",
    "                mem_available_datasets[exp][\"SVM results\"][k][\"prediction\"] = pr_json\n",
    "        \n",
    "        \n",
    "        i_svm_set.options = i_svm_set.options+[i_svm_nameprediction.value]\n",
    "        i_svm_set.value = i_svm_nameprediction.value\n",
    "    except:\n",
    "        i_svm_canvas_prediction.objects = [traceback.format_exc()]\n",
    "btn_svm_predict.on_click(svm_run_prediction)\n",
    "\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Intermap scatter tab\n",
    "########################\n",
    "lo_benchmark_intermap = pn.Column(\n",
    "    pn.Row(name=\"intermap_top\"),\n",
    "    pn.Row(name=\"intermap_scatter\"),\n",
    "    pn.layout.VSpacer(background=\"#AAAAAA\", height=3),\n",
    "    pn.Row(name=\"correlation_plots\")\n",
    ")\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "i_scatter_metric = pn.widgets.Select(name=\"Distance metric\",\n",
    "                                     options=[\"manhattan distance to average profile\",\n",
    "                                              \"manhattan distance to median profile\",\n",
    "                                              \"euclidean distance\", \"manhattan distance\",\n",
    "                                              \"1 - cosine correlation\", \"1 - pearson correlation\",])\n",
    "i_scatter_consolidation = pn.widgets.Select(name=\"Consolidation of replicate distances\",\n",
    "                                            options=[\"average\",\"median\",\"sum\"])\n",
    "i_scatter_quantile = pn.widgets.FloatInput(name=\"Highlight quantile\", value=0.5)\n",
    "i_scatter_showrug = pn.widgets.Checkbox(name=\"Show rugplot in bottom margin\", value=False)\n",
    "i_scatter_showfull = pn.widgets.Checkbox(name=\"Show additional traces for full datasets\", value=False)\n",
    "i_scatter_x_cut = pn.widgets.FloatInput(name=\"Cut x-axis at\", value=1)\n",
    "\n",
    "i_comp_corr_mode = pn.widgets.Select(options=[\"heatmap\", \"scatter\"], name=\"Show correlations as\")\n",
    "i_comp_corr_measure = pn.widgets.Select(options=[\"Pearson\", \"Spearman\"], name=\"Correlation metric\")\n",
    "i_comp_corr_fraction = pn.widgets.Select(options=[\"all fractions\"], name=\"Select fraction(s)\")\n",
    "i_comp_corr_experiment = pn.widgets.Select(options=[\"all experiments\"], name=\"Select experiment(s)\")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "lo_benchmark_intermap.objects[0].append(\n",
    "    pn.Pane(textfragments[\"benchmark_inter_top\"], width=600)\n",
    ")\n",
    "lo_benchmark_inter_widgets = pn.WidgetBox(\n",
    "    i_scatter_metric,\n",
    "    i_scatter_consolidation,\n",
    "    i_scatter_showrug,\n",
    "    i_scatter_showfull,\n",
    "    i_scatter_quantile,\n",
    "    i_scatter_x_cut\n",
    ")\n",
    "lo_benchmark_intermap.objects[1].append(lo_benchmark_inter_widgets)\n",
    "lo_benchmark_corr_widgets = pn.WidgetBox(\n",
    "    i_comp_corr_mode, \n",
    "    i_comp_corr_measure,\n",
    "    i_comp_corr_fraction,\n",
    "    i_comp_corr_experiment\n",
    ")\n",
    "lo_benchmark_intermap.objects[3].append(lo_benchmark_corr_widgets)\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# update_corr_selection_comp\n",
    "# show_correlation_comp\n",
    "# update_global_scatter_comparison\n",
    "\n",
    "@param.depends(cache_run_json.param.value, i_multi_choice.param.value, watch=True)\n",
    "def update_corr_selection_comp(run, multi_choice):\n",
    "    if run == True:\n",
    "        i_comp_corr_fraction.options = [i_comp_corr_fraction.options[0]]+i_class_comp.fractions\n",
    "        i_comp_corr_experiment.options = [i_comp_corr_experiment.options[0]]+multi_choice\n",
    "\n",
    "\n",
    "@param.depends(i_comp_corr_mode.param.value, i_comp_corr_measure.param.value,\n",
    "               i_comp_corr_fraction.param.value, i_comp_corr_experiment.param.value,\n",
    "               i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def show_correlation_comp(mode, measure, fractions, experiments, multi_choice, run):\n",
    "    if run == True:\n",
    "        try:\n",
    "            if mode == \"scatter\" and (fractions == \"all fractions\" or experiments == \"all experiments\"):\n",
    "                return \"Please select only one fraction and experiment in scatter mode.\"\n",
    "            df = i_class_comp.df_01_filtered_combined.loc[\n",
    "                i_class_comp.df_01_filtered_combined.index.get_level_values(\"Experiment\").isin(multi_choice),:].copy()\n",
    "            df = df.reset_index(\"Exp_Map\", drop=True).unstack([\"Experiment\", \"Map\"]).sort_index(axis=1, level=\"Fraction\")\n",
    "            \n",
    "            if fractions != \"all fractions\":\n",
    "                df = df.xs(fractions, level=\"Fraction\", axis=1, drop_level=False)\n",
    "            if experiments != \"all experiments\":\n",
    "                df = df.xs(experiments, level=\"Experiment\", axis=1, drop_level=False)\n",
    "            \n",
    "            df.columns = [\"_\".join(el) for el in df.columns]\n",
    "            \n",
    "            measure_dict = {\n",
    "                \"Spearman\": lambda x: spearmanr(x.values).correlation,\n",
    "                \"Pearson\": lambda x: np.corrcoef(x.T)\n",
    "            }\n",
    "            \n",
    "            plot = domaps.plot_sample_correlations(\n",
    "                df, data_columns=\"(.*)\",\n",
    "                log10=False,\n",
    "                binning=50,\n",
    "                mode=mode,\n",
    "                correlation_function=measure_dict[measure]\n",
    "            )\n",
    "            return plot\n",
    "    \n",
    "        except:\n",
    "            return traceback.format_exc()\n",
    "    else:\n",
    "        return \"Run analysis first\"\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_scatter_metric.param.value, i_scatter_consolidation.param.value,\n",
    "            cache_run_json.param.value, i_scatter_showfull.param.value,\n",
    "            i_scatter_quantile.param.value, i_scatter_showrug.param.value, i_scatter_x_cut.param.value)\n",
    "def update_global_scatter_comparison(multi_choice, metric, consolidation, run_json,\n",
    "                                     show_full, quantile, show_rug, x_cut):\n",
    "    try:\n",
    "        if run_json == False:\n",
    "            return \"Run analysis first!\"\n",
    "        if multi_choice == []:\n",
    "            return pn.Column(pn.Row(\"Please select experiments for comparison\"))\n",
    "        if \"reproducibility consolidation\" not in i_class_comp.parameters.keys() or \\\n",
    "        i_class_comp.parameters[\"reproducibility metric\"] != metric or \\\n",
    "        i_class_comp.parameters[\"reproducibility consolidation\"] != consolidation:\n",
    "            i_class_comp.calculate_global_scatter(metric, consolidation)\n",
    "        reproducibility_plot = i_class_comp.plot_reproducibility_distribution(\n",
    "            multi_choice=multi_choice,\n",
    "            x_cut=x_cut, q=quantile,\n",
    "            show_rug=show_rug, show_full=show_full)\n",
    "        return pn.Pane(reproducibility_plot, config=plotly_config)\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "lo_benchmark_intermap[1].append(update_global_scatter_comparison)\n",
    "lo_benchmark_intermap[3].append(show_correlation_comp)\n",
    "\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Profile comparison tab\n",
    "########################\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "i_compare_gene = pn.widgets.TextInput(value=\"PLEC\", name=\"Enter gene name or protein ID to see profile.\")\n",
    "i_compare_profile_style = pn.widgets.Select(options=[\n",
    "    #\"all profiles\",\n",
    "    \"mean +- stdev\", \"mean +- SEM\"])\n",
    "i_compare_compartment = pn.widgets.MultiSelect(options=[], name=\"Select compartments for which to show summary profiles.\")\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# update_profile_comparison\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_compare_gene.param.value, i_compare_compartment.param.value,\n",
    "            i_compare_profile_style.param.value, cache_run_json.param.value)\n",
    "def update_profile_comparison(multi_choice, compare_gene, compare_compartment, compare_profile_style, run_json):\n",
    "    if not run_json:\n",
    "        return \"Run analysis first!\"\n",
    "    try:\n",
    "        try:\n",
    "            plotdata = i_class_comp.df_01_filtered_combined.xs(compare_gene,level=\"Gene names\",\n",
    "                                                               axis=0, drop_level=False)\\\n",
    "                                   .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "        except:\n",
    "            try:\n",
    "                plotdata = i_class_comp.df_01_filtered_combined.xs(compare_gene,level=\"Protein IDs\",\n",
    "                                                                   axis=0, drop_level=False)\\\n",
    "                                       .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "            except:\n",
    "                try:\n",
    "                    plotdata = i_class_comp.df_01_filtered_combined.loc[[\n",
    "                        compare_gene in \" \".join(el) for el in i_class_comp.df_01_filtered_combined.index.values],:]\\\n",
    "                                           .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "                except:\n",
    "                    plotdata = pd.DataFrame()\n",
    "        if len(plotdata) > 0:\n",
    "            plotdata.drop(\"Exp_Map\", axis=1, inplace=True)\n",
    "            plotdata.sort_values(\"Fraction\", key=domaps.natsort_list_keys, inplace=True)\n",
    "            experiments = [el for el in multi_choice if el in plotdata[\"Experiment\"].values]\n",
    "            plotdata = plotdata.set_index(\"Experiment\").loc[experiments,:].reset_index()\n",
    "            plotprofile = px.line(plotdata, x=\"Fraction\", y=\"Profile [% total signal]\", line_group=\"Map\",\n",
    "                                  facet_col=\"Protein IDs\", template=\"simple_white\",\n",
    "                                  line_dash = \"Sequence\" if \"Sequence\" in plotdata.columns else None,\n",
    "                                  color=\"Experiment\", hover_data=list(plotdata.columns))\n",
    "        else:\n",
    "            plotprofile = \"No gene or protein ID matching {} found.\".format(compare_gene)\n",
    "        \n",
    "        plotdata = pd.DataFrame()\n",
    "        if compare_profile_style == \"all profiles\":\n",
    "            for el in compare_compartment:\n",
    "                el_df = i_class_comp.df_01_filtered_combined.xs(el, level=\"Compartment\", axis=0, drop_level=False)\\\n",
    "                .stack(\"Fraction\").reset_index().rename({0:\"Profile [% total signal]\"}, axis=1)\n",
    "                plotdata = plotdata.append(el_df)\n",
    "            if len(plotdata) > 0:\n",
    "                plotdata.sort_values(\"Fraction\", key=domaps.natsort_list_keys, inplace=True)\n",
    "                plotdata = plotdata.set_index(\"Experiment\").loc[multi_choice,:].reset_index()\n",
    "                plotdata.insert(0, \"PG_Map\", [str(p)+\"_\"+str(m) for p,m in zip(plotdata[\"Protein IDs\"], plotdata[\"Map\"])])\n",
    "                plotcompartments = px.box(plotdata, x=\"Fraction\", y=\"Profile [% total signal]\", color=\"Experiment\",\n",
    "                                          facet_row=\"Compartment\", template=\"simple_white\")\n",
    "            else:\n",
    "                plotcompartments = \"Please select at least one compartment\"\n",
    "        else:\n",
    "            for el in compare_compartment:\n",
    "                el_df = i_class_comp.df_01_filtered_combined.xs(el, level=\"Compartment\", axis=0, drop_level=False)\n",
    "                plotdata = plotdata.append(el_df.stack(\"Fraction\").groupby([\"Compartment\", \"Map\", \"Experiment\", \"Fraction\"])\\\n",
    "                    .apply(lambda x: pd.Series({\"Profile [% total signal]\": np.nanmean(x), \"std\":np.nanstd(x),\n",
    "                                                \"sem\":np.nanstd(x)/np.sqrt(sum(np.isfinite(x)))}))\\\n",
    "                    .reset_index().rename(columns={\"level_4\": \"measure\", 0: \"value\"})\\\n",
    "                    .set_index([\"Compartment\", \"Map\", \"Experiment\", \"Fraction\", \"measure\"]).unstack(\"measure\")\\\n",
    "                    .droplevel(0, axis=1).reset_index())\n",
    "            if len(plotdata) > 0:\n",
    "                plotdata.sort_values(\"Fraction\", key=domaps.natsort_list_keys, inplace=True)\n",
    "                plotdata = plotdata.set_index(\"Experiment\").loc[multi_choice,:].reset_index()\n",
    "                plotcompartments = px.line(plotdata, x=\"Fraction\", y=\"Profile [% total signal]\", color=\"Experiment\",\n",
    "                                           line_group=\"Map\", line_dash=\"Compartment\", template=\"simple_white\",\n",
    "                                           error_y=\"std\" if \"stdev\" in compare_profile_style else \"sem\")\n",
    "            else:\n",
    "                plotcompartments = \"Please select at least one compartment\"\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Pane(textfragments[\"benchmark_profile_top\"], width=600),\n",
    "            pn.Row(pn.Column(i_compare_gene,\n",
    "                            pn.Pane(plotprofile, config=plotly_config)),\n",
    "                  pn.Column(pn.Row(i_compare_compartment, i_compare_profile_style),\n",
    "                            pn.Pane(plotcompartments, config=plotly_config)))\n",
    "        )\n",
    "    except Exception:\n",
    "        return pn.Column(\n",
    "            pn.Pane(textfragments[\"benchmark_profile_top\"], width=600),\n",
    "            pn.Row(pn.Column(i_compare_gene,\n",
    "                             traceback.format_exc()),\n",
    "                   pn.Column(pn.Row(i_compare_compartment, i_compare_profile_style)))\n",
    "        )\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Download data\n",
    "########################\n",
    "lo_benchmark_download = pn.Column()\n",
    "\n",
    "#### Layout elements\n",
    "#### Download data\n",
    "####################\n",
    "i_benchmark_downloadselector = pn.widgets.Select(options=[\n",
    "    \"0-1 normalized data.csv\",\n",
    "    \"pca coordinates.csv\",\n",
    "    \"complex scatter.csv\",\n",
    "    \"reproducibility.csv\",\n",
    "    \"protein id alignment.csv\",\n",
    "    \"benchmarking results collection.xlsx\"\n",
    "], value=\"0-1 normalized data.csv\", width=200)\n",
    "i_benchmark_download = pn.widgets.FileDownload(label=\"Download file\", width=200, button_type=\"success\")\n",
    "o_benchmark_downloadpreview = pn.Row()\n",
    "\n",
    "#### Append layout to dashboard\n",
    "#### Download data\n",
    "###############################\n",
    "for el in [i_benchmark_downloadselector,i_benchmark_download,o_benchmark_downloadpreview]:\n",
    "    lo_benchmark_download.append(el)\n",
    "\n",
    "#### Callbacks\n",
    "#### Download data\n",
    "##################\n",
    "# benchmark_download_getsheet\n",
    "# benchmark_download_preview\n",
    "# benchmark_download\n",
    "\n",
    "def benchmark_download_getsheet(sheet, mode=\"csv\"):\n",
    "    if sheet == \"0-1 normalized data\":\n",
    "        out = i_class_comp.df_01_filtered_combined.copy()\n",
    "        out.index = out.index.droplevel(\"Exp_Map\")\n",
    "        out = out.unstack([\"Experiment\", \"Map\"])\n",
    "        out.columns = [\"_\".join(el) for el in out.columns.reorder_levels([\"Experiment\", \"Map\", \"Fraction\"]).values]\n",
    "    elif sheet == \"pca coordinates\":\n",
    "        out = i_class_comp.df_pca.copy()\n",
    "    elif sheet == \"complex scatter\":\n",
    "        out = i_class_comp.df_distance_comp.copy()\n",
    "        if mode==\"csv\":\n",
    "            out[\"Cluster\"] = ['\"'+el+'\"' for el in out[\"Cluster\"]]\n",
    "        out = out.set_index(\n",
    "            [\"Cluster\", \"Gene names\", \"Protein IDs\", \"Compartment\", \"Experiment\", \"Map\"]).drop(\n",
    "            [\"Exp_Map\", \"merge type\"], axis=1).unstack([\"Experiment\", \"Map\"]).copy()\n",
    "        out.columns = [\"_\".join(el) for el in out.columns.values]\n",
    "    elif sheet == \"reproducibility\":\n",
    "        out = i_class_comp.distances.copy()\n",
    "    elif sheet == \"protein id alignment\":\n",
    "        out = i_class_comp.id_alignment.copy()\n",
    "        out.columns = [\" \".join(el) for el in out.columns.values]\n",
    "    else:\n",
    "        raise KeyError(sheet)\n",
    "    return out\n",
    "\n",
    "@pn.depends(i_benchmark_downloadselector.param.value, cache_run_json.param.value)\n",
    "def benchmark_download_preview(file_selection, run_json):\n",
    "    if not run_json:\n",
    "        return \"Run analysis first.\"\n",
    "    if file_selection.endswith(\".csv\"):\n",
    "        out = benchmark_download_getsheet(file_selection.split(\".csv\")[0], mode=\"csv\")\n",
    "        return pn.Column(f\"{out.shape[0]} rows x {out.head().reset_index().shape[1]} columns\",\n",
    "                          pn.widgets.DataFrame(\n",
    "            out.head(10),\n",
    "            editable=False)\n",
    "                         )\n",
    "    else:\n",
    "        return \"This will download a .xlsx file with all tables as individual sheets.\"\n",
    "\n",
    "@pn.depends(i_benchmark_downloadselector.param.value)\n",
    "def benchmark_download(file_selection):\n",
    "    i_benchmark_downloadselector.disabled=True\n",
    "    i_benchmark_download.loading=True\n",
    "    # set up list of files to format\n",
    "    if file_selection.endswith(\".csv\"):\n",
    "        sheets = [file_selection.split(\".csv\")[0]]\n",
    "        mode=\"csv\"\n",
    "    elif file_selection.endswith(\".xlsx\"):\n",
    "        sheets = [\n",
    "            \"0-1 normalized data\",\n",
    "            \"pca coordinates\",\n",
    "            \"complex scatter\",\n",
    "            \"reproducibility\",\n",
    "            \"protein id alignment\",\n",
    "        ]\n",
    "        mode=\"xlsx\"\n",
    "    \n",
    "    # format outputs\n",
    "    out_dict = dict()\n",
    "    for sheet in sheets:\n",
    "        out_dict[sheet] = benchmark_download_getsheet(sheet, mode=mode)\n",
    "    \n",
    "    # return file object\n",
    "    i_benchmark_download.filename = file_selection\n",
    "    if file_selection.endswith(\".csv\"):\n",
    "        sio = StringIO()\n",
    "        out_dict[sheets[0]].to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        i_benchmark_download.loading=False\n",
    "        i_benchmark_downloadselector.disabled=False\n",
    "        return sio\n",
    "    elif file_selection.endswith(\".xlsx\"):\n",
    "        bio = BytesIO()\n",
    "        excel = pd.ExcelWriter(bio, engine_kwargs = {\"data_only\": True})\n",
    "        for sheet, df in out_dict.items():\n",
    "            df.reset_index().T.reset_index().T.to_excel(\n",
    "                excel, sheet_name=sheet,\n",
    "                merge_cells=False, index=False, header=False)\n",
    "        excel.save()\n",
    "        bio.seek(0)\n",
    "        i_benchmark_download.loading=False\n",
    "        i_benchmark_downloadselector.disabled=False\n",
    "        return bio\n",
    "i_benchmark_download.callback = benchmark_download\n",
    "\n",
    "#### Callback output positioning\n",
    "#### Download data\n",
    "################################\n",
    "o_benchmark_downloadpreview.append(benchmark_download_preview)\n",
    "\n",
    "#### Dashboard structure\n",
    "#### Overview tab\n",
    "########################\n",
    "\n",
    "#### Layout elements\n",
    "####################\n",
    "\n",
    "#### Append layout to dashboard\n",
    "###############################\n",
    "\n",
    "#### Callbacks\n",
    "##############\n",
    "# update_multi_choice\n",
    "# update_ExpOverview\n",
    "# update_benchmark_overview\n",
    "\n",
    "def update_multi_choice():\n",
    "    i_multi_choice.options = i_class_comp.exp_names\n",
    "    i_complexes_norm.options = [i_complexes_norm.options[0]]+i_class_comp.exp_names\n",
    "    i_clusterwidget.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.value = list(i_class_comp.markerproteins.keys())\n",
    "    i_multi_choice.value = i_class_comp.exp_names\n",
    "    i_svm_set.options = list(set([k for exp in i_class_comp.exp_names if exp in i_class_comp.svm_results.keys() for k in i_class_comp.svm_results[exp].keys()]))\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ExpOverview(multi_choice):\n",
    "    dict_analysis_parameters={}\n",
    "    for exp_name in multi_choice:\n",
    "        dict_analysis_parameters[exp_name] = i_class_comp.json_dict[exp_name][\"Analysis parameters\"]\n",
    "    i_ExpOverview[0] = pn.widgets.DataFrame(pd.DataFrame.from_dict(dict_analysis_parameters), height=300, disabled=True)\n",
    "@pn.depends(i_multi_choice.param.value, i_clusters_for_ranking.param.value,\n",
    "            i_scatter_metric.param.value, i_scatter_consolidation.param.value, i_scatter_quantile.param.value,\n",
    "            cache_run_json.param.value)\n",
    "\n",
    "\n",
    "def update_benchmark_overview(multi_choice, clusters,\n",
    "                              metric, consolidation, quantile,\n",
    "                              run_json):\n",
    "    try:\n",
    "        if run_json == False:\n",
    "            return \"Run analysis first!\"\n",
    "        if multi_choice == []:\n",
    "            return \"Please select experiments for comparison\"\n",
    "        if \"reproducibility consolidation\" not in i_class_comp.parameters.keys() or \\\n",
    "        i_class_comp.parameters[\"reproducibility metric\"] != metric or \\\n",
    "        i_class_comp.parameters[\"reproducibility consolidation\"] != consolidation:\n",
    "            i_class_comp.calculate_global_scatter(metric, consolidation)\n",
    "        if len(clusters) == 0:\n",
    "            return \"Please select clusters for comparison in the intramap scatter tab\"\n",
    "        \n",
    "        fig = i_class_comp.plot_overview(multi_choice, clusters, quantile)\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Pane(fig, config=plotly_config),\n",
    "            \"Note that this overview figure is affected by the settings you choose in the intra- and inter-map scatter tabs.\"\n",
    "        )\n",
    "    except:\n",
    "        return traceback.format_exc()\n",
    "\n",
    "#### Callback output positioning\n",
    "################################\n",
    "\n",
    "\n",
    "comparison_tabs.clear()\n",
    "comparison_tabs.append((\"Overview\", pn.Column(\n",
    "    \"Once you have run the analysis you can find different benchmarking outputs here and dive into the data.\",\n",
    "    update_benchmark_overview\n",
    ")))\n",
    "comparison_tabs.append((\"PCA maps\", lo_benchmark_pca))\n",
    "comparison_tabs.append((\"Depth & Coverage\", lo_benchmark_depth))\n",
    "comparison_tabs.append((\"Intermap scatter\", lo_benchmark_intermap))\n",
    "comparison_tabs.append((\"Intramap scatter\", comparison_tab_bp))\n",
    "comparison_tabs.append((\"SVM Analysis\", lo_benchmark_SVMs))\n",
    "comparison_tabs.append((\"Compare profiles\", update_profile_comparison))\n",
    "comparison_tabs.append((\"Download data\", lo_benchmark_download))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Benchmark tab upload section<a id=\"benchmarkupload\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dashboard_benchmark.objects = [\n",
    "    pn.Card(objects=[], header=\"## Manage data\", name=\"manage_data\", height_policy=\"fit\"),\n",
    "    pn.Row(objects=[], name=\"benchmark_output\", height_policy=\"fit\")\n",
    "]\n",
    "\n",
    "#### Manage data Card layout\n",
    "# This accesses mem_available_datasets and mem_benchmark.\n",
    "####\n",
    "\n",
    "## Adding datasets row\n",
    "i_upload_collection = pn.widgets.FileInput(name=\"Upload collection\")\n",
    "btn_load_reference = pn.widgets.Button(name=\"Load\", width=100)\n",
    "i_load_reference = pn.widgets.Select(options=pkg_resources.resource_listdir(\"domaps\", \"referencedata\"),\n",
    "                                     value=None, width=200)\n",
    "lo_add_datasets = pn.Row(objects=[\n",
    "    \"**Upload collection from file (.json):**\", i_upload_collection,\n",
    "    \"**Add reference set:**\", i_load_reference, btn_load_reference\n",
    "])\n",
    "\n",
    "## Selection checkbox\n",
    "i_dfs_available = pn.widgets.CheckBoxGroup(options=[], value=[],\n",
    "                                           name=\"Datasets available\")\n",
    "lo_dfs_available = pn.WidgetBox(objects=[\"**Datasets available**\", i_dfs_available], width=250)\n",
    "\n",
    "## Management button group\n",
    "i_coll_download = pn.widgets.FileDownload(label=\"Download selected as collection (.json)\", filename=\"AnalysedDatasets.json\", disabled=True)\n",
    "btn_coll_editnames = pn.widgets.Button(name=\"Edit names and comments\", disabled=True) # move from management\n",
    "btn_coll_reannotate = pn.widgets.Button(name=\"Reannotate genes/organelles/complexes\", disabled=True) # new functionality\n",
    "btn_coll_runmain = pn.widgets.Button(name=\"Align and analyse selected datasets\",\n",
    "                                     button_type=\"success\", disabled=True) # change from main comparison\n",
    "btn_coll_dropmem = pn.widgets.Button(name=\"Drop selected datasets from memory\",\n",
    "                                     button_type=\"danger\", disabled=True) # move from top of page\n",
    "lo_coll_buttons = pn.WidgetBox(objects=[\n",
    "    i_coll_download,\n",
    "#    btn_coll_editnames,\n",
    "#    btn_coll_reannotate,\n",
    "    btn_coll_runmain,\n",
    "    btn_coll_dropmem\n",
    "], width=300)\n",
    "\n",
    "## Interaction pane\n",
    "lo_instructions_datamanagement = pn.Card(pn.pane.Markdown(textfragments[\"coll_status_default\"]),\n",
    "                                         header=\"**Explanation**\", width=400, collapsed=True)\n",
    "\n",
    "lo_instructions_error_messages = pn.Card(pn.pane.Markdown(textfragments[\"benchmark_error_messages\"]),\n",
    "                                         header=\"**Common error messages**\", width=400, collapsed=True)\n",
    "\n",
    "\n",
    "o_status_datamanagement = pn.pane.Markdown(width=400)\n",
    "def set_status_datamanagement(x, append=False):\n",
    "    if not append:\n",
    "        o_status_datamanagement.object = x+\"<br><br>\"\n",
    "    else:\n",
    "        o_status_datamanagement.object += x+\"<br><br>\"\n",
    "    if x.startswith(\"Traceback\") or o_status_datamanagement.object.count(\"<br><br>\") > 1:\n",
    "        resize(dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"manage_data\")])\n",
    "    if DEBUG:\n",
    "        time.sleep(0.3)\n",
    "    o_status_datamanagement.object = x\n",
    "\n",
    "set_status_datamanagement(\"Step 1: Add datasets\")\n",
    "o_dynamic_collectionmanagement = pn.Row()\n",
    "lo_coll_interactions = pn.Column(objects=[lo_instructions_datamanagement,\n",
    "                                          #lo_instructions_error_messages, # currently empty\n",
    "                                          o_status_datamanagement,\n",
    "                                          o_dynamic_collectionmanagement])\n",
    "\n",
    "## Assemble collection management row\n",
    "lo_manage_collection = pn.Row(objects=[lo_dfs_available, lo_coll_buttons, lo_coll_interactions])\n",
    "\n",
    "#### Append elements to manage data row\n",
    "dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"manage_data\")].objects = []\n",
    "for el in [\n",
    "    pn.Pane(textfragments[\"benchmark_management_top\"], width=600),\n",
    "    lo_add_datasets, lo_manage_collection\n",
    "]:\n",
    "    dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"manage_data\")].append(el)\n",
    "\n",
    "#### Management callbacks\n",
    "# upload_collection #Done\n",
    "# load_reference\n",
    "# coll_activatebuttons # Done\n",
    "# coll_downloadjson\n",
    "# coll_editnames\n",
    "# coll_reannotate\n",
    "# coll_runmain #Button change in place\n",
    "# coll_dropmem #Done\n",
    "####\n",
    "\n",
    "lock_collection_change = [\n",
    "    #btn_coll_editnames,\n",
    "    #btn_coll_reannotate,\n",
    "    i_coll_download,\n",
    "    btn_coll_runmain,\n",
    "    btn_coll_dropmem,\n",
    "    i_dfs_available,\n",
    "    i_upload_collection,\n",
    "    btn_load_reference\n",
    "]\n",
    "\n",
    "@pn.depends(i_upload_collection.param.value, watch=True)\n",
    "def upload_collection(file):\n",
    "    \"\"\"\n",
    "    This callback adds the datasets from a .json collection to the global memory object\n",
    "    and updates interface elements accordingly.\n",
    "    \"\"\"\n",
    "    if file == None:\n",
    "        return\n",
    "    # deactivate interface\n",
    "    for el in lock_collection_change:\n",
    "        el.disabled = True\n",
    "    try:\n",
    "        set_status_datamanagement(\"Loading data ...\")\n",
    "        \n",
    "        status = \"\"\n",
    "        json_loaded = json.load(BytesIO(file))\n",
    "        \n",
    "        # Check if experiment names are still free\n",
    "        renamed_exps = []\n",
    "        n_sets = 0\n",
    "        keys = list(json_loaded.keys())\n",
    "        for exp in keys:\n",
    "            if exp in i_dfs_available.options:\n",
    "                renamed_exps.append(exp)\n",
    "                json_loaded[exp+i_upload_collection.filename] = json_loaded.pop(exp)\n",
    "            n_sets += 1\n",
    "        if len(renamed_exps) != 0:\n",
    "            status += f\"\"\"These datasets were already available:<br><br>{\", \".join(renamed_exps)}\n",
    "            <br><br>Please rename them prior to analysis.<br><br><br><br>\"\"\"\n",
    "        \n",
    "        # Load datasets into memory\n",
    "        keys = list(json_loaded.keys())\n",
    "        for exp in keys:\n",
    "            set_status_datamanagement(f\"Loading dataset {exp} ...\")\n",
    "            mem_available_datasets[exp] = json_loaded[exp]\n",
    "        \n",
    "        # Adjust list of available dataset\n",
    "        i_dfs_available.options = i_dfs_available.options + keys\n",
    "        i_dfs_available.value = i_dfs_available.value + keys\n",
    "        resize(lo_dfs_available)\n",
    "        \n",
    "        status += f\"Loaded **{n_sets}** datasets from file **{i_upload_collection.filename}**\"\n",
    "        set_status_datamanagement(status)\n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "    finally:\n",
    "        # reactivate interface\n",
    "        for el in lock_collection_change:\n",
    "            if type(el) != pn.widgets.button.Button:\n",
    "                el.disabled = False\n",
    "        coll_activatebuttons(i_dfs_available.value)\n",
    "\n",
    "\n",
    "def load_reference(event):\n",
    "    upload_collection(pkg_resources.resource_stream(\"domaps\", \"referencedata/\"+i_load_reference.value).read())\n",
    "btn_load_reference.on_click(load_reference)\n",
    "\n",
    "\n",
    "@pn.depends(i_dfs_available.param.value, watch=True)\n",
    "def coll_activatebuttons(v):\n",
    "    \"\"\"\n",
    "    Activate/Deactivate buttons based on selection of available datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn_load_reference.disabled = False\n",
    "        if len(v) == 0:\n",
    "            i_coll_download.disabled = True\n",
    "            btn_coll_reannotate.disabled = True\n",
    "            btn_coll_runmain.disabled = True\n",
    "            btn_coll_dropmem.disabled = True\n",
    "        else:\n",
    "            btn_coll_reannotate.disabled = False\n",
    "            i_coll_download.disabled = False\n",
    "            btn_coll_runmain.disabled = False\n",
    "            btn_coll_dropmem.disabled = False\n",
    "        if len(i_dfs_available.options) == 0:\n",
    "            btn_coll_editnames.disabled = True\n",
    "        else:\n",
    "            btn_coll_editnames.disabled = False\n",
    "            pass\n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "\n",
    "@pn.depends(i_dfs_available.param.value)\n",
    "def coll_downloadjson(dfs):\n",
    "    sio = StringIO()\n",
    "    json.dump(\n",
    "        {k: mem_available_datasets[k] for k in dfs}, \n",
    "        sio,\n",
    "        indent=4, \n",
    "        sort_keys=True\n",
    "    )\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "i_coll_download.callback = coll_downloadjson\n",
    "\n",
    "\n",
    "def coll_runmain(event):\n",
    "    \"\"\"\n",
    "    Runs main analysis\n",
    "    \"\"\"\n",
    "    # deactivate interface\n",
    "    for el in lock_collection_change:\n",
    "        el.disabled = True\n",
    "    try:\n",
    "        if btn_coll_runmain.button_type == \"success\":\n",
    "            set_status_datamanagement(\"Aligning and analysing data ...\")\n",
    "            cache_run_json.value=False\n",
    "            #### Main execution of the comparison\n",
    "            loading_status_comparison.objects = [loading_comparison]\n",
    "            selection = i_dfs_available.value\n",
    "            global i_class_comp\n",
    "            i_class_comp = domaps.SpatialDataSetComparison(ref_exp=selection[0])#, clusters_for_ranking=protein_cluster, organism=i_organism_comparison.value)\n",
    "            i_class_comp.json_dict = {k: mem_available_datasets[k] for k in selection}\n",
    "            set_status_datamanagement(\"Aligning data ...\", append=True)\n",
    "            i_class_comp.read_jsonFile()\n",
    "            set_status_datamanagement(\"Analysing intra-map scatter ...\", append=True)\n",
    "            i_class_comp.calc_biological_precision()\n",
    "            i_class_comp.get_complex_coverage()\n",
    "            update_multi_choice()\n",
    "            set_status_datamanagement(\"Running PCA ...\", append=True)\n",
    "            i_class_comp.perform_pca_comparison()\n",
    "            i_pca_comp_ncomp.end=len(i_class_comp.df_01_filtered_combined.columns)\n",
    "            i_compare_compartment.options = list(set(\n",
    "                i_class_comp.df_01_filtered_combined.index.get_level_values(\"Compartment\")))\n",
    "            loading_status_comparison.objects = []\n",
    "            m_diverget_fractions.object = \"\" if not i_class_comp.mixed else \"**Caution: You are comparing experiments with differently labelled fractions. This does not affect distance metrics, but the PCA and profile plots.**\"\n",
    "            m_diverget_fractions.background = None if not i_class_comp.mixed else \"salmon\"\n",
    "            cache_run_json.value=True\n",
    "            set_status_datamanagement(\"Comparison finished!\", append=True)\n",
    "            \n",
    "            #### Switch button mode\n",
    "            btn_coll_runmain.button_type = \"danger\"\n",
    "            btn_coll_runmain.name = \"Reset analysis to make new selection\"\n",
    "            set_status_datamanagement(\n",
    "                \"Next step: Use interface below to evaluate and download benchmark results.\", append=True)\n",
    "        elif btn_coll_runmain.button_type == \"danger\":\n",
    "            set_status_datamanagement(\"Resetting data analysis ...\")\n",
    "            cache_run_json.value=False\n",
    "            \n",
    "            btn_coll_runmain.button_type = \"success\"\n",
    "            btn_coll_runmain.name = \"Align and analyse selected datasets\"\n",
    "            set_status_datamanagement(\"Analysis results have been reset.\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "        cache_run_json.value=False\n",
    "        \n",
    "    finally:\n",
    "        # reactivate interface\n",
    "        if btn_coll_runmain.button_type == \"danger\":\n",
    "            for el in [\n",
    "                i_coll_download,\n",
    "                btn_coll_runmain,\n",
    "                btn_coll_editnames\n",
    "            ]:\n",
    "                el.disabled = False\n",
    "        elif btn_coll_runmain.button_type == \"success\":\n",
    "            for el in lock_collection_change:\n",
    "                if type(el) != pn.widgets.button.Button:\n",
    "                    el.disabled = False\n",
    "            coll_activatebuttons(i_dfs_available.value)\n",
    "        else:\n",
    "            for el in lock_collection_change:\n",
    "                if type(el) != pn.widgets.button.Button:\n",
    "                    el.disabled = False\n",
    "            coll_activatebuttons(i_dfs_available.value)\n",
    "            \n",
    "btn_coll_runmain.on_click(coll_runmain)\n",
    "        \n",
    "def coll_dropmem(event):\n",
    "    \"\"\"\n",
    "    Drops selected datasets from collection stored in RAM.\n",
    "    \"\"\"\n",
    "    # deactivate interface\n",
    "    for el in lock_collection_change:\n",
    "        el.disabled = True\n",
    "    try:\n",
    "        set_status_datamanagement(\"Deleting data ...\")\n",
    "        \n",
    "        keys = list(i_dfs_available.value)\n",
    "        for exp in keys:\n",
    "            set_status_datamanagement(f\"Deleting dataset {exp} ...\")\n",
    "            del mem_available_datasets[exp]\n",
    "            \n",
    "        # Adjust list of available dataset\n",
    "        i_dfs_available.options = [el for el in i_dfs_available.options if el not in keys]\n",
    "        i_dfs_available.value = [el for el in i_dfs_available.value if el not in keys]\n",
    "        resize(lo_dfs_available)\n",
    "        i_dfs_available.disabled = True\n",
    "        \n",
    "        set_status_datamanagement(f\"Deleted **{len(keys)}** datasets **{', '.join(keys)}**\")\n",
    "        \n",
    "    except Exception:\n",
    "        set_status_datamanagement(traceback.format_exc())\n",
    "    finally:\n",
    "        # reactivate interface\n",
    "        for el in lock_collection_change:\n",
    "            if type(el) != pn.widgets.button.Button:\n",
    "                el.disabled = False\n",
    "        coll_activatebuttons(i_dfs_available.value)\n",
    "btn_coll_dropmem.on_click(coll_dropmem)\n",
    "\n",
    "#### Benchmark output\n",
    "@pn.depends(cache_run_json.param.value)\n",
    "def display_benchmark_output(run_json):\n",
    "    if run_json:\n",
    "        return pn.Column(\n",
    "            \"## Benchmark results\",\n",
    "            \"Select dataset overlap to plot:\",\n",
    "            i_multi_choice,\n",
    "            comparison_tabs,\n",
    "            sizing_mode=\"stretch_width\"\n",
    "        )\n",
    "    else:\n",
    "        return \"Select data and run analysis.\"\n",
    "\n",
    "#### Append callback output to benchmark output\n",
    "dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"benchmark_output\")].objects = []\n",
    "for el in [display_benchmark_output]:\n",
    "    dashboard_benchmark.objects[[i.name for i in dashboard_benchmark].index(\"benchmark_output\")].append(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div style=\"text-align: right; font-size: 8pt\">back to top</div>](#TOC)\n",
    "## Code interactions<a id=\"interactions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case of loading a json comparison larger than 80 MB\n",
    "#with open(r\"C:\\Documents\\AnalysedDatasets.json\", \"br\") as file:\n",
    "#    i_upload_collection.value = file.read()\n",
    "#    i_upload_collection.filename = \"bla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case of loading a dingle file larger than 80 MB\n",
    "#i_FileConfig._content.file.filename = \"proteinGroups.txt\"\n",
    "#with open(r\"path\\proteinGroups.txt\", \"br\") as file:\n",
    "#    i_FileConfig._content.file.value = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Set the order of the multi choice widget manually\n",
    "#i_multi_choice.value=[\"21 min\", \"44 min\", \"100 min\", \"DDA\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
