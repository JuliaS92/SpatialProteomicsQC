{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import traceback\n",
    "import panel as pn\n",
    "pn.extension(\"plotly\")\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "from bokeh.models.widgets.tables import NumberFormatter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.findall('(\\D+)',k)\n",
    "#isteEe = [\"02\", \"33\", \"3\"]\n",
    "#items.zfill(2) for items in listeEe]\n",
    "#e.findall('(\\d+)',k)[0].zfill(2)\n",
    "#=[[re.match(name_pattern, col).group(\"frac\")] for col in df_index_LFQ.columns]\n",
    "#isteeeee = list(list(zip(*k))[0])\n",
    "#items.zfill(3) for items in listeeeee]\n",
    "#items.zfill(2) for items in [re.match(name_pattern, col).group(\"frac\")] for col in df_index_LFQ.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stringg= [\"Ratio H/L count MAP1_03K\", \"Ratio H/L variability [%] MAP1_03K\", \"Ratio H/L count MAP1_03K\", \"Ratio H/L variability [%] MAP1_03K\"]\n",
    "#stringg = LFQ intensity Control_Cyt_1\n",
    "#[re.findall(sets_SILAC, stringg) for col in stringg]\n",
    "#k=re.match(\".* (?P<rep>.*)_(?P<frac>.*)\", stringg).group(\"rep\")\n",
    "#k\n",
    "#[[s for s in sets_SILAC if col.startswith(s)][0] for col in stringg]\n",
    "#[re.match(name_pattern_SILAC, col).group(\"frac\")]\n",
    "#[re.match(\"(?P<sets>.*) (?P<rep>.*)_(?P<frac>.*)\", col).group(\"rep\") for col in stringg]\n",
    "#[re.match(\"(?P<sets>.*) (?P<rep>.*)_(?P<frac>.*)\", col).group(\"sets\") for col in stringg]\n",
    "\n",
    "\n",
    "#bool([re.match(name_pattern_SILAC, col) for col in i_class.df_original.columns])\n",
    "#any([[col.startswith(s) for s in sets_SILAC] for col in i_class.df_original.columns])\n",
    "\n",
    "#[items.zfill(3) for items in [\"03K\", \"12K\"]]\n",
    "\n",
    "#bool([re.match(name_pattern_SILAC, col) for col in i_class.df_original.columns])\n",
    "\n",
    "#[re.match(name_pattern_SILAC, col).group(\"sets\") for col in df_index_SILAC.columns]\n",
    "#[col for col in df_index.columns if [re.match(name_pattern_SILAC, col).group(\"\") for s in sets_SILAC]]\n",
    "#len([[s for s in sets_LFQ if re.match(name_pattern_SILAC, col)][0] for col in df_index.columns])\n",
    "\n",
    "\n",
    "#sets_SILAC = [\"Ratio H/L variability [%]\", \"Ratio H/L count\", \"Ratio H/L\"]\n",
    "#\"type_count_silac\": \"([Rr]atio.[Hh]/[Ll].[cC]ount)[ .].*_.*K\",\n",
    "#            \"type_var_silac\": \"([Rr]atio.[Hh]/[Ll].[Vv]ariability....)[ .].*_.*K\",\n",
    "#            \"type_ratio_silac\": \"([rR]atio.[Hh]/[Ll])[ .](?![cC]ount|[Vv]ariability).*_.*K\",\n",
    "\n",
    "#negative lookaheadf\n",
    "#if len re.findall\n",
    "#([[re.match(s, col) for s in sets_SILAC] for col in test_liast])\n",
    "\n",
    "\n",
    "#i_acquisition = pn.widgets.Select(options=[\"LFQ\",\"SILAC\"])\n",
    "#i_class.acquisition= i_acquisition.value\n",
    "#i_sets = pn.widgets.Select(options=acquisition_set_dic[i_acquisition.value])\n",
    "#\n",
    "#@pn.depends(i_acquisition.param.value, watch=True)\n",
    "#def update_sets(acquisition):\n",
    "#    acquistion = acquisition_set_dic[acquisition]\n",
    "#    i_sets.options = acquistion\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDataSet:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Import of the raw file of interest. Dataframe will be generated, which contains only the data of desired\n",
    "        column names, specified by the dictionary entry regex[\"col_shortened\"]\n",
    "\n",
    "        Dictionaries are created, that ate used for filtering and plotting, respectively.\n",
    "\n",
    "        Args:\n",
    "            filename: raw file obtained by the LFQ/SILAC approach (Protein Groups Files), processed by MaxQuant\n",
    "\n",
    "        Returns:\n",
    "            df_original: shortened data frame, contains only the information, which was determined due to\n",
    "            the values of the key \"col_shortened\"\n",
    "        \"\"\"\n",
    "        # df_original contains all information of the raw file; tab separated file is imported,\n",
    "        # without considering comments, marked with #\n",
    "\n",
    "        self.filename = \"6_deep_maps.txt\" if \"filename\" not in kwargs.keys() else kwargs[\"filename\"]\n",
    "        \n",
    "        self.map_of_interest = \"MAP1\" if \"map_of_interest\" not in kwargs.keys() else kwargs[\"map_of_interest\"]\n",
    "        self.cluster_of_interest = \"Proteasome\" if \"cluster_of_interest\" not in kwargs.keys() else kwargs[\n",
    "            \"cluster_of_interest\"]\n",
    "\n",
    "        self.summed_MSMS_counts = 2 if \"summed_MSMS_counts\" not in kwargs.keys() else kwargs[\"summed_MSMS_counts\"]\n",
    "        self.consecutive_LFQ_I = 4 if \"consecutive_LFQ_I\" not in kwargs.keys() else kwargs[\"consecutive_LFQ_I\"]\n",
    "        \n",
    "        self.RatioHLcount_1 = 3 if \"RatioHLcount_1\" not in kwargs.keys() else kwargs[\"RatioHLcount_1\"]\n",
    "        self.RatioHLcount_2 = 2 if \"RatioHLcount_2\" not in kwargs.keys() else kwargs[\"RatioHLcount_2\"]\n",
    "        self.RatioVariability = 30 if \"RatioVariability\" not in kwargs.keys() else kwargs[\"RatioVariability\"]\n",
    "        self.markerproteins = {\n",
    "            \"Proteasome\": [\"PSMA1\", \"PSMA2\", \"PSMA3\", \"PSMA4\", \"PSMA5\", \"PSMA6\", \"PSMA7\", \"PSMB1\", \"PSMB2\", \"PSMB3\",\n",
    "                           \"PSMB4\", \"PSMB5\", \"PSMB6\", \"PSMB7\"],\n",
    "            \"CCT complex\": [\"CCT2\", \"CCT3\", \"CCT4\", \"CCT5\", \"CCT6A\", \"CCT7\", \"CCT8\",\"CCT6B\", \"TCP1\"],\n",
    "            \"V-type proton ATPase\": [\"ATP6AP1\", \"ATP6V0A1\", \"ATP6V0A2\", \"ATP6V0A4\", \"ATP6V0D1\", \"ATP6V1A\", \"ATP6V1B2\",\n",
    "                                     \"ATP6V1C1\", \"ATP6V1E1\", \"ATP6V1G1\", \"ATP6V1H\"],\n",
    "            \"EMC\": [\"EMC1\", \"EMC2\", \"EMC3\", \"EMC4\", \"EMC7\", \"EMC8\", \"EMC10\",\"EMC6\",\"EMC9\"],\n",
    "            \"Lysosome\" : [\"LAMTOR1\", \"LAMTOR2\", \"LAMTOR3\", \"LAMTOR4\", \"LAMTOR5\", \"LAMP1\", \"LAMP2\", \"CTSA\", \"CTSB\", \"CTSC\", \"CTSD\", \"CTSL\", \"CTSZ\"]\n",
    "            }    \n",
    "        self.regex = {\n",
    "            \"imported_columns\": \"^[Rr]atio H/L (?!normalized|type|is.*).+|id$|[Mm][Ss].*[cC]ount.+$|[Ll][Ff][Qq].*|.*[nN]ames.*|.*[Pp][rR].*[Ii][Dd]s.*|[Pp]otential.[cC]ontaminant|[Oo]nly.[iI]dentified.[bB]y.[sS]ite|[Rr]everse|[Ss]core|[Qq]-[Vv]alue\",\n",
    "#            \"index_col_silac\": \"[Pp]rotein.[Ii][Dd]s|[Mm]ajority.[Pp]rotein.[Ii][Dd]s|[Pp]rotein.[Nn]ames|[Gg]ene.[Nn]ames|[Ii][Dd]|[Ss]core|[Qq]-[Vv]alue\",\n",
    "#            \"index_col_lfq\": \".*[Pp][rR].*[Ii][Dd]s.*|.*[nN]ames.*|[Ii][Dd]|[Ss]core|[Qq]-[Vv]alue|MS/MS.count$\",\n",
    "\n",
    "#            \"type_count_silac\": \"([Rr]atio.[Hh]/[Ll].[cC]ount)[ .].*_.*K\",\n",
    "#            \"type_var_silac\": \"([Rr]atio.[Hh]/[Ll].[Vv]ariability....)[ .].*_.*K\",\n",
    "#            \"type_ratio_silac\": \"([rR]atio.[Hh]/[Ll])[ .](?![cC]ount|[Vv]ariability).*_.*K\",\n",
    "#\n",
    "#            \"type_count_lfq\": \"([Rr]atio.[Hh]/[Ll].[cC]ount)[ .].*_.*K\",\n",
    "#            \"type_var_lfq\": \"([Rr]atio.[Hh]/[Ll].[Vv]ariability....)[ .].*_.*K\",\n",
    "#            \"type_ratio_silac\": \"([rR]atio.[Hh]/[Ll])[ .](?![cC]ount|[Vv]ariability).*_.*K\",\n",
    "#\n",
    "#            \"type_msms_lfq\": \"([Mm][Ss]/[Mm][Ss].[cC]ount)[ .].*_.*K\",\n",
    "#            \"type_intensity_lfq\": \"([Ll][Ff][Qq].[Ii]ntensity)[ .].*_.*K\",\n",
    "\n",
    "            # \"type_lfq\": \"(.*[nNTt]{1}[yYtT]{1})[ .].*_\\d+[Kk]$\",\n",
    "\n",
    "            #\"lfq_nan\": \"[Ll][Ff][Qq].*\",\n",
    "\n",
    "            #\"contaminants\": \"[Pp]otential.[cC]ontaminant\",\n",
    "            #\"sites\": \"[Oo]nly.[iI]dentified.[bB]y.[sS]ite\",\n",
    "            #\"reverse\": \"[Rr]everse\"\n",
    "        }\n",
    "        \n",
    "        self.acquisition = \"SILAC\" if \"acquisition\" not in kwargs.keys() else kwargs[\"acquisition\"]\n",
    "        \n",
    "        self.acquisition_set_dict = {\n",
    "            'LFQ': [\"[Ll][Ff][Qq].[Ii]ntensity\", \"[Mm][Ss]/[Mm][Ss].[cC]ount\", \"[Ii]ntensity\"],\n",
    "            'SILAC'  : [ \"[Rr]atio.[Hh]/[Ll](?!.[Vv]aria|.[Cc]ount)\",\"[Rr]atio.[Hh]/[Ll].[Vv]ariability.\\[%\\]\", \"[Rr]atio.[Hh]/[Ll].[cC]ount\"]\n",
    "            }\n",
    "        \n",
    "        self.name_pattern = \".* (?P<cond>.*)_(?P<rep>.*)_(?P<frac>.*)\" if \"name_pattern\" not in kwargs.keys() else kwargs[\"name_pattern\"]\n",
    "        \n",
    "        self.fraction_dict = {\"1K\": \"01K\",\"3K\": \"03K\", \"6K\": \"06K\", \"12K\": \"12K\", \"24K\": \"24K\", \"80K\": \"80K\", \n",
    "                              \"01K\": \"01K\",\"03K\": \"03K\", \"06K\": \"06K\", \"012K\": \"12K\", \"024K\": \"24K\", \"080K\": \"80K\", \n",
    "                              \"Cyt\": \"Cyt\", \"Mem\": \"Mem\", \"Nuc\": \"Nuc\", \"Prot\": \"Prot\", \"cyt\": \"Cyt\"}\n",
    "        \n",
    "        self.analysed_datasets_dict = {}\n",
    "        self.analysis_summary_dict = {}\n",
    "        self.shape_dict = {}  \n",
    "        self.expname = \"Protein_Groups\" if \"expname\" not in kwargs.keys() else kwargs[\"expname\"]\n",
    "        \n",
    "    def data_reading(self):\n",
    "        \"\"\"Data import.\n",
    "\n",
    "        Args:\n",
    "            filename: stored as attribute\n",
    "\n",
    "        Returns:\n",
    "            df_orginal: raw, unprocessed dataframe, single level column index\n",
    "        \"\"\"\n",
    "\n",
    "        self.df_original = pd.read_csv(self.filename, sep=\"\\t\", comment=\"#\",\n",
    "                                       usecols=lambda x: bool(re.match(self.regex[\"imported_columns\"], x)))\n",
    "\n",
    "        return self.df_original\n",
    "    \n",
    "    def processingdf(self):\n",
    "        \"\"\"Analysis of the SILAC/LFQ data will be performed.\n",
    "\n",
    "        The dataframe will be filtered, normalized and converted into a dataframe, characterized by a flat column index,\n",
    "        which can be used for plotting\n",
    "\n",
    "        Args:\n",
    "            acquisition mode: \"SILAC\" or \"LFQ\", which is referring to the acquisition method\n",
    "\n",
    "        Returns:\n",
    "            A dataframe, in which \"Fraction\" and \"Map\" are stacked, containing \"normalized profile\" as column,\n",
    "            additionally \"Ratio H/L count\", \"Ratio H/L variability [%]\" is found for SILAC data and \"MS/MS count\"\n",
    "            for LFQ data; represented as a flat column index\n",
    "        \"\"\"\n",
    "    \n",
    "        def indexingdf(df_original, acquisition_set_dict, acquisition, fraction_dict, name_pattern, shape_dict):\n",
    "            # deep copy of the dataframe\n",
    "            df_original = self.df_original.copy()\n",
    "            df_i = df_original.set_index([col for col in df_original.columns if any([re.match(s, col) for s in self.acquisition_set_dict[self.acquisition]]) == False])\n",
    "    \n",
    "            # multindex will be generated, by isolating the information about the Map, Fraction and Type from each\n",
    "            # individual column name\n",
    "            # names=[\"Set\", \"Map\", \"Fraction\"] defines the label of the multiindex\n",
    "            multiindex = pd.MultiIndex.from_arrays(\n",
    "                    arrays=[\n",
    "                        [item for sublist in [[re.findall(s, col)[0] for s in self.acquisition_set_dict[self.acquisition] if re.match(s,col)] for col in df_i.columns] for item in sublist],\n",
    "            \n",
    "                        [re.match(self.name_pattern, col).group(\"rep\") for col in df_i.columns] \n",
    "                        if not \"<cond>\" in self.name_pattern \n",
    "                        else [\"_\".join(re.match(self.name_pattern, col).group(\"cond\", \"rep\")) for col in df_i.columns],\n",
    "                        \n",
    "                        [self.fraction_dict[re.match(self.name_pattern, col).group(\"frac\")] for col in df_i.columns],\n",
    "                    ],\n",
    "                    names=[\"Set\", \"Map\", \"Fraction\"]\n",
    "                        )\n",
    "            df_i.columns = multiindex\n",
    "            df_i.sort_index(1, inplace=True)\n",
    "            \n",
    "            self.shape_dict[\"Shape before categorical filtering\"]=df_i.shape\n",
    "            \n",
    "            df_index = df_i.xs(\n",
    "                    np.nan, 0, \"Reverse\").xs(\n",
    "                    np.nan, 0, \"Potential contaminant\").xs(\n",
    "                    np.nan, 0, \"Only identified by site\")\n",
    "            \n",
    "            self.shape_dict[\"Shape after categorical filtering\"]=df_i.shape\n",
    "            df_index.replace(0, np.nan, inplace=True)\n",
    "            self.df_index = df_index\n",
    "            return df_index   \n",
    "    \n",
    "\n",
    "        def stringency_silac(df_index):\n",
    "            \"\"\"The multiindex dataframe is subjected to stringency filtering. Only Proteins with complete profiles are\n",
    "            considered (a set of f.e. 5 SILAC ratios in case you have 5 fractions / any proteins with missing values\n",
    "            were rejected). Proteins were retained with 3 or more quantifications in each subfraction (=count). Furthermore,\n",
    "            proteins with only 2 quantification events in one or more subfraction were retained, if their ratio\n",
    "            variability for ratios obtained with 2 quantification events was below 30% (=var).\n",
    "            SILAC ratios were linearly normalized by division through the fraction median. Subsequently normalization\n",
    "            to SILAC loading was performed.\n",
    "\n",
    "            Args:\n",
    "                df_index: multiindex dataframe, which contains 3 level labels: MAP, Fraction, Type\n",
    "\n",
    "            Returns:\n",
    "                df_stringency_mapfracstacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked;\n",
    "                the columns \"Ratio H/L count\", \"Ratio H/L variability [%]\", and \"Ratio H/L\" stored as single level indices\n",
    "            \"\"\"\n",
    "\n",
    "            # Fraction and Map will be stacked\n",
    "            df_stack = df_index.stack([\"Fraction\", 'Map'])\n",
    "\n",
    "            len_fractions = len(df_stack.index.get_level_values(\"Fraction\").unique())\n",
    "\n",
    "            # filtering for sufficient number of quantifications (count in 'Ratio H/L count'), taken\n",
    "            # variability (var in Ratio H/L variability [%]) into account\n",
    "            # zip: allows direct comparison of count and var\n",
    "            # only if the filtering parameters are fulfilled the data will be introduced into df_countvarfiltered_stacked\n",
    "            \n",
    "            df_countvarfiltered_stacked = df_stack.loc[[count >= self.RatioHLcount_1 or (count >= self.RatioHLcount_2 and var < self.RatioVariability) \n",
    "                                            for var, count in zip(df_stack[\"Ratio H/L variability [%]\"], df_stack['Ratio H/L count'])]]\n",
    "            \n",
    "            self.shape_dict[\"Shape after Ratio H/L count (>= 3)/var (count>=2, var<30) filtering\"]=df_countvarfiltered_stacked.shape\n",
    "\n",
    "            # \"Ratio H/L\":normalization to SILAC loading, each individual experiment (FractionXMap) will be divided by its median\n",
    "            # np.median([...]): only entries, that are not NANs are considered\n",
    "            df_normsilac_stacked = df_countvarfiltered_stacked[\"Ratio H/L\"].unstack([\"Fraction\", \"Map\"]).apply(\n",
    "                lambda x: x / np.median([el for el in x if not np.isnan(el)]), axis=0).stack([\"Map\", \"Fraction\"])\n",
    "\n",
    "            df_stringency_mapfracstacked = df_countvarfiltered_stacked[[\"Ratio H/L count\",\n",
    "                                                                    \"Ratio H/L variability [%]\"]].join(\n",
    "                pd.DataFrame(df_normsilac_stacked, columns=[\"Ratio H/L\"]))\n",
    "\n",
    "            # dataframe is grouped (Map, id), that allows the filtering for complete profiles\n",
    "            df_stringency_mapfracstacked = df_stringency_mapfracstacked.groupby([\"Map\", \"id\"]).filter(\n",
    "                lambda x: len(x) >= len_fractions)\n",
    "            \n",
    "            self.shape_dict[\"Shape after filtering for complete profiles\"]=df_stringency_mapfracstacked.shape\n",
    "            \n",
    "            # Ratio H/L is converted into Ratio L/H\n",
    "            df_stringency_mapfracstacked[\"Ratio H/L\"] = df_stringency_mapfracstacked[\"Ratio H/L\"].transform(lambda x: 1 / x)\n",
    "\n",
    "\n",
    "            return df_stringency_mapfracstacked\n",
    "\n",
    "\n",
    "        def normalization_01_silac(df_stringency_mapfracstacked):\n",
    "            \"\"\"The multiindex dataframe, that was subjected to stringency filtering, is 0-1 normalized (\"Ratio H/L\").\n",
    "\n",
    "            Args:\n",
    "                df_stringency_mapfracstacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked;\n",
    "                the columns \"Ratio H/L count\", \"Ratio H/L variability [%]\", and \"Ratio H/L\" stored as single level indices\n",
    "\n",
    "            Returns:\n",
    "                df_01_stacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked; data in the column\n",
    "                \"Ratio H/L\" is 0-1 normalized and renamed to \"normalized profile\"; the columns \"Ratio H/L count\",\n",
    "                \"Ratio H/L variability [%]\", and \"normalized profile\" stored as single level indices;\n",
    "                plotting is possible now\n",
    "            \"\"\"\n",
    "\n",
    "            df_01norm_unstacked = df_stringency_mapfracstacked[\"Ratio H/L\"].unstack(\"Fraction\")\n",
    "\n",
    "            # 0:1 normalization of Ratio L/H\n",
    "            df_01norm_unstacked = df_01norm_unstacked.div(df_01norm_unstacked.sum(axis=1), axis=0)\n",
    "\n",
    "            df_01_stacked = df_stringency_mapfracstacked[[\"Ratio H/L count\", \"Ratio H/L variability [%]\"]].join(pd.DataFrame\n",
    "                (\n",
    "                df_01norm_unstacked.stack(\n",
    "                    \"Fraction\"),\n",
    "                columns=[\n",
    "                    \"Ratio H/L\"]))\n",
    "\n",
    "            # \"Ratio H/L\" will be renamed to \"normalized profile\"\n",
    "            df_01_stacked.columns = [col if col != \"Ratio H/L\" else \"normalized profile\" for col in\n",
    "                                     df_01_stacked.columns]\n",
    "\n",
    "            return df_01_stacked\n",
    "\n",
    "\n",
    "        def logarithmization_silac(df_stringency_mapfracstacked):\n",
    "            \"\"\"The multiindex dataframe, that was subjected to stringency filtering, is logarithmized (\"Ratio H/L\").\n",
    "\n",
    "            Args:\n",
    "                df_stringency_mapfracstacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked;\n",
    "                the columns \"Ratio H/L count\", \"Ratio H/L variability [%]\", and \"Ratio H/L\" stored as single level indices\n",
    "\n",
    "            Returns:\n",
    "                df_log_stacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked; data in the column\n",
    "                \"log profile\" originates from logarithmized \"Ratio H/L\" data; the columns \"Ratio H/L count\",\n",
    "                \"Ratio H/L variability [%]\" and  \"log profile\" are stored as single level indices; PCA is possible now\n",
    "\n",
    "            \"\"\"\n",
    "            # logarithmizing, basis of 2\n",
    "            df_lognorm_ratio_stacked = df_stringency_mapfracstacked[\"Ratio H/L\"].transform(np.log2)\n",
    "            df_log_stacked = df_stringency_mapfracstacked[[\"Ratio H/L count\", \"Ratio H/L variability [%]\"]].join(\n",
    "                pd.DataFrame\n",
    "                (df_lognorm_ratio_stacked, columns=[\"Ratio H/L\"]))\n",
    "\n",
    "            # \"Ratio H/L\" will be renamed to \"log profile\"\n",
    "            df_log_stacked.columns = [col if col != \"Ratio H/L\" else \"log profile\" for col in df_log_stacked.columns]\n",
    "\n",
    "            return df_log_stacked\n",
    "\n",
    "        \n",
    "        def stringency_lfq(df_index):\n",
    "            \"\"\"The multiindex dataframe is subjected to stringency filtering. Only Proteins which were identified with\n",
    "            at least [4] consecutive data points regarding the \"LFQ intensity\", and if summed MS/MS counts >= n(fractions)*[2]\n",
    "            (LFQ5: min 10 and LFQ6: min 12, respectively; coverage filtering) were included.\n",
    "\n",
    "            Args:\n",
    "                df_index: multiindex dataframe, which contains 3 level labels: MAP, Fraction, Typ\n",
    "\n",
    "            Returns:\n",
    "                df_stringency_mapfracstacked: dataframe, in which \"Map\" and \"Fraction\" is stacked;\n",
    "                \"LFQ intensity\" and \"MS/MS count\" define a single-level column index\n",
    "                \"\"\"\n",
    "\n",
    "            # retrieve number of fractions that are present in the dataset\n",
    "            df_fractionnumber_stacked = df_index.copy().stack(\"Fraction\")\n",
    "            number_fractions = len(df_fractionnumber_stacked.index.get_level_values(\"Fraction\").unique())\n",
    "\n",
    "            df_index = df_index.stack(\"Map\")\n",
    "\n",
    "            # sorting the level 0, in order to have LFQ intensity -\tMS/MS count instead of continuous alternation\n",
    "            df_index.sort_index(axis=1, level=0, inplace=True)\n",
    "\n",
    "            # \"MS/MS count\"-column: take the sum over the fractions;\n",
    "            # if the sum is larger than n[fraction]*2, it will be stored in the new dataframe\n",
    "            df_mscount_mapstacked = df_index.loc[df_index[('MS/MS count')].apply(np.sum, axis=1) >= (\n",
    "                    number_fractions * self.summed_MSMS_counts)]\n",
    "\n",
    "            self.shape_dict[\"Shape after MS/MS value filtering\"]=df_mscount_mapstacked.shape\n",
    "            \n",
    "            df_stringency_mapfracstacked = df_mscount_mapstacked.copy()\n",
    "\n",
    "            # series no dataframe is generated; if there are at least i.e. 4 consecutive non-NANs, data will be retained\n",
    "            df_stringency_mapfracstacked = df_stringency_mapfracstacked.loc[\n",
    "                df_stringency_mapfracstacked[(\"LFQ intensity\")].apply(lambda x: any(\n",
    "                    np.invert(np.isnan(x)).rolling(window=self.consecutive_LFQ_I).sum() >=\n",
    "                    self.consecutive_LFQ_I), axis=1)]\n",
    "            \n",
    "            self.shape_dict[\"Shape after consecutive value filtering\"]=df_stringency_mapfracstacked.shape\n",
    "\n",
    "            df_stringency_mapfracstacked = df_stringency_mapfracstacked.copy().stack(\"Fraction\")\n",
    "\n",
    "            return df_stringency_mapfracstacked\n",
    "\n",
    "\n",
    "        def normalization_01_lfq(df_stringency_mapfracstacked):\n",
    "            \"\"\"The multiindex dataframe, that was subjected to stringency filtering, is 0-1 normalized (\"LFQ intensity\").\n",
    "\n",
    "            Args:\n",
    "                df_stringency_mapfracstacked: dataframe, in which \"Map\" and \"Fraction\" is stacked;\n",
    "                \"LFQ intensity\" and \"MS/MS count\" define a single-level column index\n",
    "\n",
    "\n",
    "            Returns:\n",
    "                df_01_stacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked; data in the column\n",
    "                \"LFQ intensity\" is 0-1 normalized and renamed to \"normalized profile\";\n",
    "                the columns \"\"normalized profile\"\" and \"MS/MS count\" are stored as\n",
    "                single level indices; plotting is possible now\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            df_01norm_mapstacked = df_stringency_mapfracstacked[\"LFQ intensity\"].unstack(\"Fraction\")\n",
    "\n",
    "            # 0:1 normalization of Ratio L/H\n",
    "            df_01norm_unstacked = df_01norm_mapstacked.div(df_01norm_mapstacked.sum(axis=1), axis=0)\n",
    "\n",
    "            df_01_stacked = df_stringency_mapfracstacked[[\"MS/MS count\"]].join(pd.DataFrame(df_01norm_unstacked.stack(\n",
    "                   \"Fraction\"),columns=[\"LFQ intensity\"]))\n",
    "\n",
    "            # rename columns: \"LFQ intensity\" into \"normalized profile\"\n",
    "            df_01_stacked.columns = [col if col != \"LFQ intensity\" else \"normalized profile\" for col in\n",
    "                                     df_01_stacked.columns]\n",
    "            df_01_stacked = df_01_stacked.sort_index()\n",
    "            return df_01_stacked\n",
    "\n",
    "\n",
    "        def logarithmization_lfq(df_stringency_mapfracstacked):\n",
    "            \"\"\"The multiindex dataframe, that was subjected to stringency filtering, is logarithmized (\"LFQ intensity\").\n",
    "\n",
    "            Args:\n",
    "                df_stringency_mapfracstacked: dataframe, in which \"Map\" and \"Fraction\" is stacked;\n",
    "                \"LFQ intensity\" and \"MS/MS count\" define a single-level column index\n",
    "\n",
    "            Returns:\n",
    "                df_log_stacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked; data in the column \"log profile\"\n",
    "                originates from logarithmized  \"LFQ intensity\"; the columns \"log profile\" and \"MS/MS count\" are\n",
    "                stored as single level indices; PCA is possible now\n",
    "            \"\"\"\n",
    "\n",
    "            df_lognorm_ratio_stacked = df_stringency_mapfracstacked[\"LFQ intensity\"].transform(np.log2)\n",
    "            #df_log_stacked = df_stringency_mapfracstacked.copy()\n",
    "            #df_log_stacked[\"LFQ intensity\"] = df_lognorm_mapstacked\n",
    "            #df_log_stacked = df_log_stacked.fillna(0).stack(\"Fraction\")\n",
    "\n",
    "            df_log_stacked = df_stringency_mapfracstacked[[\"MS/MS count\"]].join(pd.DataFrame\n",
    "                (df_lognorm_ratio_stacked, columns=[\"LFQ intensity\"]))\n",
    "\n",
    "            # \"Ratio H/L\" will be renamed to \"log profile\"\n",
    "            df_log_stacked.columns = [col if col != \"LFQ intensity\" else \"log profile\" for col in df_log_stacked.columns]\n",
    "\n",
    "            return df_log_stacked\n",
    "\n",
    "\n",
    "        if self.acquisition == \"SILAC\":\n",
    "            df_index = indexingdf(self.df_original, self.acquisition_set_dict, self.acquisition, self.fraction_dict, self.name_pattern, self.shape_dict)\n",
    "            df_stringency_mapfracstacked = stringency_silac(df_index)\n",
    "            df_01_stacked = normalization_01_silac(df_stringency_mapfracstacked)\n",
    "            df_log_stacked = logarithmization_silac(df_stringency_mapfracstacked)\n",
    "            self.df_log_stacked = df_log_stacked\n",
    "            self.df_01_stacked = df_01_stacked\n",
    "            fractions = df_01_stacked.index.get_level_values(\"Fraction\").unique()\n",
    "            self.fractions = fractions\n",
    "            map_names = self.df_01_stacked.index.get_level_values(\"Map\").unique()\n",
    "            self.map_names = map_names\n",
    "            \n",
    "            self.analysis_summary_dict[\"changes in Shape after filtering\"] = self.shape_dict.copy() \n",
    "            self.analysis_parameters = {\"acquisition\" : self.acquisition, \n",
    "                                        \"filename\" : self.filename,\n",
    "                                        \"Ratio H/L count 1 (>= X)\" : self.RatioHLcount_1,\n",
    "                                        \"Ratio H/L count 2 (>=Y, var<Z)\" : self.RatioHLcount_2,\n",
    "                                        \"Ratio variability (<Z, count>=Y)\" : self.RatioVariability}\n",
    "            \n",
    "            self.analysis_summary_dict[\"Analysis parameters\"] = self.analysis_parameters.copy() \n",
    "            self.analysed_datasets_dict[self.expname] = self.analysis_summary_dict.copy() \n",
    "\n",
    "            self.analysis_summary_dict.clear()\n",
    "            self.shape_dict.clear()\n",
    "            self.analysis_parameters.clear() \n",
    "            return self.analysed_datasets_dict\n",
    "\n",
    "\n",
    "        elif self.acquisition == \"LFQ\":\n",
    "            df_index = indexingdf(self.df_original, self.acquisition_set_dict, self.acquisition, self.fraction_dict, self.name_pattern, self.shape_dict)\n",
    "            df_stringency_mapfracstacked = stringency_lfq(df_index)\n",
    "            df_01_stacked = normalization_01_lfq(df_stringency_mapfracstacked)\n",
    "            df_log_stacked = logarithmization_lfq(df_stringency_mapfracstacked)\n",
    "            self.df_log_stacked = df_log_stacked\n",
    "            self.df_01_stacked = df_01_stacked\n",
    "            fractions = df_01_stacked.index.get_level_values(\"Fraction\").unique()\n",
    "            self.fractions = fractions\n",
    "            map_names = self.df_01_stacked.index.get_level_values(\"Map\").unique()\n",
    "            self.map_names = map_names\n",
    "            \n",
    "            self.analysis_summary_dict[\"changes in Shape after filtering\"] = self.shape_dict.copy() \n",
    "            self.analysis_parameters = {\"acquisition\" : self.acquisition, \n",
    "                            \"filename\" : self.filename,\n",
    "                            \"consecutive data points\" : self.consecutive_LFQ_I,\n",
    "                            \"summed MS/MS counts\" : self.summed_MSMS_counts}\n",
    "            self.analysis_summary_dict[\"Analysis parameters\"] = self.analysis_parameters.copy() \n",
    "            self.analysed_datasets_dict[self.expname] = self.analysis_summary_dict.copy() \n",
    "            \n",
    "            self.analysis_summary_dict.clear()\n",
    "            self.shape_dict.clear()\n",
    "            self.analysis_parameters.clear() \n",
    "            return self.analysed_datasets_dict\n",
    "\n",
    "\n",
    "        else:\n",
    "            return \"I don't know this\"    \n",
    "        \n",
    "#list_drop_col = [column for column in df_index.columns if not column.startswith(\"Ratio \")]\n",
    "#list_endcount = [column for column in df_index.columns if column.endswith(\"count\")]\n",
    "#list_endpct = [column for column in df_index.columns if column.endswith(\"[%]\")]\n",
    "#list_drop_col.extend(list_endcount)\n",
    "#list_drop_col.extend(list_endpct)\n",
    "#df_index = df_index.drop(list_drop_col, axis=1)\n",
    "#df_index[[col for col in df_index.columns if re.match(regex[\"lfq_nan\"], col)]] = \\\n",
    "#df_index[[col for col in df_index.columns if re.match(regex[\"lfq_nan\"], col)]].replace(0, np.nan)\n",
    "    def perform_pca(self):\n",
    "        \"\"\"PCA will be performed, using logarithmized data.\n",
    "\n",
    "        Args:\n",
    "            self.df_log_stacked: dataframe, in which \"MAP\" and \"Fraction\" are stacked; data in the column \"log profile\"\n",
    "                originates from logarithmized  \"LFQ intensity\" and \"Ratio H/L\", respectively; additionally the columns\n",
    "                \"MS/MS count\" and \"Ratio H/L count|Ratio H/L variability [%]\" are stored as single level indices\n",
    "\n",
    "        Returns:\n",
    "                df_pca_all_marker_cluster_maps: PCA processed dataframe, containing the columns \"PC1\", \"PC2\", \"PC3\",\n",
    "                filtered for marker genes, that are consistent throughout all maps / coverage filtering.\n",
    "        \"\"\"\n",
    "\n",
    "        map_names = self.map_names\n",
    "        markerproteins = self.markerproteins\n",
    "\n",
    "        # isolate only logarithmized profile, and unstack \"Fraction\"\n",
    "        df_log_stacked = self.df_log_stacked\n",
    "        df_log_fracunstacked = df_log_stacked[\"log profile\"].unstack(\"Fraction\")\n",
    "\n",
    "        pca = PCA(n_components=3)\n",
    "\n",
    "        # df_pca: PCA processed dataframe, containing the columns \"PC1\", \"PC2\", \"PC3\"\n",
    "        df_pca = pd.DataFrame(pca.fit_transform(df_log_fracunstacked))\n",
    "        df_pca.columns = [\"PC1\", \"PC2\", \"PC3\"]\n",
    "        df_pca.index = df_log_fracunstacked.index\n",
    "\n",
    "        df_pca_all_marker_cluster_maps_unfiltered = pd.DataFrame()\n",
    "\n",
    "        for maps in map_names:\n",
    "            for clusters in markerproteins:\n",
    "                for marker in markerproteins[clusters]:\n",
    "                    if marker not in df_pca.index.get_level_values(\"Gene names\"):\n",
    "                        continue\n",
    "                    plot_try_pca = df_pca.xs((marker, maps), level=[\"Gene names\", \"Map\"], drop_level=False)\n",
    "                    df_pca_all_marker_cluster_maps_unfiltered = df_pca_all_marker_cluster_maps_unfiltered.append(\n",
    "                        plot_try_pca)\n",
    "\n",
    "        # genes are droped, if they are not present in all maps\n",
    "        df_pca_all_marker_cluster_maps = df_pca_all_marker_cluster_maps_unfiltered.groupby([\"Gene names\"]).filter(\n",
    "            lambda x: len(x) >= len(map_names))\n",
    "        \n",
    "        self.df_pca_all_marker_cluster_maps = df_pca_all_marker_cluster_maps\n",
    "        self.analysis_summary_dict[\"PCA analyzed data\"] = self.df_pca_all_marker_cluster_maps.copy() \n",
    "\n",
    "\n",
    "    def plot_pca(self):\n",
    "        \"\"\"\n",
    "        PCA plot will be generated\n",
    "\n",
    "        Args:\n",
    "            df_pca_all_marker_cluster_maps: PCA processed dataframe, containing the columns \"PC1\", \"PC2\", \"PC3\",\n",
    "                filtered for marker genes, that are consistent throughout all maps / coverage filtering.\n",
    "\n",
    "        Returns:\n",
    "            pca_figure: PCA plot, for one protein cluster all maps are plotted\n",
    "        \"\"\"\n",
    "\n",
    "        df_pca_all_marker_cluster_maps = self.df_pca_all_marker_cluster_maps\n",
    "        map_names = self.map_names\n",
    "        markerproteins = self.markerproteins\n",
    "\n",
    "        for maps in map_names:\n",
    "            df_setofproteins = pd.DataFrame()\n",
    "            for marker in markerproteins[self.cluster_of_interest]:\n",
    "                if marker not in df_pca_all_marker_cluster_maps.index.get_level_values(\"Gene names\"):\n",
    "                    continue\n",
    "                plot_try_pca = df_pca_all_marker_cluster_maps.xs((marker, maps), level=[\"Gene names\", \"Map\"],\n",
    "                                                                 drop_level=False)\n",
    "                df_setofproteins = df_setofproteins.append(plot_try_pca)\n",
    "\n",
    "            df_setofproteins.reset_index(inplace=True)\n",
    "            if maps == map_names[0]:\n",
    "                pca_figure = go.Figure(\n",
    "                    data=[go.Scatter3d(x=df_setofproteins.PC1, y=df_setofproteins.PC2, z=df_setofproteins.PC3,\n",
    "                                       hovertext=df_setofproteins[\"Gene names\"], mode=\"markers\", name=maps\n",
    "                                       # marker=dict(color=[f'rgb({np.random.randint(0,256)}, {np.random.randint(0,256)}, {np.random.randint(0,256)})' for _   in range(25)])\n",
    "                                       )])\n",
    "            else:\n",
    "                pca_figure.add_trace(go.Scatter3d(x=df_setofproteins.PC1, y=df_setofproteins.PC2, z=df_setofproteins.PC3,\n",
    "                                               hovertext=df_setofproteins[\"Gene names\"], mode=\"markers\", name=maps\n",
    "                                               ))\n",
    "\n",
    "        pca_figure.update_layout(autosize=False, width=500, height=500,\n",
    "                              title=\"PCA plot for <br>the protein cluster: {}\".format(self.cluster_of_interest))\n",
    "\n",
    "        return pca_figure\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_mapwidget = pn.widgets.Select(options=[i_class.map_of_interest], name=\"Map of interest\", width=300)\n",
    "i_expname = pn.widgets.TextInput(name='Experiment Name', placeholder='Enter a string here...')\n",
    "i_acquisition = pn.widgets.Select(name='Acquisition', options=[\"LFQ\",\"SILAC\"])\n",
    "i_file = pn.widgets.Select(name='File',options=[\"6_deep_maps.txt\",\"LFQ_proteinGroups.txt\",\"proteinGroupsCOVID.txt\", \n",
    "                                              \"proteinGroups_LFQ_Deep_3_Maps.txt\"])\n",
    "i_name_pattern = pn.widgets.Select(name='Name pattern',options=[\".* (?P<rep>.*_.*)_(?P<frac>.*)\",\".* (?P<rep>.*)_(?P<frac>.*)\",\n",
    "                                                                \".* (?P<cond>.*)_(?P<frac>.*)_(?P<rep>.*)\", \"Custom\"])\n",
    "i_custom_namepattern = pn.widgets.TextInput(name='Customized Name Pattern', placeholder='Enter a string here...e.g. \".* (?P<rep>.*)_(?P<frac>.*)\"')\n",
    "regex_pattern = {\n",
    "    \".* (?P<rep>.*)_(?P<frac>.*)\" : [\"MAP1_03K\",\"MAP3_03K\"],\n",
    "    \".* (?P<rep>.*_.*)_(?P<frac>.*)\" : [\"EGF_rep1_06K\",\"EGF_rep3_06K\"],\n",
    "    \".* (?P<cond>.*)_(?P<frac>.*)_(?P<rep>.*)\" : [\"Control_Mem_1\", \"Control_Cyt_1\"]\n",
    "    }\n",
    "pattern_examples = pn.widgets.Select(name = \"Examples\", options=regex_pattern[i_name_pattern.value])\n",
    "\n",
    "\n",
    "@pn.depends(i_name_pattern.param.value, i_custom_namepattern)\n",
    "def custimization(name_pattern, custom_namepattern):\n",
    "    if name_pattern == \"Custom\":\n",
    "        return i_custom_namepattern\n",
    "    else:\n",
    "        example_for_name_pattern = regex_pattern[name_pattern]\n",
    "        pattern_examples.options = example_for_name_pattern\n",
    "        return pattern_examples\n",
    "\n",
    "def update_object_selector(i_mapwidget):\n",
    "    i_mapwidget.options = list(i_class.map_names)\n",
    "    if i_class.map_of_interest not in list(i_class.map_names):\n",
    "            i_class.map_of_interest = i_class.map_names[0]\n",
    "    \n",
    "    \n",
    "pn.Column(pn.Row(i_acquisition, i_file, i_expname), pn.Row(i_name_pattern, custimization, i_mapwidget))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_check= [i_file.value, i_acquisition.value, i_name_pattern.value, i_expname.value]\n",
    "double_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class = SpatialDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class.filename= i_file.value\n",
    "i_class.acquisition = i_acquisition.value\n",
    "if not i_name_pattern.value == \"Custom\":\n",
    "    i_class.name_pattern = i_name_pattern.value\n",
    "else:\n",
    "    i_class.name_pattern = i_custom_namepattern.value\n",
    "i_class.expname = i_expname.value\n",
    "i_class.data_reading()\n",
    "i_class.processingdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_object_selector(i_mapwidget)\n",
    "df_log_fracunstackedWO=i_class.df_log_stacked[\"log profile\"].unstack(\"Fraction\")\n",
    "df_log_fracunstackedWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class.df_log_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_fracunstacked=i_class.df_log_stacked[\"log profile\"].unstack(\"Fraction\")\n",
    "df_log_fracunstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i_class.perform_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i_class.plot_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#analysed_datastes\n",
    "#with open(\"AnalysedDatasets.json\", \"w\") as write_file:\n",
    "#    json.dump(developer, write_file, indent=4)\n",
    "    \n",
    "with open(\"AnalysedDatasets.json\", \"w\") as write_file:\n",
    "    json.dump(analysed_datastes, write_file, indent=4)\n",
    "    \n",
    "with open('AnalysedDatasets.json', 'r') as write_file:\n",
    "    data = json.load(write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictio_all[i_expname.value]=dictio\n",
    "\n",
    "analysed_dataset = {\n",
    "    FILENAME_FOR_COMPARISON: {\n",
    "        data: processed dataframe reduced to available complexes,\n",
    "        analysis_summary: {\n",
    "            pgs_before: n,\n",
    "            pgs_categorical: n,\n",
    "            ...\n",
    "            },\n",
    "        analysis_parameters: {\n",
    "            acquisition:\n",
    "            filename:\n",
    "            msms_counts:\n",
    "            ...\n",
    "            },\n",
    "        spread_table: output_table\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_index = i_class.df_original.set_index([col for col in i_class.df_original.columns if any([re.match(s, col) for s in sets_LFQ]) == False])\n",
    "\n",
    "#df_index\n",
    "#pn.Row(i_file, read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
