{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import traceback\n",
    "import panel as pn\n",
    "pn.extension()#\"plotly\")\n",
    "import io\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "from bokeh.models.widgets.tables import NumberFormatter\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import ast\n",
    "import statistics \n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import matplotlib_venn as venn\n",
    "from matplotlib_venn import venn2, venn3, venn3_circles\n",
    "from PIL import Image\n",
    "\n",
    "from upsetplot import from_memberships\n",
    "from upsetplot import plot\n",
    "#pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"class.py\").read())\n",
    "\n",
    "i_file = pn.widgets.FileInput(name=\"Upload file\")\n",
    "\n",
    "loading_status = pn.Row()\n",
    "idle = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "\n",
    "button_analysis = pn.widgets.Button(name=\"Analyse clusters\", width=50)\n",
    "button_SVM_analysis = pn.widgets.Button(name=\"Analyse misclassification matrix\", width=50)\n",
    "\n",
    "i_logOR01_selection = pn.widgets.Select(options=[\"0/1 normalized data\", \"log transformed data\"], name=\"Select type of data for download\", width=300)\n",
    "\n",
    "i_acquisition = pn.widgets.Select(options=[\"LFQ6 - Spectronaut\", \"LFQ5 - Spectronaut\", \"LFQ5 - MQ\", \"LFQ6 - MQ\", \"SILAC - MQ\"], name=\"Acquisition\", width=300)\n",
    "i_organism = pn.widgets.Select(options=list(SpatialDataSet.markerproteins_set.keys()), name=\"Organism\", width=300) #\n",
    "\n",
    "i_SVM_table = pn.widgets.input.TextAreaInput(name=\"Misclassification matrix from Perseus\", placeholder=\"Copy matrix here...\")\n",
    "\n",
    "i_comment = pn.widgets.input.TextAreaInput(name=\"Additional Comments\", placeholder=\"Write any kind of information assoiciated with this dataset here...\")\n",
    "\n",
    "i_clusterwidget = pn.widgets.Select(options=[\"Proteasome\", \"Lysosome\"], name=\"Cluster of interest\", width=300)\n",
    "i_mapwidget = pn.widgets.Select(options=[\"Map1\", \"Map2\"], name=\"Map of interest\", width=300)\n",
    "\n",
    "i_collapse_maps_PCA = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "\n",
    "cache_uploaded = pn.widgets.Checkbox(value=False)\n",
    "cache_uploaded_SVM = pn.widgets.Checkbox(value=False)\n",
    "\n",
    "cache_run = pn.widgets.Checkbox(value=False)\n",
    "\n",
    "analysis_status = pn.Pane(\"No analysis run yet\", width=1000)\n",
    "analysis_status_SVM = pn.Pane(\"No SVM analysis run yet\", width=1000)\n",
    "filereading_status = pn.Pane(\"No data import yet\", width=1000)\n",
    "\n",
    "i_expname = pn.widgets.TextInput(name=\"Experiment Name\", placeholder=\"Enter your experiment name here here...\")\n",
    "\n",
    "i_consecutiveLFQi = pn.widgets.IntSlider(name=\"Consecutive LFQ intensities\", start=1, end=10, step=1, value=4)\n",
    "i_summed_MSMS_counts = pn.widgets.IntSlider(name=\"Summed MS/MS counts\", start=1, end=10, step=1, value=2)\n",
    "\n",
    "i_RatioHLcount = pn.widgets.IntSlider(name=\"Ratio H/L Count\", start=1, end=10, step=1, value=2)\n",
    "i_RatioVariability = pn.widgets.IntSlider(name=\"Ratio H/L variability [%]\", start=0, end=100, step=5, value=30)\n",
    "\n",
    "i_name_pattern = pn.widgets.Select(name=\"Name pattern\",options=[\"(?P<rep>.*)_(?P<frac>.*)\", \".* (?P<cond>.*)_(?P<rep>.*)_(?P<frac>.*)\",\".* (?P<rep>.*)_(?P<frac>.*)\",\n",
    "                                                                \".* (?P<cond>.*)_(?P<frac>.*)_(?P<rep>.*)\", \"Custom\"])\n",
    "i_custom_namepattern = pn.widgets.TextInput(name=\"Customized Name Pattern\", placeholder=\"Enter a string here...e.g.: .* (?P<rep>.*)_(?P<frac>.*)\")\n",
    "regex_pattern = {\n",
    "    \"(?P<rep>.*)_(?P<frac>.*)\" : [\"Spectronaut MAP1_03K\"],\n",
    "    \".* (?P<rep>.*)_(?P<frac>.*)\" : [\"MAP1_03K\",\"MAP3_03K\"],\n",
    "    \".* (?P<cond>.*)_(?P<rep>.*)_(?P<frac>.*)\" : [\"EGF_rep1_06K\",\"EGF_rep3_06K\"],\n",
    "    \".* (?P<cond>.*)_(?P<frac>.*)_(?P<rep>.*)\" : [\"Control_Mem_1\", \"Control_Cyt_1\"]\n",
    "    }\n",
    "i_pattern_examples = pn.widgets.Select(name = \"Examples\", options=regex_pattern[i_name_pattern.value])\n",
    "\n",
    "@pn.depends(i_name_pattern.param.value, i_custom_namepattern, i_pattern_examples)\n",
    "def custimization(name_pattern, custom_namepattern, pattern_examples):\n",
    "    if name_pattern == \"Custom\":\n",
    "        return i_custom_namepattern\n",
    "    else:\n",
    "        example_for_name_pattern = regex_pattern[name_pattern]\n",
    "        i_pattern_examples.options = example_for_name_pattern\n",
    "        return i_pattern_examples\n",
    "\n",
    "i_x_vs_yAxis_PCA = {\n",
    "    \"PC1\" : [\"PC3\", \"PC2\"],\n",
    "    \"PC2\" : [\"PC1\", \"PC3\"],\n",
    "    \"PC3\" : [\"PC1\", \"PC2\"],\n",
    "    }\n",
    "\n",
    "i_xAxis_PCA = pn.widgets.Select(name=\"X-Axis\", options=[\"PC1\", \"PC2\",\"PC3\"])\n",
    "i_yAxis_PCA = pn.widgets.Select(name=\"Y-Axis\", options=i_x_vs_yAxis_PCA[i_xAxis_PCA.value])\n",
    "\n",
    "i_xAxis_PCA_comp = pn.widgets.Select(name=\"X-Axis\", options=[\"PC1\", \"PC2\",\"PC3\"])\n",
    "i_yAxis_PCA_comp = pn.widgets.Select(name=\"Y-Axis\", options=i_x_vs_yAxis_PCA[i_xAxis_PCA_comp.value])\n",
    "\n",
    "@pn.depends(i_xAxis_PCA_comp.param.value, watch=True)\n",
    "def custimization_PCA_comp(xAxis_PCA_comp):\n",
    "    yAxis_PCA_comp = i_x_vs_yAxis_PCA[xAxis_PCA_comp]\n",
    "    i_yAxis_PCA_comp.options = yAxis_PCA_comp\n",
    "    #return i_yAxis_PCA_comp\n",
    "\n",
    "@pn.depends(i_xAxis_PCA.param.value, watch=True)\n",
    "def custimization_PCA(xAxis_PCA):\n",
    "    yAxis_PCA = i_x_vs_yAxis_PCA[xAxis_PCA]\n",
    "    i_yAxis_PCA.options = yAxis_PCA\n",
    "    #return i_yAxis_PCA\n",
    "\n",
    "#define widgets, that will be displayed\n",
    "@pn.depends(i_acquisition.param.value, i_consecutiveLFQi, i_summed_MSMS_counts)\n",
    "def acquisition_response(acquisition, consecutiveLFQi, summed_MSMS_counts):\n",
    "    if acquisition == \"SILAC\":\n",
    "        return pn.Column(pn.Row(i_RatioHLcount, i_RatioVariability), pn.Row(i_name_pattern, custimization))\n",
    "    else:\n",
    "        return pn.Column(pn.Row(i_consecutiveLFQi,i_summed_MSMS_counts), pn.Row(i_name_pattern, custimization))\n",
    "\n",
    "#define widgets that should be disbled after run==True\n",
    "wdgts = [i_acquisition,i_name_pattern,i_expname, i_pattern_examples, button_analysis, i_expname, i_organism, i_consecutiveLFQi, i_summed_MSMS_counts, \n",
    "         i_RatioHLcount, i_RatioVariability, i_comment]            \n",
    "\n",
    "@pn.depends(i_file.param.value)\n",
    "def read_file(file):\n",
    "    if file is None:\n",
    "        filereading_status = \"No file is uploaded\"\n",
    "        cache_uploaded.value = False\n",
    "        return filereading_status\n",
    "    else:\n",
    "        cache_uploaded.value = False\n",
    "        try:\n",
    "            df_original = pd.read_csv(BytesIO(file), sep=\"\\t\", comment=\"#\", nrows=5, usecols=lambda x: bool(re.match(SpatialDataSet.regex[\"imported_columns\"], x)), low_memory=True)\n",
    "            cache_uploaded.value = True\n",
    "            for wdgt in wdgts:\n",
    "                wdgt.disabled = False\n",
    "\n",
    "            return pn.Column(pn.Row(pn.widgets.DataFrame(df_original, height=200, width=600, disabled=True)),#i_class.\n",
    "                             pn.Row(i_expname), \n",
    "                             pn.Row(i_organism, i_acquisition), \n",
    "                             pn.Row(acquisition_response), \n",
    "                             pn.Row(i_comment),\n",
    "                             pn.Row(button_analysis))\n",
    "\n",
    "        except Exception: \n",
    "            filereading_status = traceback.format_exc()\n",
    "            cache_uploaded.value = False\n",
    "            return filereading_status   \n",
    "        \n",
    "        \n",
    "def execution(event):\n",
    "    #prevent execution, if no data is uploaded yet\n",
    "    if cache_uploaded.value == False:\n",
    "        analysis_status.object = \"Please upload a file first\"\n",
    "    elif i_expname.value == \"\":\n",
    "        analysis_status.object = \"Please enter an experiment name first\"\n",
    "    else:        \n",
    "        \n",
    "        dashboard_analysis.objects = []\n",
    "        dashboard_svm.objects = []\n",
    "        \n",
    "        cache_run.value = False\n",
    "        for wdgt in wdgts:\n",
    "            wdgt.disabled = True\n",
    "        #if you did already your comparison, but add another experiment afterwards - without reloading your AnylsedDatasets.json\n",
    "        #for wdgt in wdgts_comparison:\n",
    "        #    wdgt.disabled = True\n",
    "        try:\n",
    "            \n",
    "            dashboard_analysis.append(i_clusterwidget)\n",
    "            dashboard_analysis.append(i_mapwidget)\n",
    "            dashboard_analysis.append(analysis_tabs)\n",
    "            \n",
    "            i_SVM_table.value = \"\"\n",
    "            \n",
    "            analysis_status_SVM.object = \"Please paste a SVM Matrix first\"\n",
    "            \n",
    "            dashboard_svm.append(i_SVM_table)\n",
    "            dashboard_svm.append(read_SVM_matrix)\n",
    "            dashboard_svm.append(analysis_status_SVM)\n",
    "            #analysis_status.object = loading#, pn.Row(pn.Pane(\"No analysis run yet\")))\n",
    "            loading_status.objects = [loading]\n",
    "            analysis_status.object = \"Analysis in progress\"\n",
    "            \n",
    "            #define name pattern\n",
    "            if i_name_pattern.value == \"Custom\":\n",
    "                namePattern = i_custom_namepattern.value\n",
    "            else:\n",
    "                namePattern = i_name_pattern.value\n",
    "            \n",
    "            global i_class\n",
    "            if i_acquisition.value == \"SILAC\":\n",
    "                i_class = SpatialDataSet(i_file.filename, i_expname.value, i_acquisition.value, comment=i_comment.value, name_pattern=namePattern, organism=i_organism.value, \n",
    "                                         RatioHLcount=i_RatioHLcount.value, RatioVariability=i_RatioVariability.value)\n",
    "            else:\n",
    "                i_class = SpatialDataSet(i_file.filename, i_expname.value, i_acquisition.value, comment=i_comment.value, name_pattern=namePattern, organism=i_organism.value,\n",
    "                                         consecutiveLFQi=i_consecutiveLFQi.value, summed_MSMS_counts=i_summed_MSMS_counts.value)\n",
    "            \n",
    "            analysis_status.object = \"Data Reading\"\n",
    "            i_class.data_reading(content=BytesIO(i_file.value))\n",
    "            analysis_status.object = \"Data Processing\"\n",
    "            i_class.processingdf()\n",
    "            update_object_selector(i_mapwidget, i_clusterwidget)\n",
    "            i_class.quantity_profiles_proteinGroups()\n",
    "            analysis_status.object = \"PCA\"\n",
    "            i_class.perform_pca()\n",
    "            i_class.multiple_iterations()\n",
    "            analysis_status.object = \"Calculating Manhattan Distance\"\n",
    "            i_class.distance_calculation()\n",
    "            analysis_status.object = \"Calculating Dynamic Range\"\n",
    "            i_class.dynamic_range()\n",
    "            analysis_status.object = \"Writing Overview Table\"\n",
    "            i_class.results_overview_table()\n",
    "            analysis_status.object = \"Writing Analysed Dataset Dictionary\"\n",
    "            SpatialDataSetComparison.analysed_datasets_dict.update(i_class.analysed_datasets_dict)\n",
    "            loading_status.objects = [idle]\n",
    "            analysis_status.object = \"Analysis finished! Please open the 'Analysis' tab!\"\n",
    "            cache_run.value = True\n",
    "            #exc_info = sys.exc_info()\n",
    "        except:\n",
    "            for wdgt in wdgts:\n",
    "                wdgt.disabled = False\n",
    "            loading_status.objects = [\"\"]\n",
    "            #The traceback gives no traceback, so out of that there will be still the output: Analysis in progress, although it is not possible. Out of that i removed the traceback\n",
    "            analysis_status.object = traceback.format_exc()\n",
    "            cache_run.value = False\n",
    "\n",
    "button_analysis.on_click(execution)  \n",
    "\n",
    "def update_object_selector(i_mapwidget, i_clusterwidget):\n",
    "    i_mapwidget.options = list(i_class.map_names)\n",
    "    i_clusterwidget.options = list(i_class.markerproteins.keys())\n",
    "#    if i_class.map_of_interest not in list(i_class.map_names):\n",
    "#            i_class.map_of_interest = i_class.map_names[0]        \n",
    "        \n",
    "        \n",
    "@pn.depends(i_SVM_table.param.value)\n",
    "def read_SVM_matrix(SVM_table):   \n",
    "    if SVM_table == \"\":\n",
    "        SVM_reading_status = \"No misclassification matrix is uploaded\"\n",
    "        cache_uploaded_SVM.value = False\n",
    "    else:\n",
    "        cache_uploaded_SVM.value = False\n",
    "        try:\n",
    "            i_class.df_SVM = pd.read_table(StringIO(SVM_table), sep=\"\\t\")\n",
    "            SVM_reading_status = i_class.df_SVM\n",
    "            cache_uploaded_SVM.value = True\n",
    "            return pn.Column(pn.Pane(SVM_reading_status, width=60*SVM_reading_status.shape[1]),\n",
    "                             pn.Row(button_SVM_analysis))\n",
    "        except Exception: \n",
    "            SVM_reading_status = traceback.format_exc()\n",
    "            cache_uploaded_SVM.value = False\n",
    "            return SVM_reading_status \n",
    "\n",
    "        \n",
    "def execution_SVM(event):\n",
    "    #prevent execution, if no data is uploaded yet\n",
    "    if cache_uploaded_SVM.value == False:\n",
    "        analysis_status_SVM.object = \"Please paste a SVM Matrix first\"\n",
    "    else:        \n",
    "        #cache_run.value = False\n",
    "        try:\n",
    "           # cache_run.value = True\n",
    "            #exc_info = sys.exc_info()\n",
    "            i_class.svm_processing()\n",
    "            analysis_status_SVM.object = \"Analysis finished! Check the dictionary!\"\n",
    "            #cache_run.value = True\n",
    "        except Exception:\n",
    "            analysis_status_SVM.object = traceback.format_exc()\n",
    "            #cache_run.value = False\n",
    "            \n",
    "button_SVM_analysis.on_click(execution_SVM)                  \n",
    "  \n",
    "@pn.depends(i_mapwidget.param.value, cache_run.param.value, i_collapse_maps_PCA.param.value, i_clusterwidget.param.value, i_xAxis_PCA.param.value, i_yAxis_PCA.param.value)\n",
    "def update_data_overview(mapwidget, run, collapse_maps_PCA, clusterwidget, xAxis_PCA, yAxis_PCA):\n",
    "    try:\n",
    "        if run == True:\n",
    "            pca_plot = i_class.global_pca_plot(map_of_interest=mapwidget , cluster_of_interest=clusterwidget, x_PCA=xAxis_PCA, y_PCA=yAxis_PCA, collapse_maps=collapse_maps_PCA)\n",
    "            log_histogram = i_class.plot_log_data()\n",
    "            visualization_map = pn.Column(\n",
    "                    pn.Row(i_collapse_maps_PCA),\n",
    "                    pn.Row(pn.Pane(pca_plot, width=1000)),\n",
    "                    pn.Row(i_xAxis_PCA, i_yAxis_PCA),\n",
    "                    pn.Row(pn.Pane(log_histogram, width=1000))\n",
    "                    )\n",
    "            app_tabs.active = 1\n",
    "            return visualization_map\n",
    "        else:\n",
    "            visualization_map = \"Run analysis first!\"\n",
    "            return visualization_map\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "            \n",
    "\n",
    "@pn.depends(i_clusterwidget.param.value,i_mapwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_overview(clusterwidget, mapwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            list_genes = [goi for goi in i_class.genenames_sortedout_list if goi in i_class.markerproteins[clusterwidget]]\n",
    "            i_class.cache_cluster_quantified = True\n",
    "            distance_boxplot = i_class.distance_boxplot(cluster_of_interest=clusterwidget)\n",
    "            if i_class.cache_cluster_quantified == False:\n",
    "                return \"This protein cluster was not quantified\"\n",
    "            \n",
    "            else:\n",
    "                df_quantification_overview = i_class.quantification_overview(cluster_of_interest=clusterwidget)\n",
    "                profiles_plot = i_class.profiles_plot(map_of_interest = mapwidget, cluster_of_interest=clusterwidget)\n",
    "                pca_plot = i_class.pca_plot(cluster_of_interest=clusterwidget)\n",
    "                \n",
    "                cluster_overview = pn.Column(\n",
    "                        pn.Row(pn.Pane(pca_plot, width=500),\n",
    "                               pn.Pane(distance_boxplot, width=500),\n",
    "                               pn.Pane(profiles_plot, width=500)),\n",
    "                        pn.Row(pn.Pane(\"In total {} proteins across all maps were quantified, whereas the following proteins were not consistently quantified throughout all maps: {}\".format(\n",
    "                                i_class.proteins_qunatified_across_all_maps, \", \".join(list_genes)) if len(list_genes) != 0 else\n",
    "                            \"All genes from this cluster are quantified in all maps.\"), width=1000),\n",
    "                        pn.Row(pn.widgets.DataFrame(df_quantification_overview, height=200, width=500, disabled=True))  \n",
    "                        )\n",
    "                return cluster_overview\n",
    "        \n",
    "        else:\n",
    "            cluster_overview = \"Run analysis first!\"\n",
    "            return cluster_overview\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(i_clusterwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_details(clusterwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            cluster_details = i_class.distance_to_median_boxplot(cluster_of_interest = clusterwidget)\n",
    "            return cluster_details\n",
    "        else:\n",
    "            cluster_details = \"Run analysis first!\"\n",
    "            return cluster_details\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_quantity(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            fig_npg, fig_npr, fig_npr_dc, fig_npg_F, fig_npgf_F, fig_npg_F_dc = i_class.plot_quantity_profiles_proteinGroups()\n",
    "            return pn.Column(\n",
    "                    pn.Row(pn.Column(fig_npg), pn.Column(fig_npr), pn.Column(fig_npr_dc)) ,\n",
    "                pn.Row(pn.Column(fig_npg_F), pn.Column(fig_npgf_F), pn.Column(fig_npg_F_dc)))\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_dynamic_range(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            fig_dynamic_range = i_class.plot_dynamic_range()\n",
    "            return pn.Row(fig_dynamic_range)\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "            \n",
    "    \n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def show_tabular_overview(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            content = pn.Column(\n",
    "                pn.widgets.DataFrame(pd.read_json(i_class.analysed_datasets_dict[i_expname.value][\"Overview table\"]), height=200, width=600, disabled=True),\n",
    "                #pn.panel(i_class.results_overview_table(),width=500),\n",
    "                pn.widgets.FileDownload(\n",
    "                    callback=table_download, filename=\"cluster_distances.csv\"),\n",
    "                i_logOR01_selection,\n",
    "                df01_download_widget,\n",
    "                json_download,\n",
    "            )\n",
    "            return content\n",
    "        else:\n",
    "            content = \"Please, upload a file first and press ‘Analyse clusters’\"\n",
    "            return content\n",
    "    except Exception:\n",
    "        content = traceback.format_exc()\n",
    "        return content\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def table_download(run):\n",
    "    df = i_class.results_overview_table()\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "@pn.depends(cache_run.param.value, i_logOR01_selection.param.value)\n",
    "def df01_download_widget(run, logOR01_selection):\n",
    "    if logOR01_selection == \"0/1 normalized data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df01_download, filename = \"01_normalized_data.csv\")) \n",
    "    else:\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=dflog_download, filename = \"log_transformed_data.csv\"))\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def df01_download(run):\n",
    "    df_01 = i_class.reframe_df_01ORlog_for_svm(i_class.df_01_stacked)\n",
    "    sio = StringIO()\n",
    "    df_01.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "    \n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def dflog_download(run):\n",
    "    df_log = i_class.reframe_df_01ORlog_for_svm(i_class.df_log_stacked)\n",
    "    sio = StringIO()\n",
    "    df_log.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "    \n",
    "json_download = pn.widgets.Button(name=\"Download AnalysedDatasets.json\", width=50)\n",
    "\n",
    "def analysedDatasets_download(event):\n",
    "    #with open(\"AnalysedDatasets.json\", \"w\") as write_file:\n",
    "        json.dump(\n",
    "            SpatialDataSetComparison.analysed_datasets_dict, \n",
    "            sio\n",
    "            #write_file, \n",
    "            indent=4, \n",
    "            sort_keys=True\n",
    "        )\n",
    "\n",
    "json_download.on_click(analysedDatasets_download)\n",
    "\n",
    "dashboard_svm = pn.Column(\"Please, upload a file first and press 'Analyse clusters'\", name=\"SVM Analysis\", css_classes=[\"content-width\"])\n",
    "\n",
    "dasboard_home = pn.Column(i_file, read_file,analysis_status, loading_status, name=\"Home\", css_classes=[\"content-width\"])\n",
    "dashboard_analysis = pn.Column(\"Please, upload a file first and press 'Analyse clusters'\", name=\"Analysis\", css_classes=[\"content-width\"])\n",
    "\n",
    "analysis_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "analysis_tabs.append((\"Data overview\", update_data_overview))\n",
    "analysis_tabs.append((\"Cluster Overview\", update_cluster_overview))\n",
    "analysis_tabs.append((\"Cluster Details\", update_cluster_details))\n",
    "analysis_tabs.append((\"Protein Groups/Profiles Quantity\", update_quantity))\n",
    "analysis_tabs.append((\"Dynamic Range\", update_dynamic_range))\n",
    "\n",
    "cache_uploaded_json = pn.widgets.Checkbox(value=False)\n",
    "cache_run_json = pn.widgets.Checkbox(value=False)\n",
    "\n",
    "button_comparison = pn.widgets.Button(name=\"Compare experiments\", width=50)\n",
    "i_jsonFile = pn.widgets.FileInput(name=\"Upload JSON file for comparison\")\n",
    "\n",
    "i_organism_comparison = pn.widgets.Select(options=list(SpatialDataSet.markerproteins_set.keys()), name=\"Organism\", width=300) #\n",
    "#i_clusterwidget_comparison = pn.widgets.Select(options=list(SpatialDataSet.markerproteins.keys()), name=\"Cluster of interest\", width=300)\n",
    "i_clusterwidget_comparison = pn.widgets.Select(options=[\"Proteasome\", \"Lysosome\"], name=\"Cluster of interest\", width=300)\n",
    "\n",
    "\n",
    "i_clusters_for_ranking = pn.widgets.CrossSelector(name=\"Select clusters to be considered for ranking calculation\", #value=list(SpatialDataSet.markerproteins.keys()), \n",
    "                                                options=[\"Proteasome\", \"Lysosome\"], size=8)\n",
    "\n",
    "i_multi_choice = pn.widgets.CrossSelector(name=\"Select experiments for comparison\", value=[\"a\", \"b\"], options=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "#i_multi_choice_venn = pn.widgets.MultiChoice(name=\"Select experiments for comparison\", value=[\"a\", \"b\"], options=[\"a\", \"b\", \"c\"], max_items = 3, width = 800)\n",
    "\n",
    "i_ref_exp = pn.widgets.Select(name=\"Select experiments as reference\", options=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "dashboard_json = pn.Column(\"Please, upload a file first and press 'Compare clusters'\", name=\"Comparison\", css_classes=[\"content-width\"])\n",
    "\n",
    "comparison_status = pn.Pane(\"No datasets were compared yet\")\n",
    "\n",
    "i_collapse_maps = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "i_collapse_cluster = pn.widgets.Checkbox(value=False, name=\"Collapse cluster\")\n",
    "i_markerset_or_cluster = pn.widgets.Checkbox(value=False, name=\"Display only protein clusters\")\n",
    "i_ranking_boxPlot = pn.widgets.Checkbox(value=False, name=\"Display box plot\")\n",
    "i_toggle_sumORmedian = pn.widgets.Toggle(name=\"Sum or Median\", button_type=\"primary\")\n",
    "i_ExpOverview = pn.widgets.DataFrame()\n",
    "\n",
    "wdgts_comparison = [button_comparison,i_organism_comparison] \n",
    "\n",
    "@pn.depends(i_jsonFile.param.value)#cache_run.param.value\n",
    "def open_jsonFile(jsonFile):#run\n",
    "    cache_run_json.value = False\n",
    "    if jsonFile is None:\n",
    "        filereading_status_json = \"No file is uploaded\"\n",
    "        cache_uploaded_json.value = False\n",
    "        return filereading_status_json\n",
    "    else:\n",
    "        cache_uploaded_json.value = False\n",
    "        try:\n",
    "            global json_dict\n",
    "            #i_jsonFile.value\n",
    "            json_dict = ast.literal_eval(jsonFile.decode(\"UTF-8\")) #i_class.\n",
    "            if hasattr(json_dict, \"keys\") == False: #i_class.\n",
    "                return \"Your json-File does not fulfill the requirements\"\n",
    "            else:\n",
    "                filereading_status_json = list(json_dict.keys())# list(set(list(SpatialDataSet.analysed_datasets_dict.keys()) + )) #i_class.\n",
    "                cache_uploaded_json.value = True\n",
    "                for wdgt in wdgts_comparison:\n",
    "                    wdgt.disabled = False\n",
    "                return pn.Column(pn.Row(\"You will compare following experiments:\\n {}, and {}\".format(\", \".join(filereading_status_json[:-1]), filereading_status_json[-1])),\n",
    "                                 pn.Row(i_organism_comparison),\n",
    "                                 pn.Row(button_comparison),\n",
    "                                 )\n",
    "\n",
    "        except Exception: \n",
    "            filereading_status_json = traceback.format_exc()\n",
    "            cache_uploaded_json.value = False\n",
    "            return filereading_status_json                 \n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusterwidget_comparison.param.value, cache_run_json.param.value, i_xAxis_PCA_comp.param.value, i_yAxis_PCA_comp.param.value, \n",
    "            i_markerset_or_cluster.param.value)\n",
    "def update_visualization_map_comparison(multi_choice, clusterwidget_comparison, run_json, xAxis_PCA_comp, yAxis_PCA_comp, markerset_or_cluster):\n",
    "    try:\n",
    "        if run_json == True:\n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\")\n",
    "                                )\n",
    "            else:\n",
    "                pass\n",
    "            pca_global_comparison = i_class_comp.plot_global_pca_comparison(cluster_of_interest_comparison=clusterwidget_comparison, x_PCA=xAxis_PCA_comp, y_PCA=yAxis_PCA_comp, \n",
    "                                                                       markerset_or_cluster=markerset_or_cluster, multi_choice=multi_choice)\n",
    "            if markerset_or_cluster == False:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(i_clusterwidget_comparison),\n",
    "                                 pn.Row(i_markerset_or_cluster),\n",
    "                                 pn.Row(pca_global_comparison),\n",
    "                                 pn.Row(i_xAxis_PCA_comp, i_yAxis_PCA_comp)  \n",
    "                                )\n",
    "            else:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(i_markerset_or_cluster),\n",
    "                                 pn.Row(pca_global_comparison),\n",
    "                                 pn.Row(i_xAxis_PCA_comp, i_yAxis_PCA_comp)   \n",
    "                                )\n",
    "        else:\n",
    "            pca_global_comparison = \"Run analysis first!\"\n",
    "            return pca_global_comparison\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusterwidget_comparison.param.value, i_collapse_maps.param.value, i_collapse_cluster.param.value, i_clusters_for_ranking.param.value, i_ref_exp.param.value,\n",
    "            i_ranking_boxPlot.param.value, i_toggle_sumORmedian.param.value, cache_run_json.param.value)\n",
    "def update_distance_and_pca(multi_choice, clusterwidget_comparison, collapse_maps, collapse_cluster, clusters_for_ranking, ref_exp, ranking_boxPlot, toggle_sumORmedian, run_json):\n",
    "    try:\n",
    "        if run_json == True:\n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\")\n",
    "                                )\n",
    "            else:\n",
    "                pass\n",
    "            #i_ref_exp.options = multi_choice\n",
    "            update_ref_exp(i_ref_exp)\n",
    "            if clusters_for_ranking == []:\n",
    "                clusters_for_ranking = [clusterwidget_comparison]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            distance_ranking_comparison = i_class_comp.distance_ranking_barplot_comparison(collapse_cluster=collapse_cluster, multi_choice=multi_choice, ref_exp=ref_exp, \n",
    "                                                                                           clusters_for_ranking=clusters_for_ranking, ranking_boxPlot=ranking_boxPlot, \n",
    "                                                                                           toggle_sumORmedian=toggle_sumORmedian)\n",
    "            i_class_comp.cache_cluster_quantified = True\n",
    "            distance_comparison = i_class_comp.distance_boxplot_comparison(collapse_maps=collapse_maps, cluster_of_interest_comparison=clusterwidget_comparison, multi_choice=multi_choice)\n",
    "            if i_class_comp.cache_cluster_quantified == False:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(i_clusterwidget_comparison),\n",
    "                                 pn.Row(\"Cluster was not quantified at all\")\n",
    "                                )\n",
    "            else:\n",
    "                pca_comparison = i_class_comp.plot_pca_comparison(collapse_maps=collapse_maps, cluster_of_interest_comparison=clusterwidget_comparison, multi_choice=multi_choice)\n",
    "                if clusters_for_ranking == []:\n",
    "                    return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                     pn.Row(i_clusterwidget_comparison),\n",
    "                                     pn.Row(i_collapse_maps),\n",
    "                                     pn.Row(pca_comparison),\n",
    "                                     pn.Row(distance_comparison),\n",
    "                                     pn.Row(i_clusters_for_ranking),\n",
    "                                     pn.Row(\"Select at least one cluster\"))\n",
    "                else:\n",
    "                    if i_collapse_cluster.value == True:\n",
    "                        if i_toggle_sumORmedian.value == True:\n",
    "                            return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                         pn.Row(i_clusterwidget_comparison),\n",
    "                                         pn.Row(i_collapse_maps),\n",
    "                                         pn.Row(pca_comparison),\n",
    "                                         pn.Row(distance_comparison),\n",
    "                                         pn.Row(i_clusters_for_ranking),\n",
    "                                         pn.Row(i_collapse_cluster),\n",
    "                                         pn.Row(i_toggle_sumORmedian),\n",
    "                                         pn.Row(i_ref_exp),\n",
    "                                         pn.Row(distance_ranking_comparison),\n",
    "                                         pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster, height=200, width=1800, disabled=True)) #, autosize_mode=\"fit_columns\"\n",
    "                                        )\n",
    "                        else:\n",
    "                            return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                         pn.Row(i_clusterwidget_comparison),\n",
    "                                         pn.Row(i_collapse_maps),\n",
    "                                         pn.Row(pca_comparison),\n",
    "                                         pn.Row(distance_comparison),\n",
    "                                         pn.Row(i_clusters_for_ranking),\n",
    "                                         pn.Row(i_collapse_cluster, i_ranking_boxPlot),\n",
    "                                         pn.Row(i_toggle_sumORmedian),\n",
    "                                         pn.Row(i_ref_exp),\n",
    "                                         pn.Row(distance_ranking_comparison),\n",
    "                                         pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster, height=200, width=1800, disabled=True)) #, autosize_mode=\"fit_columns\"\n",
    "                                        )\n",
    "                    else:\n",
    "                        return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                     pn.Row(i_clusterwidget_comparison),\n",
    "                                     pn.Row(i_collapse_maps),\n",
    "                                     pn.Row(pca_comparison),\n",
    "                                     pn.Row(distance_comparison),\n",
    "                                     pn.Row(i_clusters_for_ranking),\n",
    "                                     pn.Row(i_collapse_cluster),\n",
    "                                     pn.Row(i_ref_exp),\n",
    "                                     pn.Row(distance_ranking_comparison),\n",
    "                                     pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster, height=200, width=1800, disabled=True)) #, autosize_mode=\"fit_columns\"\n",
    "                                    )\n",
    "        else:\n",
    "            pca_comparison = \"Run analysis first!\"\n",
    "            return pca_comparison\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_npr_ngg_nprDc(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                fig_quantity_pg, fig_quantity_pr = i_class_comp.quantity_pr_pg_barplot_comparison(multi_choice=multi_choice)\n",
    "                coverage_barplot = i_class_comp.coverage_comparison(multi_choice=multi_choice)\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(fig_quantity_pg), \n",
    "                                 pn.Row(fig_quantity_pr),\n",
    "                                 pn.Row(coverage_barplot)\n",
    "                                )\n",
    "        else:\n",
    "            completeness_barplot = \"Run analysis first!\"\n",
    "            return completeness_barplot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status \n",
    "    \n",
    "#@pn.depends(i_multi_choice_venn.param.value, cache_run_json.param.value)\n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_venn(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            venn_plot = []\n",
    "            if len(multi_choice)<=1:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select 2 or more experiments for comparison\"))\n",
    "            else:\n",
    "                venn_plot, figure_UpSetPlot = i_class_comp.venn_sections(multi_choice_venn = multi_choice)\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Pane(venn_plot),\n",
    "                                 pn.Row(figure_UpSetPlot)\n",
    "                                )\n",
    "        else:\n",
    "            venn_plot = \"Run analysis first!\"\n",
    "            return venn_plot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status    \n",
    "    \n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_ref_exp.param.value, i_collapse_cluster.param.value, cache_run_json.param.value)\n",
    "def update_dynamic_range_comparison(multi_choice, ref_exp, collapse_cluster, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                dynamic_range_barplot = i_class_comp.dynamic_range_comparison(collapse_cluster=collapse_cluster, multi_choice=multi_choice, ref_exp=ref_exp)\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(dynamic_range_barplot),\n",
    "                                 pn.Row(i_collapse_cluster),\n",
    "                                 pn.Row(i_ref_exp)\n",
    "                                )\n",
    "        else:\n",
    "            dynamic_range_barplot = \"Run analysis first!\"\n",
    "            return dynamic_range_barplot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status \n",
    "\n",
    "    \n",
    "def update_multi_choice(i_multi_choice, i_clusterwidget, i_clusters_for_ranking):\n",
    "    i_multi_choice.options = list(json_dict.keys())\n",
    "    i_clusterwidget.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.value = list(i_class_comp.markerproteins.keys())\n",
    "    i_multi_choice.value = list(json_dict.keys())\n",
    "    \n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ref_exp(multi_choice):\n",
    "    i_ref_exp.options = i_multi_choice.value #i_multi_choice.value\n",
    "    #return i_ref_exp\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ExpOverview(multi_choice):\n",
    "    dict_analysis_parameters={}\n",
    "    for exp_name in multi_choice:\n",
    "        dict_analysis_parameters[exp_name] = json_dict[exp_name][\"Analysis parameters\"]\n",
    "    i_ExpOverview.value = pd.DataFrame.from_dict(dict_analysis_parameters)\n",
    "    i_ExpOverview.disabled = True\n",
    "    i_ExpOverview.height = 200\n",
    "    #dashboard_comparison.insert(4, i_ExpOverview)\n",
    "    #return \n",
    "        \n",
    "def execution_comparison(event):\n",
    "    #prevent execution, if no data is uploaded yet\n",
    "    if cache_uploaded_json.value == False:\n",
    "        comparison_status.object = \"Please upload a JSON-file first\"\n",
    "    else:        \n",
    "        dashboard_comparison.objects[2:] = []\n",
    "        cache_run_json.value = False\n",
    "        for wdgt in wdgts_comparison:\n",
    "            wdgt.disabled = True\n",
    "        try:\n",
    "            comparison_status.object = \"Analysis in progress\"\n",
    "            protein_cluster = SpatialDataSet.markerproteins_set[i_organism_comparison.value].keys()\n",
    "            update_ref_exp(i_ref_exp)\n",
    "            global i_class_comp\n",
    "            i_class_comp = SpatialDataSetComparison(clusters_for_ranking=protein_cluster, ref_exp=i_ref_exp.value, organism=i_organism_comparison.value)\n",
    "            i_class_comp.json_dict = json_dict\n",
    "            i_class_comp.read_jsonFile()\n",
    "            update_multi_choice(i_multi_choice, i_clusterwidget_comparison, i_clusters_for_ranking)\n",
    "            #update_multi_choice_venn(i_multi_choice_venn)\n",
    "            i_class_comp.perform_pca_comparison()\n",
    "            #dashboard_comparison.append(pn.Column(pn.Row(i_multi_choice), pn.Row(i_ExpOverview)))\n",
    "            dashboard_comparison.append(pn.Row(i_multi_choice))\n",
    "            dashboard_comparison.append(pn.Row(i_ExpOverview))\n",
    "            dashboard_comparison.append(comparison_tabs)\n",
    "            comparison_status.object = \"Comparison finished!\"\n",
    "            cache_run_json.value = True\n",
    "        except Exception:\n",
    "            for wdgt in wdgts_comparison:\n",
    "                wdgt.disabled = False\n",
    "            comparison_status.object = traceback.format_exc()\n",
    "            cache_run_json.value = False\n",
    "button_comparison.on_click(execution_comparison)\n",
    "\n",
    "dashboard_comparison = pn.Column(i_jsonFile, open_jsonFile, comparison_status)\n",
    "\n",
    "\n",
    "i_jsonFile_amendments_intended = pn.widgets.FileInput(name=\"Upload JSON file to be amended\")\n",
    "i_json_ExpSelector = pn.widgets.CrossSelector(name=\"Select experiments, that will be removed from JSON file\", width=1000)\n",
    "\n",
    "cache_uploaded_json_amendment = pn.widgets.Checkbox(value=False)\n",
    "#cache_reset = pn.widgets.Checkbox(value=False)\n",
    "cache_run_json_amendment = pn.widgets.Checkbox(value=False)\n",
    "button_reset = pn.widgets.Button(name=\"Reset\", width=50)\n",
    "button_json_amendment = pn.widgets.Button(name=\"Download amendet JSON file\", width=50)\n",
    "download_status = pn.Pane(\"Upload a JSON file first\", width=1000)\n",
    "i_df_ExpComment = pn.widgets.DataFrame()\n",
    "#pn.widgets.Select(name=\"Rename Experiment\")#, options=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "#i_renameExp_text = pn.widgets.TextInput(name=\"Relabel Experiment\", placeholder=\"Enter a experiment name here...\")\n",
    "wdgt_json = [button_reset, button_json_amendment]\n",
    "json_dict_amendments_intended = {}\n",
    "#dict_old_new_ExpName = {}\n",
    "#make a cache, and say, if this hasnt been executed so far, please reset it\n",
    "    \n",
    "@pn.depends(i_jsonFile_amendments_intended.param.value)#cache_run.param.value\n",
    "def open_jsonFile_amendment(jsonFile_amendments):#run\n",
    "    cache_run_json_amendment.value = False\n",
    "    if jsonFile_amendments is None:\n",
    "        #filereading_status_json = \"No file is uploaded\"\n",
    "        cache_uploaded_json_amendment.value = False\n",
    "        #return filereading_status_json\n",
    "    else:\n",
    "        cache_uploaded_json_amendment.value = False\n",
    "        try:\n",
    "            json_dict_cache = ast.literal_eval(i_jsonFile_amendments_intended.value.decode(\"UTF-8\"))\n",
    "            if hasattr(json_dict_cache, \"keys\") == False: \n",
    "                    return \"Your json-File does not fulfill the requirements\"\n",
    "            else:\n",
    "                global json_dict_amendments_intended\n",
    "                #global dict_old_new_ExpName\n",
    "                try:\n",
    "                    json_dict_amendments_intended.update(json_dict_cache)\n",
    "                except Exception:# NameError:\n",
    "                    json_dict_amendments_intended = json_dict_cache\n",
    "\n",
    "                i_json_ExpSelector.options = list(json_dict_amendments_intended.keys())\n",
    "                cache_uploaded_json_amendment.value = True\n",
    "                for wdgt in wdgt_json:\n",
    "                    wdgt.disabled = False\n",
    "                download_status.object = \"Upload sucessful\"\n",
    "                return pn.Column(i_json_ExpSelector,\n",
    "                                 i_df_ExpComment,\n",
    "                                 button_json_amendment,\n",
    "                                 button_reset,\n",
    "                                 )\n",
    "        except Exception: \n",
    "            filereading_status_json = traceback.format_exc()\n",
    "            cache_uploaded_json_amendment.value = False\n",
    "            return filereading_status_json\n",
    "\n",
    "def json_amendment_download(event):\n",
    "    if i_json_ExpSelector.value == []:\n",
    "        download_status.object = \"No experiments are selected\"\n",
    "        return\n",
    "    else:\n",
    "        json_new = json_dict_amendments_intended.copy()\n",
    "        exp_names_del = [elem for elem in i_json_ExpSelector.options if not elem in i_json_ExpSelector.value]        \n",
    "        for key in exp_names_del:\n",
    "            del json_new[key]\n",
    "        checked_exp = set()\n",
    "        redundant_expNames = set(new_Exp for new_Exp in dict_new_expNames.values() if new_Exp in checked_exp or checked_exp.add(new_Exp))\n",
    "        if redundant_expNames != set():\n",
    "            download_status.object = \"Experiments are not allowed to be labelled identically\"\n",
    "            return\n",
    "        else:\n",
    "            for exp_name in json_new:\n",
    "                json_new[exp_name][\"Analysis parameters\"][\"comment\"] = dict_new_expNames[exp_name]\n",
    "            json_new = {dict_new_expNames[oldK]: value for oldK, value in json_new.items()}\n",
    "            with open(\"AnalysedDatasets_2.0.json\", \"w\") as write_file:\n",
    "                json.dump(\n",
    "                    json_new, \n",
    "                    write_file, \n",
    "                    indent=4, \n",
    "                    sort_keys=True\n",
    "                )\n",
    "            download_status.object = \"Download sucessful\"\n",
    "button_json_amendment.on_click(json_amendment_download)\n",
    "\n",
    "def reset_json_amendment(event):\n",
    "    global json_dict_amendments_intended\n",
    "    json_dict_amendments_intended = {}\n",
    "    i_json_ExpSelector.options = []\n",
    "    i_df_ExpComment.value = pd.DataFrame()\n",
    "    for wdgt in wdgt_json:\n",
    "        wdgt.disabled = True\n",
    "    download_status.object = \"Reset sucessful\"\n",
    "button_reset.on_click(reset_json_amendment)\n",
    "\n",
    "dict_new_expNames = {}\n",
    "dict_new_comments = {}\n",
    "\n",
    "@pn.depends(i_json_ExpSelector.param.value, watch=True)\n",
    "def update_renameExp(json_ExpSelector):\n",
    "    dict_ExpComments = {}\n",
    "    for exp_name in json_ExpSelector:\n",
    "        dict_ExpComments[exp_name] = json_dict_amendments_intended[exp_name][\"Analysis parameters\"][\"comment\"]\n",
    "    df_ExpComments = pd.DataFrame(dict_ExpComments.items(), columns=[\"Experiment name - old\", \"Comment\"])#pd.DataFrame.from_dict(dict_ExpComments)\n",
    "    df_ExpComments.insert(0, \"Experiment name - new\", df_ExpComments[\"Experiment name - old\"])\n",
    "    df_ExpComments.set_index(\"Experiment name - old\", inplace=True)\n",
    "    df_ExpComments.replace({\"Experiment name - new\": dict_new_expNames}, inplace=True)\n",
    "    exp_previous = list(dict_new_comments.keys())\n",
    "    for exp in exp_previous:\n",
    "        if exp not in json_ExpSelector:\n",
    "            del dict_new_comments[exp]\n",
    "    df_ExpComments.loc[dict_new_comments.keys(),\"Comment\"] = list(dict_new_comments.values())\n",
    "    i_df_ExpComment.value = df_ExpComments\n",
    "    i_df_ExpComment.height=len(json_ExpSelector)*50\n",
    "    #return i_df_ExpComment\n",
    "\n",
    "@pn.depends(i_df_ExpComment.param.value, watch=True)\n",
    "def update_newExpNames(df_ExpComment):\n",
    "    try:        \n",
    "        global dict_new_expNames \n",
    "        changed_expName = set(list(dict_new_expNames.values())+list(df_ExpComment[\"Experiment name - new\"]))-set(dict_new_expNames.values())\n",
    "        dict_new_expNames = dict(zip(df_ExpComment.index, df_ExpComment[\"Experiment name - new\"]))\n",
    "        global dict_new_comments\n",
    "        dict_new_comments_cache = dict_new_comments.copy()\n",
    "        changed_comment = set(list(dict_new_comments.values())+list(df_ExpComment[\"Comment\"]))-set(dict_new_comments.values())\n",
    "        dict_new_comments = dict(zip(df_ExpComment.index, df_ExpComment[\"Comment\"]))\n",
    "        if changed_expName == set() and changed_comment == set():\n",
    "            tracked_change = \"No changes saved\"\n",
    "        elif changed_expName != set() and changed_comment != set():\n",
    "            tracked_change = \"Upload sucessful\"\n",
    "        elif changed_expName != set():\n",
    "            new_exp = list(changed_expName)[0]\n",
    "            old_exp = list(dict_new_expNames.keys())[list(dict_new_expNames.values()).index(new_exp)]\n",
    "            tracked_change = \"Experiment name was changed from {} to {}\".format(old_exp, new_exp)\n",
    "        else:# changed_comment != set():\n",
    "            new_comment = list(changed_comment)[0]\n",
    "            exp = list(dict_new_comments.keys())[list(dict_new_comments.values()).index(new_comment)]\n",
    "            old_comment = dict_new_comments_cache[exp]\n",
    "            tracked_change = \"Comment of the experiment {} was changed from {} to {}\".format(exp, old_comment, new_comment)\n",
    "        download_status.object = tracked_change\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "dashboard_managaDatasets = pn.Column(i_jsonFile_amendments_intended, open_jsonFile_amendment, download_status)\n",
    "\n",
    "\n",
    "comparison_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "comparison_tabs.append((\"Visualization - organellar maps\", update_visualization_map_comparison))\n",
    "comparison_tabs.append((\"PCA plot and Distance box plot\", update_distance_and_pca))\n",
    "comparison_tabs.append((\"Venn Diagram\", update_venn))\n",
    "comparison_tabs.append((\"Protein Groups/Profiles Quantity\", update_npr_ngg_nprDc))\n",
    "comparison_tabs.append((\"Dynamic Range\", update_dynamic_range_comparison))\n",
    "\n",
    "app_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "app_tabs.append((\"Home\", dasboard_home))\n",
    "app_tabs.append((\"Analysis\", dashboard_analysis))\n",
    "app_tabs.append((\"Download\", show_tabular_overview))\n",
    "app_tabs.append((\"SVM Analysis\", dashboard_svm))\n",
    "app_tabs.append((\"Data comparison\", dashboard_comparison))\n",
    "app_tabs.append((\"Mangae Datasets\", dashboard_managaDatasets))\n",
    "\n",
    "app_tabs.append((\"About\", pn.Row(\"Explanation of what's going on here\", width=2000)))\n",
    "\n",
    "#i_search = pn.widgets.TextInput(name=\"Search\")\n",
    "app_center = pn.Column(pn.Row(pn.Pane(\"# QC tool for Spatial Proteomics\", width = 600),\n",
    "                              pn.layout.HSpacer(),\n",
    "                              #i_search,\n",
    "                              #width=1600, \n",
    "                              margin=10),\n",
    "                       app_tabs,\n",
    "                       #pn.Spacer(background=\"#DDDDDD\", height=100, margin=0)\n",
    "                      )\n",
    "app = pn.GridSpec()#sizing_mode=\"stretch_both\", margin=0)\n",
    "app[0,0] = pn.Spacer(background=\"white\", margin=0) #\"#DDDDDD\"\n",
    "app[0,9] = pn.Spacer(background=\"white\", margin=0) #\"#DDDDDD\"\n",
    "app[0,1:8] = app_center\n",
    "\n",
    "pwd = pn.widgets.PasswordInput(name=\"Please enter password for access.\")\n",
    "app_container = pn.Column(pwd)\n",
    "\n",
    "def check_pwd(event, app=app):\n",
    "    pwd = event.new\n",
    "    if pwd == \"pwd\":\n",
    "        app_container[0]=app\n",
    "pwd.param.watch(check_pwd, \"value\")\n",
    "\n",
    "try:\n",
    "    server.stop()\n",
    "except Exception:\n",
    "    print(\"First server startup\")\n",
    "server = app.show(port=5065, websocket_max_message_size=2000000000)\n",
    "#app.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01ORlog_svm = df_01ORlog.copy()\n",
    "        \n",
    "#df_01_filtered_combined = df_01_filtered_combined.stack([\"Experiment\", \"Map\"]).swaplevel(0,1, axis=0).dropna(axis=1)\n",
    "index_ExpMap = df_01ORlog_svm.index.get_level_values(\"Map\")+\"_\"+df_01ORlog_svm.index.get_level_values(\"Fraction\")\n",
    "index_ExpMap.name = \"Map_Frac\"\n",
    "df_01ORlog_svm.set_index(index_ExpMap, append=True, inplace=True)      \n",
    "\n",
    "df_01ORlog_svm.index = df_01ORlog_svm.index.droplevel([\"Map\", \"Fraction\"])\n",
    "df_01ORlog_svm = df_01ORlog_svm.unstack(\"Map_Frac\")\n",
    "df_01ORlog_svm.columns = [\"_\".join(col) for col in df_01ORlog_svm.columns.values]\n",
    "df_01ORlog_svm.rename(index={\"undefined\" : np.nan}, level=\"Compartment\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eLifeMarkers = pd.read_csv(\"eLife_markers.txt\", sep=\"\\t\", comment=\"#\",\n",
    "#                                       usecols=lambda x: bool(re.match(\"Gene name|Compartment\", x)))\n",
    "#df_eLifeMarkers = df_eLifeMarkers.rename(columns={\"Gene name\":\"Gene names\"})\n",
    "#df_eLifeMarkers = df_eLifeMarkers.astype({\"Gene names\": \"str\"})\n",
    "#i_class.df_eLifeMarkers =  df_eLifeMarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "df = pd.read_csv(\"proteinGroups_DAN.txt\", sep=\"\\t\", comment=\"#\", usecols=lambda x: bool(re.match(SpatialDataSet.regex[\"imported_columns\"], x)), low_memory=False)\n",
    "df.info(verbose=False, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map1_index = df_test[\"Map1\"].to_records()\n",
    "Map2_index = df_test[\"Map2\"].to_records()\n",
    "Map3_index = df_test[\"Map3\"].to_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map1_index#- Map2_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1 = i_class.df_01_stacked_for_quantity.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map1\"].to_numpy()\n",
    "MAP2 = i_class.df_01_stacked_for_quantity.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map2\"].to_numpy()\n",
    "MAP3 = i_class.df_01_stacked_for_quantity.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map3\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1 = i_class.df_01_stacked_for_quantity.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map1\"].to_numpy()\n",
    "MAP2 = i_class.df_01_stacked_for_quantity.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map2\"].to_numpy()\n",
    "MAP3 = i_class.df_01_stacked_for_quantity.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map3\"].to_numpy()\n",
    "\n",
    "MAP1_2 = MAP1-MAP2\n",
    "MAP1_3 = MAP1-MAP3\n",
    "MAP2_3 = MAP2-MAP3\n",
    "\n",
    "df_globalDeltaProfiles = pd.DataFrame(np.concatenate((MAP1_2, MAP1_3, MAP2_3), axis=1)).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_globalDeltaProfiles = pd.DataFrame(np.concatenate((MAP1_2, MAP1_3, MAP2_3), axis=1)).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_globalDeltaProfiles_unimputed = pd.DataFrame(np.concatenate((MAP1_2, MAP1_3, MAP2_3), axis=1)).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df_globalDeltaProfiles, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df_globalDeltaProfiles, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(df_globalDeltaProfiles_unimputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(df_globalDeltaProfiles)#, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df_globalDeltaProfiles_unimputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(0.3)-np.array(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.histogram(df_globalDeltaProfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1_imp = i_class.df_01_stacked.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map1\"].values\n",
    "MAP2_imp = i_class.df_01_stacked.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map2\"].values\n",
    "MAP3_imp = i_class.df_01_stacked.unstack([\"Map\", \"Fraction\"])[\"normalized profile\"][\"Map3\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class.df_01_stacked.unstack([\"Map\", \"Fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP2_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1_imp_noNAN = np.nan_to_num(MAP1_imp)\n",
    "MAP2_imp_noNAN = np.nan_to_num(MAP2_imp)\n",
    "MAP3_imp_noNAN = np.nan_to_num(MAP3_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1_MAP3_noNAN = MAP1_imp_noNAN-MAP3_imp_noNAN\n",
    "MAP1_MAP2_noNAN = MAP1_imp_noNAN-MAP2_imp_noNAN\n",
    "MAP2_MAP3_noNAN = MAP2_imp_noNAN-MAP3_imp_noNAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP2_MAP3_noNAN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_CONCACT = np.concatenate((MAP1_MAP3_noNAN, MAP1_MAP2_noNAN, MAP2_MAP3_noNAN), axis=0)\n",
    "MAP_CONCACT_noZ = MAP_CONCACT[~np.all(MAP_CONCACT == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP1_MAP3_noZ = MAP1_MAP3_noNAN[~np.all(MAP1_MAP3_noNAN == 0, axis=1)]\n",
    "MAP1_MAP2_noZ = MAP1_MAP2_noNAN[~np.all(MAP1_MAP2_noNAN == 0, axis=1)]\n",
    "MAP2_MAP3_noZ = MAP2_MAP3_noNAN[~np.all(MAP2_MAP3_noNAN == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_globalDeltaProfiles_Z = pd.DataFrame(MAP_CONCACT_noZ).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df_globalDeltaProfiles_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean,std\n",
    "mean = mean(df_globalDeltaProfiles_Z)\n",
    "std = std(df_globalDeltaProfiles_Z)\n",
    "print (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "pdf = stats.norm.pdf(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "distribution = norm(mean, std)\n",
    "\n",
    "min_weight = min(df_globalDeltaProfiles_Z)\n",
    "max_weight = max(df_globalDeltaProfiles_Z)\n",
    "#values = list(range(int(min_weight), int(max_weight), 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(min_weight, max_weight, 0.001) # range of x in spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [distribution.pdf(v) for v in x]\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(df_globalDeltaProfiles_unimputed, bins=200, density=True) # , \n",
    "pyplot.plot(x, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [distribution.pdf(v) for v in x]\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(df_globalDeltaProfiles_unimputed, bins=200, density=True) # , \n",
    "pyplot.plot(x, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['nanoExpl_100min_33w_libaryDIA_Spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eLifeMarkers = pd.read_csv(\"eLife_markers.txt\", sep=\"\\t\", comment=\"#\",\n",
    "#                                       usecols=lambda x: bool(re.match(\"Gene name|Compartment\", x)))\n",
    "#df_eLifeMarkers = df_eLifeMarkers.rename(columns={\"Gene name\":\"Gene names\"})\n",
    "#df_eLifeMarkers = df_eLifeMarkers.astype({\"Gene names\": \"str\"})\n",
    "#i_class.df_eLifeMarkers =  df_eLifeMarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_choice = i_multi_choice.value\n",
    "clusters_for_ranking = i_class_comp.clusters_for_ranking\n",
    "ref_exp = i_ref_exp.value\n",
    "ranking_boxPlot=True\n",
    "collapse_cluster=False\n",
    "\n",
    "df_distance_comp = i_class_comp.df_distance_comp.copy()\n",
    "df_distance_comp = df_distance_comp[df_distance_comp[\"Experiment\"].isin(multi_choice)]\n",
    "df_distance_comp = df_distance_comp[df_distance_comp[\"Cluster\"].isin(clusters_for_ranking)]\n",
    "\n",
    "df_quantified_cluster = df_distance_comp.reset_index()\n",
    "df_quantified_cluster = df_quantified_cluster.drop_duplicates(subset=[\"Cluster\", \"Experiment\"]).set_index([\"Cluster\", \n",
    "                                                                                                           \"Experiment\"])[\"distance\"].unstack(\"Cluster\")\n",
    "i_class_comp.df_quantified_cluster = df_quantified_cluster.notnull().replace({True: \"x\", False: \"-\"})\n",
    "\n",
    "#dict_cluster_normalizedMedian = {}\n",
    "dict_cluster_normalizedMedian_ref = {}\n",
    "dict_median_distance_ranking = {}\n",
    "for cluster in i_class_comp.markerproteins.keys():\n",
    "    try:\n",
    "        df_cluster = df_distance_comp[df_distance_comp[\"Cluster\"]==cluster]\n",
    "        all_median_one_cluster_several_exp = {}\n",
    "        for exp in multi_choice:\n",
    "            median = df_cluster[df_cluster[\"Experiment\"]==exp].median()\n",
    "            all_median_one_cluster_several_exp[exp] = float(median)\n",
    "            #new\n",
    "            if exp == ref_exp:\n",
    "                ref = median\n",
    "        dict_median_distance_ranking[cluster] = all_median_one_cluster_several_exp\n",
    "        min_median = min(all_median_one_cluster_several_exp.items(), key=lambda x: x[1])[1]\n",
    "        #median_ranking = {exp: median/min_median for exp, median in all_median_one_cluster_several_exp.items()}\n",
    "        #dict_cluster_normalizedMedian[cluster] = median_ranking\n",
    "        #new\n",
    "        median_ranking_ref = {exp: median/ref[0] for exp, median in all_median_one_cluster_several_exp.items()}\n",
    "        dict_cluster_normalizedMedian_ref[cluster] = median_ranking_ref\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "#df_cluster_normalizedMedian = pd.DataFrame(dict_cluster_normalizedMedian)\n",
    "#df_cluster_normalizedMedian.index.name=\"Experiment\"\n",
    "#df_cluster_normalizedMedian.rename_axis(\"Cluster\", axis=1, inplace=True)\n",
    "#df_ranking = df_cluster_normalizedMedian.stack(\"Cluster\")\n",
    "#df_ranking.name=\"Normalized Median\"\n",
    "#df_ranking = df_ranking.reset_index()\n",
    "#ranking_sum = df_cluster_normalizedMedian.sum(axis=1).round(2)\n",
    "#ranking_sum.name = \"Normalized Median - Sum\"\n",
    "#ranking_product = df_cluster_normalizedMedian.product(axis=1).round(2)\n",
    "#ranking_product.name = \"Normalized Median - Product\"\n",
    "#df_globalRanking = pd.concat([pd.DataFrame(ranking_sum), pd.DataFrame(ranking_product)], axis=1).reset_index()\n",
    "#self.sorting_list = list(df_globalRanking.sort_values(\"Normalized Median - Sum\")[\"Experiment\"])\n",
    "#set categroical column, allowing lexicographic sorting\n",
    "#df_ranking = df_ranking.assign(Experiment_lexicographic_sort = pd.Categorical(df_ranking[\"Experiment\"], categories=self.sorting_list, ordered=True))\n",
    "#df_ranking.sort_values(\"Experiment_lexicographic_sort\", inplace=True)\n",
    "#df_globalRanking = df_globalRanking.assign(Experiment_lexicographic_sort = pd.Categorical(df_globalRanking[\"Experiment\"], categories=self.sorting_list,\n",
    "#                                                                                          ordered=True))\n",
    "#df_globalRanking.sort_values(\"Experiment_lexicographic_sort\", inplace=True)\n",
    "#df_ranking[\"Experiment_lexicographic_sort\"] = pd.Categorical(df_ranking[\"Experiment\"], categories=self.sorting_list, ordered=True)\n",
    "#df_ranking.sort_values(\"Experiment_lexicographic_sort\", inplace=True)\n",
    "#df_globalRanking[\"Experiment_lexicographic_sort\"] = pd.Categorical(df_globalRanking[\"Experiment\"], categories=self.sorting_list, ordered=True)\n",
    "#df_globalRanking.sort_values(\"Experiment_lexicographic_sort\", inplace=True)\n",
    "\n",
    "df_cluster_normalizedMedian_ref = pd.DataFrame(dict_cluster_normalizedMedian_ref)\n",
    "df_cluster_normalizedMedian_ref.index.name=\"Experiment\"\n",
    "df_cluster_normalizedMedian_ref.rename_axis(\"Cluster\", axis=1, inplace=True)\n",
    "\n",
    "#median makes a huge differnece, improves result of DIA, MQ, libary\n",
    "df_RelDistanceRanking = pd.concat([df_cluster_normalizedMedian_ref.median(axis=1), df_cluster_normalizedMedian_ref.sem(axis=1)], axis=1, \n",
    "                                  keys=[\"Distance Ranking (rel, median)\", \"SEM\"]).reset_index().sort_values(\"Distance Ranking (rel, median)\")\n",
    "i_class_comp.sorting_list = list(df_RelDistanceRanking[\"Experiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_sum = df_cluster_normalizedMedian_ref.sum(axis=1).round(2)\n",
    "ranking_sum.name = \"Normalized Median - Sum\"\n",
    "df_ranking_sum = ranking_sum.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df_ranking_sum.sort_values(\"Normalized Median - Sum\"), \n",
    "                                            x=\"Experiment\",\n",
    "                                            y=\"Normalized Median - Sum\", \n",
    "                                            title=\"Ranking - median of all individual normalized medians - reference experiment: {}\".format(ref_exp),\n",
    "                                            color=\"Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_normalizedMedian_ref = df_cluster_normalizedMedian_ref.stack(\"Cluster\")\n",
    "df_cluster_normalizedMedian_ref.name=\"Normalized Median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_normalizedMedian_ref = df_cluster_normalizedMedian_ref.reset_index()\n",
    "\n",
    "df_cluster_normalizedMedian_ref = df_cluster_normalizedMedian_ref.assign(Experiment_lexicographic_sort = pd.Categorical(df_cluster_normalizedMedian_ref[\"Experiment\"], \n",
    "                                                                                                                        categories=i_class_comp.sorting_list, ordered=True))\n",
    "df_cluster_normalizedMedian_ref.sort_values(\"Experiment_lexicographic_sort\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_normalizedMedian_ref.unstack(\"Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_uploaded_json_amendments=False\n",
    "cache_uploaded_json_amendments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache_uploaded_json_amendments == True and \"json_dict\" in globals():\n",
    "    print(\"NICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anal(event):\n",
    "    with open(\"AnalysedDatasets.json\", \"w\") as write_file:\n",
    "        json.dump(\n",
    "            SpatialDataSetComparison.analysed_datasets_dict, \n",
    "            write_file, \n",
    "            indent=4, \n",
    "            sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedoWN = pn.widgets.FileDownload(\n",
    "    =anal, filename = \"oral.csv\")\n",
    "pn.Column(filedoWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cluster_normalizedMedian.index.name=\"Experiment\"\n",
    "#df_cluster_normalizedMedian.rename_axis(\"Cluster\", axis=1, inplace=True)\n",
    "#df_ranking = df_cluster_normalizedMedian.stack(\"Cluster\")\n",
    "#df_ranking.name=\"Normalized Median\"\n",
    "#df_ranking = df_ranking.reset_index()\n",
    "#ranking_sum = df_cluster_normalizedMedian.sum(axis=1).round(2)\n",
    "#ranking_sum.name = \"Normalized Median - Sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collapse_cluster == False:\n",
    "    fig_ranking = px.bar(df_cluster_normalizedMedian_ref, \n",
    "                         x=\"Cluster\", \n",
    "                         y=\"Normalized Median\", \n",
    "                         color=\"Experiment\", \n",
    "                         barmode=\"group\", \n",
    "                         title=\"Ranking - normalization to reference experiment: {}\".format(ref_exp)\n",
    "                        )\n",
    "    \n",
    "    fig_ranking.update_xaxes(categoryorder=\"total ascending\")\n",
    "    fig_ranking.update_layout(autosize=False,\n",
    "                              width=1200 if len(multi_choice)<=3 else 300*len(multi_choice),\n",
    "                              height=500\n",
    "                             )\n",
    "    \n",
    "    #fig_ranking = px.bar(df_ranking, \n",
    "    #                     x=\"Cluster\", \n",
    "    #                     y=\"Normalized Median\", \n",
    "    #                     color=\"Experiment\", \n",
    "    #                     barmode=\"group\", \n",
    "    #                     title=\"Ranking - normalization to smallest median (=1)\"\n",
    "    #                    )\n",
    "    #\n",
    "    #fig_ranking.update_xaxes(categoryorder=\"total ascending\")\n",
    "    #fig_ranking.update_layout(autosize=False,\n",
    "    #                          width=1200 if len(multi_choice)<=3 else 300*len(multi_choice),\n",
    "    #                          height=500\n",
    "    #                         )\n",
    "    #\n",
    "    #return pn.Column(pn.Row(fig_ranking), pn.Row(fig_ranking2))\n",
    "    return fig_ranking\n",
    "\n",
    "else:\n",
    "    #fig_globalRanking = px.bar(df_globalRanking, \n",
    "    #                           x=\"Experiment\", \n",
    "    #                           y=\"Normalized Median - Sum\", \n",
    "    #                           color=\"Experiment\", \n",
    "    #                           title=\"Ranking - sum of all individual normalized medians\"\n",
    "    #                          )\n",
    "    #fig_globalRanking.update_layout(autosize=False,\n",
    "    #                                width=250*len(multi_choice),\n",
    "    #                                height=500,\n",
    "    #                               )\n",
    "    if ranking_boxPlot ==False:\n",
    "        fig_globalRanking = px.bar(df_RelDistanceRanking.sort_values(\"Distance Ranking (rel, median)\"), \n",
    "                                    x=\"Experiment\",\n",
    "                                    y=\"Distance Ranking (rel, median)\", \n",
    "                                    title=\"Ranking - median of all individual normalized medians - reference experiment: {}\".format(ref_exp),\n",
    "                                    error_x=\"SEM\", error_y=\"SEM\", \n",
    "                                    color=\"Experiment\")\n",
    "        \n",
    "\n",
    "                                    \n",
    "    else: \n",
    "        fig_globalRanking = px.box(df_cluster_normalizedMedian_ref,\n",
    "                                   x=\"Experiment\",\n",
    "                                   y=\"Normalized Median\", \n",
    "                                   title=\"Ranking - median of all individual normalized medians - reference experiment: {}\".format(ref_exp),\n",
    "                                   color=\"Experiment\",\n",
    "                                   points=\"all\",\n",
    "                                   hover_name=\"Cluster\")\n",
    "    #return pn.Column(pn.Row(fig_globalRanking), pn.Row(fig_globalRanking2))\n",
    "    \n",
    "    \n",
    "    fig_globalRanking.update_layout(autosize=False,\n",
    "                                    width=250*len(multi_choice),\n",
    "                                    height=500,\n",
    "                                    )\n",
    "    \n",
    "    return fig_globalRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = i_class.df_log_stacked#[\"log profile\"]#.unstack(\"Map\").reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(df.reset_index(), group_labels, colors=colors, bin_size=.25,\n",
    "                         show_curve=False)\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title_text=\"Hist and Rug Plot\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_npgf_dc = i_class.df_npgf_dc\n",
    "df_npgf_dc.rename_axis(\"Fraction\", axis=1, inplace=True)\n",
    "df_npgf_dc = df_npgf_dc.rename({0:\"Data completeness\"}, axis=\"index\").T\n",
    "df_npgf_dc.reset_index(inplace=True)\n",
    "df_npgf_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df_npgf_dc, x=\"Fraction\", y=\"Data completeness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC and PG - Fractions / Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df_npgf, x=\"Fraction\", y=\"Protein Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BytesIO(i_file.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(BytesIO(i_file.value), sep=\"\\t\", comment=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_file = pn.widgets.FileInput(name=\"Upload file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_file.get_param_values()#[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.literal_eval(i_jsonFile.value.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_jsonFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_jsonFile_adjusting = pn.widgets.FileInput(name=\"Upload JSON file for adjustment\")\n",
    "i_jsonFile_adjusting#.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict_adjusting = ast.literal_eval(i_jsonFile_adjusting.value.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_UpSetPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_multi_choice.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#    if i_class_comp.multi_choice not in list(i_class_comp.json_dict.keys()):\n",
    "#    #        i_class_comp.multi_choice = list(i_class_comp.json_dict.keys())[0:2] \n",
    "#            i_multi_choice.value = list(i_class_comp.json_dict.keys())[0:2] \n",
    "#        \n",
    "#    if i_class_comp.i_clusterwidget not in list(i_class_comp.json_dict.keys()):\n",
    "##        i_class_comp.multi_choice = list(i_class_comp.json_dict.keys())[0:2] \n",
    "#        i_multi_choice.value = list(i_class_comp.json_dict.keys())[0:2] \n",
    "#    \n",
    "#    if i_class_comp.i_clusters_for_ranking not in list(i_class_comp.json_dict.keys()):\n",
    "##    i_class_comp.multi_choice = list(i_class_comp.json_dict.keys())[0:2] \n",
    "#    i_multi_choice.value = list(i_class_comp.json_dict.keys())[0:2] \n",
    "#            \n",
    "##def update_object_selector(i_mapwidget, i_clusterwidget):\n",
    "##    i_mapwidget.options = list(i_class.map_names)\n",
    "##    i_clusterwidget.options = list(i_class.markerproteins.keys())\n",
    "\n",
    "    \n",
    "#def update_multi_choice_venn(i_multi_choice_venn):\n",
    "#    i_multi_choice_venn.options = list(json_dict.keys())\n",
    "    #i_multi_choice_venn.value = list(json_dict.keys())[0:2]\n",
    "    \n",
    "    #if i_class_comp.multi_choice_venn not in list(i_class_comp.json_dict.keys()):\n",
    "    #        i_class_comp.multi_choice_venn = list(i_class_comp.json_dict.keys())[0:2]  \n",
    "    #        i_multi_choice_venn.value = list(i_class_comp.json_dict.keys())[0:2] \n",
    "     \n",
    "        \n",
    "#                   i_renameExp.disabled =  False\n",
    "                    #dict_old_new_ExpName = dict.fromkeys(json_dict_amendments_intended.keys(), \"\")\n",
    "#                    i_renameExp.disabled =  True\n",
    "                    #dict_old_new_ExpName = dict.fromkeys(json_dict_amendments_intended.keys(), \"\")\n",
    "                #filereading_status_json = list(json_dict_amendments_intended.keys())\n",
    "              \n",
    "    #        for key in dict_old_new_ExpName.keys():\n",
    "    #            if dict_old_new_ExpName[key] == \"\":\n",
    "    #                dict_old_new_ExpName[key] = key  \n",
    "    \n",
    "#    i_renameExp.disabled = False\n",
    "#    i_renameExp.options = json_ExpSelector\n",
    "#    return i_renameExp \n",
    "#\n",
    "#@pn.depends(i_renameExp_text.param.value, watch=True)\n",
    "#def update_dict_expName(renameExp_text):\n",
    "#    dict_old_new_ExpName[i_renameExp.value] = renameExp_text \n",
    "#\n",
    "#@pn.depends(i_renameExp.param.value, watch=True)\n",
    "#def update_text(renameExp):\n",
    "#    i_renameExp_text.value = dict_old_new_ExpName[renameExp]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####SILAC\n",
    "###df_data_completeness = i_class.df_index[\"Ratio H/L\"].stack([\"Fraction\"])\n",
    "###data_completeness = 1-df_data_completeness.apply(np.isnan).apply(sum)/len(df_data_completeness)\n",
    "###data_completeness = data_completeness.append(pd.Series(data_completeness.mean(), index=[\"combined\"]))\n",
    "###data_completeness.rename(\"Data completness\", inplace=True)\n",
    "####LFQ\n",
    "###df_data_completeness = i_class.df_index[\"LFQ intensity\"].stack([\"Fraction\"])\n",
    "###data_completeness = 1-df_data_completeness.apply(np.isnan).apply(sum)/len(df_data_completeness)\n",
    "###data_completeness = data_completeness.append(pd.Series(data_completeness.mean(), index=[\"combined\"]))\n",
    "###data_completeness.rename(\"Data completness\", inplace=True)\n",
    "####LFQ and SILAC\n",
    "###df_profile_completeness = i_class.df_01_stacked[\"normalized profile\"].xs(i_class.fractions[0], level=\"Fraction\", axis=0).unstack([\"Map\"])\n",
    "###profile_completeness = 1-df_profile_completeness.apply(np.isnan).apply(sum)/len(df_profile_completeness)\n",
    "###profile_completeness = profile_completeness.append(pd.Series(profile_completeness.mean(), index=[\"combined\"]))\n",
    "###profile_completeness.rename(\"Profile completness\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "###df_completeness = pd.concat([data_completeness, profile_completeness], axis=1)\n",
    "###df_completeness.index.name = \"Map\"###################\n",
    "#SVM\n",
    "###################\n",
    "\n",
    "#df_01_test = i_class.df_01_stacked.copy()\n",
    "#\n",
    "##df_01_filtered_combined = df_01_filtered_combined.stack([\"Experiment\", \"Map\"]).swaplevel(0,1, axis=0).dropna(axis=1)\n",
    "#index_ExpMap = df_01_test.index.get_level_values(\"Map\")+\"_\"+df_01_test.index.get_level_values(\"Fraction\")\n",
    "#index_ExpMap.name = \"Map_Frac\"\n",
    "#df_01_test.set_index(index_ExpMap, append=True, inplace=True)      \n",
    "#\n",
    "#df_01_test.index = df_01_test.index.droplevel([\"Map\", \"Fraction\"])\n",
    "#df_01_test = df_01_test.unstack(\"Map_Frac\")\n",
    "#df_01_test.columns = [\"_\".join(col) for col in df_01_test.columns.values]\n",
    "#df_01_test.rename(index={\"undefined\" : np.nan}, level=\"Compartment\", inplace=True)\n",
    "#df_01_test\n",
    "#compartments = i_class.df_eLifeMarkers[\"Compartment\"].unique()\n",
    "#compartment_color = dict(zip(compartments, i_class.css_color))\n",
    "#compartment_color[\"undefined\"] = \"lightgrey\"\n",
    "#\n",
    "#collapse_maps_PCA_test = False\n",
    "#df_global_pca = i_class.df_pca.unstack(\"Map\").swaplevel(0,1, axis=1)[i_class.map_of_interest].reset_index()\n",
    "##        else:\n",
    "# #           df_global_pca = self.df_pca.rename_axis(\"PC\", axis=\"columns\").unstack(\"Map\").stack(\"PC\").median(axis=1).to_frame().unstack(\"PC\")\n",
    "# #           df_global_pca.columns = df_global_pca.columns.droplevel()\n",
    "  #          df_global_pca.reset_index(inplace=True)\n",
    "\n",
    "#if collapse_maps_PCA_test == False:\n",
    "#    title_PCA = \"Protein subcellular localization by PCA of {}\".format(i_class.map_of_interest)\n",
    "#else:\n",
    "#    title_PCA = \"Protein subcellular localization by PCA of combined maps\"\n",
    "        \n",
    "#fig_global_pca = px.scatter(data_frame=df_global_pca,\n",
    "#                            x=\"PC1\",\n",
    "#                            y=\"PC3\",\n",
    "#                            color=\"Compartment\",\n",
    "#                            color_discrete_map=compartment_color,\n",
    "#                            title= \"Protein subcellular localization by PCA for {}\".format(i_class.map_of_interest) \n",
    "#                            if collapse_maps_PCA_test == False else \"Protein subcellular localization by PCA of combined maps\", \n",
    "#                            #title_PCA,\n",
    "#                            hover_data=[\"Protein IDs\", \"Gene names\", \"Compartment\"],\n",
    "#                    #       custom_data=df_annotated_all.columns,\n",
    "#                            opacity=0.9\n",
    "#                            )\n",
    "#fig_global_pca       \n",
    "######################################################WITH RNAKING \n",
    "##dict_cluster_ranking = {}\n",
    "##dict_cluster_normalizedMedian = {}\n",
    "##for cluster in i_class.markerproteins.keys():\n",
    "##    df_cluster = df_distance_map_cluster_gene_in_index.xs(cluster, level=\"Cluster\")\n",
    "##    all_median_one_cluster_several_exp = {}\n",
    "##    for exp in i_class.json_dict.keys():\n",
    "##        median = df_cluster.xs(exp, level=\"Experiment\").median()\n",
    "##        #df_cluster[df_cluster.index.get_level_values(\"Experiment\").isin([exp])].median()\n",
    "##        all_median_one_cluster_several_exp[exp] = float(median)\n",
    "##    \n",
    "##    min_median = min(all_median_one_cluster_several_exp.items(), key=lambda x: x[1])[1]\n",
    "##    median_ranking = {exp: median/min_median for exp, median in all_median_one_cluster_several_exp.items()}\n",
    "##    dict_cluster_normalizedMedian[cluster] = median_ranking\n",
    "##    \n",
    "##    dict_cluster_normalizedMedian[cluster] = median_ranking\n",
    "##    \n",
    "##    ranking_one_cluster = {key: rank for rank, key in enumerate(sorted(all_median_one_cluster_several_exp, key=all_median_one_cluster_several_exp.get), 1)}\n",
    "##    dict_cluster_ranking[cluster] = ranking_one_cluster\n",
    "##\n",
    "##df_ranking = pd.concat([pd.DataFrame(dict_cluster_ranking), pd.DataFrame(dict_cluster_normalizedMedian)], keys=[\"Ranking\", \"Normalized median\"], names=[\"Type\", \"Experiment\"])   \n",
    "##df_ranking.rename_axis(\"Cluster\", axis=1, inplace=True)\n",
    "##df_rank_for_plot = df_ranking.unstack(\"Type\").stack(\"Cluster\").reset_index()\n",
    "##df_rank_for_plot = df_rank_for_plot.sort_values([\"Normalized median\", \"Experiment\"])\n",
    "##fig2 =px.bar(df_rank_for_plot, x=\"Cluster\", y=\"Normalized median\", color=\"Experiment\", barmode=\"group\")\n",
    "##fig2.update_xaxes(categoryorder=\"total ascending\")\n",
    "##fig2 \n",
    "\n",
    "##ranking_sum = df_ranking.sum(axis=1).round(2)\n",
    "##df_ranking2 = df_ranking.copy()\n",
    "##df_ranking2[\"Ranking - Product\"] = df_ranking2.product(axis=1).round(2)\n",
    "##df_ranking2[\"Ranking - Sum\"] = ranking_sum\n",
    "##df_ranking2\n",
    "\n",
    "#on_validProfiles2 = {}\n",
    "#or maps in i_class.map_names : \n",
    "#   series_non_validProfiles_map = df_MQ[\"normalized profile\"].xs(maps, level=\"Map\").unstack(\"Fraction\").apply(lambda x: x.isnull().any(), axis=1)\n",
    "#   total_protein_ids_map = len(series_non_validProfiles_map)\n",
    "#   non_validProfiles_map = len(series_non_validProfiles_map[series_non_validProfiles_map == True].index)\n",
    "#   non_validProfiles2[maps] = non_validProfiles_map/total_protein_ids_map*100\n",
    "#eries_non_validProfiles_combined = df_MQ.unstack([\"Fraction\", \"Map\"])[\"normalized profile\"].apply(lambda x: x.isnull().any(), axis=1)\n",
    "#otal_protein_ids_combined = len(series_non_validProfiles_combined)\n",
    "#on_validProfiles_combined = len(series_non_validProfiles_combined[series_non_validProfiles_combined == True].index)\n",
    "#on_validProfiles2[\"combined maps\"] = non_validProfiles_combined/total_protein_ids_combined*100\n",
    "#self.shape_dict[\"Non valid profiles\"] = non_validProfiles\n",
    "#on_validProfiles2\n",
    "\n",
    "\n",
    "##df_lognorm_MAP1 = df_lognorm_ratio_stacked.loc(axis=0)[:,:,:,:,:,:,:,:,[\"MAP1\"]]\n",
    "##px.histogram(df_lognorm_MAP1.reset_index(), \n",
    "##x=\"normalized profile\",\n",
    "##color=\"Map_Frac\",\n",
    "##facet_row=\"Map_Frac\")\n",
    "#\n",
    "#timsTOF = i_class.df_01_stacked.copy()\n",
    "#timsTOF_tojson = i_class.df_01_stacked[\"normalized profile\"].unstack(\"Map\").median(axis=1).to_frame(name=\"normalized profile\")#.unstack(\"PC\")\n",
    "#\n",
    "#\n",
    "#test_summary_dict = {\"timsTOF\" : {\"0/1 normalized data\" : timsTOF_tojson.reset_index().to_json() }}\n",
    "#test_summary_dict[\"Expl\"] = {\"0/1 normalized data\" : Expl_tojson.reset_index().to_json() }\n",
    "#\n",
    "#for exp_name in test_summary_dict.keys():\n",
    "#    #print(exp_name)\n",
    "#    for data_type in test_summary_dict[exp_name].keys():\n",
    "#        if data_type == \"0/1 normalized data\" and exp_name == list(test_summary_dict.keys())[0]:\n",
    "#            #convert into dataframe\n",
    "#            df_01_combined = pd.read_json(test_summary_dict[exp_name][data_type])\n",
    "#            #get only 01 normalized data \n",
    "#            df_01_combined = df_01_combined.set_index([\"Fraction\", \"Gene names\", \"Protein IDs\", \"Compartment\"])[[\"normalized profile\"]].unstack([\"Fraction\"])\n",
    "#            df_01_combined.rename(columns = {\"normalized profile\":exp_name}, inplace=True)\n",
    "#\n",
    "#        elif data_type == \"0/1 normalized data\" and exp_name != list(test_summary_dict.keys())[0]:\n",
    "#            df_01_toadd = pd.read_json(test_summary_dict[exp_name][data_type])\n",
    "#            df_01_toadd = df_01_toadd.set_index([\"Fraction\", \"Gene names\", \"Protein IDs\", \"Compartment\"])[[\"normalized profile\"]].unstack([\"Fraction\"])\n",
    "#            df_01_toadd.rename(columns = {\"normalized profile\":exp_name}, inplace=True)\n",
    "#            #dataframes will be concatenated, only proteins/Profiles that are in both df will be retained\n",
    "#            df_01_combined = pd.concat([df_01_combined, df_01_toadd], axis=1, join=\"inner\")\n",
    "#            \n",
    "#            \n",
    "#df_01_filtered_combined = df_01_combined.dropna()    \n",
    "#df_01_filtered_combined.columns.names = [\"Experiment\", \"Fraction\"]\n",
    "##reframe it to make it ready for PCA\n",
    "#df_01_filtered_combined = df_01_filtered_combined.stack([\"Experiment\"])\n",
    "#\n",
    "#\n",
    "#pca = PCA(n_components=3)\n",
    "#\n",
    "## df_pca: PCA processed dataframe, containing the columns \"PC1\", \"PC2\", \"PC3\"\n",
    "#df_pca = pd.DataFrame(pca.fit_transform(df_01_filtered_combined))\n",
    "#df_pca.columns = [\"PC1\", \"PC2\", \"PC3\"]\n",
    "#df_pca.index = df_01_filtered_combined.index\n",
    "#\n",
    "#\n",
    "#\n",
    "#df_pca_for_plotting = i_class.df_global_pca_for_plotting\n",
    "#df_pca_for_plotting\n",
    "#\n",
    "#\n",
    "#fig_pca = px.scatter(data_frame=i_class.df_global_pca_for_plotting.reset_index(),\n",
    "#                     x=\"PC1\",\n",
    "#                     y=\"PC3\",\n",
    "#                     color=\"Compartment\",\n",
    "#                     color_discrete_map={\n",
    "#                         \"undefined\": \"lightgrey\",\n",
    "#                         \"Endosome\": \"green\",\n",
    "#                         \"Ergic/cisGolgi\": \"blue\",\n",
    "#                         \"Large Protein Complex\": \"orange\",\n",
    "#                         \"Nuclear pore complex\": \"purple\",\n",
    "#                         \"Peroxisome\" : \"goldenrod\",\n",
    "#                         \"Lysosome\": \"yellow\",\n",
    "#                         \"Plasma membrane\" : \"lightcoral\",\n",
    "#                         \"Actin binding proteins\" : \"magenta\",\n",
    "#                         \"ER\": \"brown\",\n",
    "#                         \"ER_high_curvature\" : \"lightpink\",\n",
    "#                         \"Golgi\": \"red\",\n",
    "#                         \"Mitochondrion\": \"turquoise\",\n",
    "#                     },\n",
    "#                     facet_col=\"Experiment\",\n",
    "#                     #facet_col_wrap=4,\n",
    "##  facet_col_wrap=len(i_class.fractions),     \n",
    "#                         \n",
    "#                     )\n",
    "#fig_pca\n",
    "#\n",
    "#\n",
    "##compartments = self.df_eLifeMarkers[\"Compartment\"].unique()\n",
    "##compartment_color = dict(zip(compartments, css_color))\n",
    "##compartment_color[\"undefined\"] = \"lightgrey\"\n",
    "#\n",
    "#fig_pca = px.scatter(data_frame=i_class.df_pca.reset_index(),\n",
    "#                     x=\"PC1\",\n",
    "#                     y=\"PC3\",\n",
    "#                     color=\"Compartment\",\n",
    "#                     color_discrete_map=compartment_color,\n",
    "#                    title=\"Protein subcellular localization by PCA\",\n",
    "#                    hover_data=[\"Gene names\", \"Compartment\"], #\"Protein names\", \n",
    "#                    #custom_data=df_annotated_all.columns,\n",
    "#                    opacity=0.9\n",
    "#                    )\n",
    "#fig_pca                         \n",
    "\n",
    "#df_fractionnumber_stacked = i_class.df_index.copy().stack(\"Fraction\")\n",
    "#number_fractions = len(df_fractionnumber_stacked.index.get_level_values(\"Fraction\").unique())\n",
    "#\n",
    "#df_index = i_class.df_index.stack(\"Map\")\n",
    "#df_index.sort_index(axis=1, level=0, inplace=True)\n",
    "#df_mscount_mapstacked = df_index.loc[df_index[(\"MS/MS count\")].apply(np.sum, axis=1) >= (\n",
    "#        number_fractions * 2)]\n",
    "#df_stringency_mapfracstacked = df_mscount_mapstacked.copy()\n",
    "#df_stringency_mapfracstacked = df_stringency_mapfracstacked.loc[\n",
    "#    df_stringency_mapfracstacked[(\"LFQ intensity\")].apply(lambda x: any(\n",
    "#        np.invert(np.isnan(x)).rolling(window=4).sum() >=\n",
    "#        4), axis=1)]\n",
    "#df_stringency_mapfracstacked = df_stringency_mapfracstacked.copy().stack(\"Fraction\")\n",
    "#\n",
    "#df_stringency_mapfracstacked\n",
    "\n",
    "#df_eLifeMarkers = pd.read_csv(\"eLife_markers.txt\", sep=\"\\t\", comment=\"#\",\n",
    "#                                       usecols=lambda x: bool(re.match(\"Gene name|Compartment\", x)))\n",
    "#df_eLifeMarkers = df_eLifeMarkers.rename(columns={\"Gene name\":\"Gene names\"})\n",
    "#df_eLifeMarkers = df_eLifeMarkers.astype({\"Gene names\": \"str\"})\n",
    "#df_eLifeMarkers\n",
    "\n",
    "#df_index_annotated_noIndex = df_stringency_mapfracstacked.reset_index()\n",
    "#df_index_annotated_noIndex = df_index_annotated_noIndex.merge(df_eLifeMarkers, how=\"outer\", on=\"Gene names\", indicator = True)\n",
    "##df_index_annotated_noIndex = df_index_annotated_noIndex.sort_values(\"Compartment\")\n",
    "#df_an = df_index_annotated_noIndex.loc[df_index_annotated_noIndex[\"_merge\"].isin([\"both\",\"left_only\"])].drop(\"_merge\", axis=1)\n",
    "#df_an.set_index([c for c in df_an.columns if c != \"MS/MS count\" and c != \"LFQ intensity\"], inplace=True)\n",
    "#df_an.rename(index={np.nan : \"undefined\"}, level=\"Compartment\", inplace=True)\n",
    "#df_an\n",
    "\n",
    "#df_pca_MAP1 = i_class.df_pca.unstack(\"Map\").swaplevel(0,1, axis=1)[\"Map2\"].reset_index()\n",
    "#df_pca_MAP1\n",
    "\n",
    "#fig_pca = px.scatter(data_frame=df_pca_MAP1,\n",
    "#                     x=\"PC1\",\n",
    "#                     y=\"PC3\",\n",
    "#                     color=\"Compartment\",\n",
    "#                     color_discrete_map=compartment_color,\n",
    "#                    title=\"Protein subcellular localization by PCA\",\n",
    "#                    hover_data=[\"Gene names\", \"Compartment\"], #\"Protein names\", \n",
    "#                    #custom_data=df_annotated_all.columns,\n",
    "#                    opacity=0.9\n",
    "#                    )\n",
    "#fig_pca  \n",
    "\n",
    "\n",
    "#i_class.df_pca.query(\"Compartment == \"ER\" or Compartment == \"Plasma membrane\"\")#\"& Compartment == \"Plasma memebrane\"\")  and \n",
    "\n",
    "#json_dict = ast.literal_eval(i_jsonFile.value.decode(\"UTF-8\"))\n",
    "\n",
    "\n",
    "\n",
    "#    @pn.depends(i_SVM_table.param.value, cache_run.param.value)\n",
    "#def read_SVM_matrix(SVM_table, run):   \n",
    "#    try:\n",
    "#        if run == True:\n",
    "#            if SVM_table == \"\":\n",
    "#                SVM_reading_status = \"No misclassification matrix is uploaded\"\n",
    "#                cache_uploaded_SVM.value = False\n",
    "#            else:\n",
    "#                cache_uploaded_SVM.value = False\n",
    "#                try:\n",
    "#                    i_class.df_SVM = pd.read_table(StringIO(SVM_table), sep=\"\\t\")\n",
    "#                    SVM_reading_status = i_class.df_SVM\n",
    "#                    cache_uploaded_SVM.value = True\n",
    "#                    return pn.Column(pn.Pane(SVM_reading_status, width=60*SVM_reading_status.shape[1]),\n",
    "#                                     pn.Row(button_SVM_analysis))\n",
    "#                except: \n",
    "#                    SVM_reading_status = traceback.format_exc()\n",
    "#                    cache_uploaded_SVM.value = False\n",
    "#                    return SVM_reading_status \n",
    "#        else:\n",
    "#            SVM_reading_status = \"Please, upload a file first and press ‘Analyse clusters’\"\n",
    "#            return SVM_reading_status\n",
    "#    except:\n",
    "#        SVM_reading_status = traceback.format_exc()\n",
    "#        return SVM_reading_status#        \n",
    "\n",
    "\n",
    "#from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\n",
    "#from matplotlib_venn import venn3, venn3_circles\n",
    "#from matplotlib import pyplot as plt\n",
    "#Course1=[\"A\", \"B\",\"C\", \"D\", \"E\", \"F\", \"G\", \"I\", \"P\", \"Q\"]\n",
    "#Course2=[\"B\", \"E\", \"F\", \"H\",\"K\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"Z\"]\n",
    "#Course3=[\"C\", \"E\", \"G\", \"H\", \"J\", \"K\", \"O\", \"Q\", \"Z\"]\n",
    "#\n",
    "#vd3=venn3([set(Course1),set(Course2),set(Course3)],\n",
    "# set_labels=(\"Course1\", \"Course2\",\"Course3\"),\n",
    "# set_colors=(), \n",
    "# alpha = 0.8)\n",
    "#venn3_circles([set(Course1), set(Course2),set(Course3)], linestyle=\"-.\", linewidth=2, color=\"grey\")\n",
    "#for text in vd3.set_labels:\n",
    "#    text.set_fontsize(16);\n",
    "#for text in vd3.subset_labels:\n",
    "#    text.set_fontsize(16)\n",
    "##plt.title(‘Venn Diagram for 3 courses’,fontname=’Times New Roman’,fontweight=’bold’,fontsize=20,\n",
    "## pad=30,backgroundcolor=’#cbe7e3\",color=’black’,style=’italic’);\n",
    "#plt.show()\n",
    "\n",
    "#                    data_completeness: series, for each individual map, as well as combined maps: 1 - (percentage of NANs)\n",
    "\n",
    "            ##profile completness - percentage of valid profiles (profiles that do not contain NaN)\n",
    "            #df_profile_completeness = df_01_stacked[\"normalized profile\"].xs(self.fractions[0], level=\"Fraction\", axis=0).unstack([\"Map\"])\n",
    "            #profile_completeness = 1-df_profile_completeness.apply(np.isnan).apply(sum)/len(df_profile_completeness)\n",
    "            #profile_completeness = profile_completeness.append(pd.Series(profile_completeness.mean(), index=[\"Combined Maps\"]))\n",
    "            #profile_completeness.rename(\"Profile completeness\", inplace=True)\n",
    "            #\n",
    "            #df_completeness = pd.concat([self.data_completeness, profile_completeness], axis=1)\n",
    "            #df_completeness.index.name = \"Map\"\n",
    "            #self.analysis_summary_dict[\"Data/Profile Completeness\"] = df_completeness.reset_index().to_json()\n",
    "            \n",
    "            \n",
    "            \n",
    "                        ##profile completness - percentage of valid profiles (profiles that do not contain NaN)\n",
    "            #df_profile_completeness = df_01_stacked[\"normalized profile\"].xs(self.fractions[0], level=\"Fraction\", axis=0).unstack([\"Map\"])\n",
    "            #profile_completeness = 1-df_profile_completeness.apply(np.isnan).apply(sum)/len(df_profile_completeness)\n",
    "            #profile_completeness = profile_completeness.append(pd.Series(profile_completeness.mean(), index=[\"Combined Maps\"]))\n",
    "            #profile_completeness.rename(\"Profile completeness\", inplace=True)\n",
    "            #            \n",
    "            #df_completeness = pd.concat([self.data_completeness, profile_completeness], axis=1)\n",
    "            #df_completeness.index.name = \"Map\"\n",
    "            #self.analysis_summary_dict[\"Data/Profile Completeness\"] = df_completeness.reset_index().to_json()\n",
    "            #\n",
    "#df_completeness_combined : df, with information about Data/Profile Completeness, index: \"Experiment\", \"Map\", \n",
    "#                                       column names: \"Data completeness\", \"Profile completeness\"\n",
    "\n",
    "\n",
    "#                    data_completeness : series, for each individual map, as well as combined maps: 1 - (percentage of NANs)\n",
    "#            #data completness - percentage of NANs\n",
    "#            if self.acquisition == \"SILAC\":\n",
    "#                df_data_completeness = df_index[\"Ratio H/L\"].stack([\"Fraction\"])\n",
    "#            elif self.acquisition == \"LFQ\":\n",
    "#                df_data_completeness = df_index[\"LFQ intensity\"].stack([\"Fraction\"])\n",
    "#                \n",
    "#            data_completeness = 1-df_data_completeness.apply(np.isnan).apply(sum)/len(df_data_completeness)\n",
    "#            data_completeness = data_completeness.append(pd.Series(data_completeness.mean(), index=[\"Combined Maps\"]))\n",
    "#            data_completeness.rename(\"Data completeness\", inplace=True)\n",
    "#            self.data_completeness = data_completeness \n",
    "\n",
    "\n",
    "\n",
    "#                    data_completeness : series, for each individual map, as well as combined maps: 1 - (percentage of NANs)\n",
    "            \n",
    "#            #data completness - percentage of NANs\n",
    "#            df_data_completeness = df_index[\"LFQ intensity\"].stack([\"Fraction\"])\n",
    "#            data_completeness = 1-df_data_completeness.apply(np.isnan).apply(sum)/len(df_data_completeness)\n",
    "#            data_completeness = data_completeness.append(pd.Series(data_completeness.mean(), index=[\"Combined Maps\"]))\n",
    "#            data_completeness.rename(\"Data completeness\", inplace=True)\n",
    "#            self.data_completeness = data_completeness\n",
    "\n",
    "\n",
    "                \n",
    "#                self:\n",
    "#                    analysis_summary_dict[\"Data/Profile Completeness\"] : df, with information about Data/Profile Completeness\n",
    "#                                        column: \"Experiment\" \t\"Map\" \t\"Data completeness\" \t\"Profile completeness\"\n",
    "#                                        no row index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
