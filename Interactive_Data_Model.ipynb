{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import re\n",
    "import traceback\n",
    "import panel as pn\n",
    "pn.extension(\"plotly\")\n",
    "import io\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "from bokeh.models.widgets.tables import NumberFormatter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise as pw\n",
    "import json\n",
    "import statistics \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_venn as venn\n",
    "from matplotlib_venn import venn2, venn3, venn3_circles\n",
    "from PIL import Image\n",
    "from upsetplot import from_memberships\n",
    "from upsetplot import plot\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec(open(\"class.py\").read())\n",
    "\n",
    "i_file = pn.widgets.FileInput(name=\"Upload file\")\n",
    "\n",
    "loading_status = pn.Row()\n",
    "idle = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "\n",
    "button_analysis = pn.widgets.Button(name=\"Analyse clusters\", width=50)\n",
    "\n",
    "i_logOR01_selection = pn.widgets.Select(options=[\"0/1 normalized data\", \"log transformed data\", \"stringency filtered raw data\"], name=\"Select type of data for download\", width=300)\n",
    "\n",
    "i_acquisition = pn.widgets.Select(options=[\"LFQ6 - Spectronaut\", \"LFQ5 - Spectronaut\", \"LFQ6 - MQ\", \"LFQ5 - MQ\",  \"SILAC - MQ\"], name=\"Acquisition\", width=300)\n",
    "i_organism = pn.widgets.Select(options=list(SpatialDataSet.markerproteins_set.keys()), name=\"Organism\", width=300) #\n",
    "\n",
    "\n",
    "i_comment = pn.widgets.input.TextAreaInput(name=\"Additional Comments\", placeholder=\"Write any kind of information assoiciated with this dataset here...\")\n",
    "\n",
    "i_clusterwidget = pn.widgets.Select(options=[\"Proteasome\", \"Lysosome\"], name=\"Cluster of interest\", width=300)\n",
    "i_mapwidget = pn.widgets.Select(options=[\"Map1\", \"Map2\"], name=\"Map of interest\", width=300)\n",
    "\n",
    "i_collapse_maps_PCA = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "\n",
    "cache_uploaded = pn.widgets.Checkbox(value=False)\n",
    "\n",
    "\n",
    "cache_run = pn.widgets.Checkbox(value=False)\n",
    "\n",
    "analysis_status = pn.Pane(\"No analysis run yet\", width=1000)\n",
    "filereading_status = pn.Pane(\"No data import yet\", width=1000)\n",
    "\n",
    "i_expname = pn.widgets.TextInput(name=\"Experiment Name\", placeholder=\"Enter your experiment name here here...\")\n",
    "\n",
    "i_consecutiveLFQi = pn.widgets.IntSlider(name=\"Consecutive LFQ intensities\", start=1, end=10, step=1, value=4)\n",
    "i_summed_MSMS_counts = pn.widgets.IntSlider(name=\"Summed MS/MS counts ≥ 2n x number of fractions\", start=1, end=10, step=1, value=2)\n",
    "\n",
    "i_RatioHLcount = pn.widgets.IntSlider(name=\"Quantification events (Ratio H/L Count)\", start=1, end=10, step=1, value=2)\n",
    "i_RatioVariability = pn.widgets.IntSlider(name=\"Ratio H/L variability [%]\", start=0, end=100, step=5, value=30)\n",
    "\n",
    "i_name_pattern = pn.widgets.Select(name=\"Name pattern\",options=[\"(?P<rep>.*)_(?P<frac>.*)\", \".* (?P<cond>.*)_(?P<rep>.*)_(?P<frac>.*)\",\".* (?P<rep>.*)_(?P<frac>.*)\",\n",
    "                                                                \".* (?P<cond>.*)_(?P<frac>.*)_(?P<rep>.*)\", \".* (?P<cond>.*)_(?P<frac>.*_.*)_(?P<rep>.*)\", \"Custom\"])\n",
    "i_custom_namepattern = pn.widgets.TextInput(name=\"Customized Name Pattern\", placeholder=\"Enter a string here...e.g.: .* (?P<rep>.*)_(?P<frac>.*)\")\n",
    "regex_pattern = {\n",
    "    \"(?P<rep>.*)_(?P<frac>.*)\" : [\"Spectronaut MAP1_03K\"],\n",
    "    \".* (?P<rep>.*)_(?P<frac>.*)\" : [\"MAP1_03K\",\"MAP3_03K\"],\n",
    "    \".* (?P<cond>.*)_(?P<rep>.*)_(?P<frac>.*)\" : [\"EGF_rep1_06K\",\"EGF_rep3_06K\"],\n",
    "    \".* (?P<cond>.*)_(?P<frac>.*)_(?P<rep>.*)\" : [\"Control_Mem_1\", \"Control_Cyt_1\"],\n",
    "    \".* (?P<cond>.*)_(?P<frac>.*_.*)_(?P<rep>.*)\" : [\"4h_mem_frac3_1\", \"co_mem_frac2_2\"]\n",
    "    }\n",
    "i_pattern_examples = pn.widgets.Select(name = \"Examples\", options=regex_pattern[i_name_pattern.value])\n",
    "\n",
    "@pn.depends(i_name_pattern.param.value, i_custom_namepattern, i_pattern_examples)\n",
    "def custimization(name_pattern, custom_namepattern, pattern_examples):\n",
    "    if name_pattern == \"Custom\":\n",
    "        return i_custom_namepattern\n",
    "    else:\n",
    "        example_for_name_pattern = regex_pattern[name_pattern]\n",
    "        i_pattern_examples.options = example_for_name_pattern\n",
    "        return i_pattern_examples\n",
    "\n",
    "i_x_vs_yAxis_PCA = {\n",
    "    \"PC1\" : [\"PC3\", \"PC2\"],\n",
    "    \"PC2\" : [\"PC1\", \"PC3\"],\n",
    "    \"PC3\" : [\"PC1\", \"PC2\"],\n",
    "    }\n",
    "\n",
    "i_xAxis_PCA = pn.widgets.Select(name=\"X-Axis\", options=[\"PC1\", \"PC2\",\"PC3\"])\n",
    "i_yAxis_PCA = pn.widgets.Select(name=\"Y-Axis\", options=i_x_vs_yAxis_PCA[i_xAxis_PCA.value])\n",
    "\n",
    "i_xAxis_PCA_comp = pn.widgets.Select(name=\"X-Axis\", options=[\"PC1\", \"PC2\",\"PC3\"])\n",
    "i_yAxis_PCA_comp = pn.widgets.Select(name=\"Y-Axis\", options=i_x_vs_yAxis_PCA[i_xAxis_PCA_comp.value])\n",
    "\n",
    "@pn.depends(i_xAxis_PCA_comp.param.value, watch=True)\n",
    "def custimization_PCA_comp(xAxis_PCA_comp):\n",
    "    yAxis_PCA_comp = i_x_vs_yAxis_PCA[xAxis_PCA_comp]\n",
    "    i_yAxis_PCA_comp.options = yAxis_PCA_comp\n",
    "    #return i_yAxis_PCA_comp\n",
    "\n",
    "@pn.depends(i_xAxis_PCA.param.value, watch=True)\n",
    "def custimization_PCA(xAxis_PCA):\n",
    "    yAxis_PCA = i_x_vs_yAxis_PCA[xAxis_PCA]\n",
    "    i_yAxis_PCA.options = yAxis_PCA\n",
    "    #return i_yAxis_PCA\n",
    "\n",
    "@pn.depends(i_acquisition.param.value, i_consecutiveLFQi, i_summed_MSMS_counts)\n",
    "def acquisition_response(acquisition, consecutiveLFQi, summed_MSMS_counts):\n",
    "    if acquisition == \"SILAC - MQ\":\n",
    "        return pn.Column(pn.Row(i_name_pattern, custimization), pn.Row(pn.Pane(\"Stringency filtering\")), pn.Row(i_RatioHLcount, i_RatioVariability))\n",
    "    else:\n",
    "        return pn.Column(pn.Row(pn.Row(i_name_pattern, custimization)), pn.Row(pn.Pane(\"Stringency filtering\")), pn.Row(i_consecutiveLFQi, i_summed_MSMS_counts))\n",
    "\n",
    "#define widgets that should be disbled after run==True\n",
    "wdgts = [i_acquisition,i_name_pattern,i_expname, i_pattern_examples, button_analysis, i_expname, i_organism, i_consecutiveLFQi, i_summed_MSMS_counts, \n",
    "         i_RatioHLcount, i_RatioVariability, i_comment]            \n",
    "\n",
    "@pn.depends(i_file.param.value)\n",
    "def read_file(file):\n",
    "    if file is None:\n",
    "        filereading_status = \"No file is uploaded\"\n",
    "        cache_uploaded.value = False\n",
    "        return filereading_status\n",
    "    else:\n",
    "        cache_uploaded.value = False\n",
    "        try:\n",
    "            if i_file.filename[-3:] == \"xls\" or i_file.filename[-3:] == \"txt\":\n",
    "                df_original = pd.read_csv(BytesIO(file), sep=\"\\t\", comment=\"#\", nrows=5, usecols=lambda x: bool(re.match(SpatialDataSet.regex[\"imported_columns\"], x)), low_memory=True)\n",
    "            elif i_file.filename[-3:] == \"csv\":\n",
    "                df_original = pd.read_csv(BytesIO(file), sep=\",\", comment=\"#\", nrows=5, usecols=lambda x: bool(re.match(SpatialDataSet.regex[\"imported_columns\"], x)), low_memory=True)\n",
    "            else:\n",
    "                return pn.Column(\"Upload either csv, xls, or txt formatted files.\")\n",
    "            cache_uploaded.value = True\n",
    "            for wdgt in wdgts:\n",
    "                wdgt.disabled = False\n",
    "\n",
    "            return pn.Column(pn.Row(pn.widgets.DataFrame(df_original, height=200, width=600, disabled=True)),#i_class.\n",
    "                             pn.Row(i_expname), \n",
    "                             pn.Row(i_organism, i_acquisition), \n",
    "                             pn.Row(acquisition_response), \n",
    "                             pn.Row(i_comment),\n",
    "                             pn.Row(button_analysis))\n",
    "\n",
    "        except Exception: \n",
    "            filereading_status = traceback.format_exc()\n",
    "            cache_uploaded.value = False\n",
    "            return filereading_status   \n",
    "        \n",
    "        \n",
    "def execution(event):\n",
    "    if cache_uploaded.value == False:\n",
    "        analysis_status.object = \"Please upload a file first\"\n",
    "    elif i_expname.value == \"\":\n",
    "        analysis_status.object = \"Please enter an experiment name first\"\n",
    "    else:        \n",
    "        dashboard_analysis.objects = []\n",
    "        cache_run.value = False\n",
    "        for wdgt in wdgts:\n",
    "            wdgt.disabled = True\n",
    "        #if you did already your comparison, but add another experiment afterwards - without reloading your AnylsedDatasets.json\n",
    "        #for wdgt in wdgts_comparison:\n",
    "        #    wdgt.disabled = True\n",
    "        try:\n",
    "            \n",
    "            dashboard_analysis.append(i_clusterwidget)\n",
    "            dashboard_analysis.append(i_mapwidget)\n",
    "            dashboard_analysis.append(analysis_tabs)\n",
    "            loading_status.objects = [loading]\n",
    "            analysis_status.object = \"Analysis in progress\"\n",
    "            if i_name_pattern.value == \"Custom\":\n",
    "                namePattern = i_custom_namepattern.value\n",
    "            else:\n",
    "                namePattern = i_name_pattern.value\n",
    "            global i_class\n",
    "            if i_acquisition.value == \"SILAC\":\n",
    "                i_class = SpatialDataSet(i_file.filename, i_expname.value, i_acquisition.value, comment=i_comment.value, name_pattern=namePattern, organism=i_organism.value, \n",
    "                                         RatioHLcount=i_RatioHLcount.value, RatioVariability=i_RatioVariability.value)\n",
    "            else:\n",
    "                i_class = SpatialDataSet(i_file.filename, i_expname.value, i_acquisition.value, comment=i_comment.value, name_pattern=namePattern, organism=i_organism.value,\n",
    "                                         consecutiveLFQi=i_consecutiveLFQi.value, summed_MSMS_counts=i_summed_MSMS_counts.value)\n",
    "            analysis_status.object = \"Data Reading\"\n",
    "            i_class.data_reading(content=BytesIO(i_file.value))\n",
    "            analysis_status.object = \"Data Processing\"\n",
    "            i_class.processingdf()\n",
    "            update_object_selector(i_mapwidget, i_clusterwidget)\n",
    "            i_class.quantity_profiles_proteinGroups()\n",
    "            analysis_status.object = \"PCA\"\n",
    "            i_class.perform_pca()\n",
    "            analysis_status.object = \"Calculating Manhattan Distance\"\n",
    "            i_class.multiple_iterations()\n",
    "            i_class.distance_calculation()\n",
    "            analysis_status.object = \"Calculating Dynamic Range\"\n",
    "            i_class.dynamic_range()\n",
    "            analysis_status.object = \"Writing Overview Table\"\n",
    "            i_class.results_overview_table()\n",
    "            analysis_status.object = \"Writing Analysed Dataset Dictionary\"\n",
    "            SpatialDataSetComparison.analysed_datasets_dict.update(i_class.analysed_datasets_dict)\n",
    "            loading_status.objects = [idle]\n",
    "            analysis_status.object = \"Analysis finished! Please open the 'Analysis' tab!\"\n",
    "            cache_run.value = True\n",
    "            #exc_info = sys.exc_info()\n",
    "        except:\n",
    "            for wdgt in wdgts:\n",
    "                wdgt.disabled = False\n",
    "            loading_status.objects = [\"\"]\n",
    "            #The traceback gives no traceback, so out of that there will be still the output: Analysis in progress, although it is not possible. Out of that i removed the traceback\n",
    "            analysis_status.object = traceback.format_exc()\n",
    "            cache_run.value = False\n",
    "\n",
    "button_analysis.on_click(execution)  \n",
    "\n",
    "def update_object_selector(i_mapwidget, i_clusterwidget):\n",
    "    i_mapwidget.options = list(i_class.map_names)\n",
    "    i_clusterwidget.options = list(i_class.markerproteins.keys())\n",
    "\n",
    "\n",
    "@pn.depends(i_mapwidget.param.value, cache_run.param.value, i_collapse_maps_PCA.param.value, i_clusterwidget.param.value, i_xAxis_PCA.param.value, i_yAxis_PCA.param.value)\n",
    "def update_data_overview(mapwidget, run, collapse_maps_PCA, clusterwidget, xAxis_PCA, yAxis_PCA):\n",
    "    try:\n",
    "        if run == True:\n",
    "            pca_plot = i_class.plot_global_pca(map_of_interest=mapwidget , cluster_of_interest=clusterwidget, x_PCA=xAxis_PCA, y_PCA=yAxis_PCA, collapse_maps=collapse_maps_PCA)\n",
    "            log_histogram = i_class.plot_log_data()\n",
    "            visualization_map = pn.Column(\n",
    "                    pn.Row(i_collapse_maps_PCA),\n",
    "                    pn.Row(pn.Pane(pca_plot, width=1000)),\n",
    "                    pn.Row(i_xAxis_PCA, i_yAxis_PCA),\n",
    "                    pn.Row(pn.Pane(log_histogram, width=1000))\n",
    "                    )\n",
    "            app_tabs.active = 1\n",
    "            return visualization_map\n",
    "        else:\n",
    "            visualization_map = \"Run analysis first!\"\n",
    "            return visualization_map\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "            \n",
    "\n",
    "@pn.depends(i_clusterwidget.param.value,i_mapwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_overview(clusterwidget, mapwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            list_genes = [goi for goi in i_class.genenames_sortedout_list if goi in i_class.markerproteins[clusterwidget]]\n",
    "            i_class.cache_cluster_quantified = True\n",
    "            distance_boxplot = i_class.distance_boxplot(cluster_of_interest=clusterwidget)\n",
    "            if i_class.cache_cluster_quantified == False:\n",
    "                return \"This protein cluster was not quantified\"\n",
    "            \n",
    "            else:\n",
    "                df_quantification_overview = i_class.quantification_overview(cluster_of_interest=clusterwidget)\n",
    "                profiles_plot = i_class.profiles_plot(map_of_interest = mapwidget, cluster_of_interest=clusterwidget)\n",
    "                pca_plot = i_class.plot_cluster_pca(cluster_of_interest=clusterwidget)\n",
    "                cluster_overview = pn.Column(\n",
    "                        pn.Row(pn.Pane(pca_plot, width=500),\n",
    "                               pn.Pane(distance_boxplot, width=500),\n",
    "                               pn.Pane(profiles_plot, width=500)),\n",
    "                        pn.Row(pn.Pane(\"In total {} proteins across all maps were quantified, whereas the following proteins were not consistently quantified throughout all maps: {}\".format(\n",
    "                                i_class.proteins_quantified_across_all_maps, \", \".join(list_genes)) if len(list_genes) != 0 else\n",
    "                            \"All genes from this cluster are quantified in all maps.\"), width=1000),\n",
    "                        pn.Row(pn.widgets.DataFrame(df_quantification_overview, height=200, width=500, disabled=True))  \n",
    "                        )\n",
    "                return cluster_overview\n",
    "        \n",
    "        else:\n",
    "            cluster_overview = \"Run analysis first!\"\n",
    "            return cluster_overview\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(i_clusterwidget.param.value, cache_run.param.value)\n",
    "def update_cluster_details(clusterwidget, run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            cluster_details = i_class.distance_to_median_boxplot(cluster_of_interest = clusterwidget)\n",
    "            return pn.Row(cluster_details, width=1000)\n",
    "        else:\n",
    "            cluster_details = \"Run analysis first!\"\n",
    "            return pn.Row(cluster_details, width=1000)\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_quantity(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            fig_npg, fig_npr, fig_npr_dc, fig_npg_F, fig_npgf_F, fig_npg_F_dc = i_class.plot_quantity_profiles_proteinGroups()\n",
    "            return pn.Column(\n",
    "                    pn.Row(pn.Column(fig_npg), pn.Column(fig_npr), pn.Column(fig_npr_dc)) ,\n",
    "                pn.Row(pn.Column(fig_npg_F), pn.Column(fig_npgf_F), pn.Column(fig_npg_F_dc)))\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def update_dynamic_range(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            fig_dynamic_range = i_class.plot_dynamic_range()\n",
    "            return pn.Row(fig_dynamic_range)\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "            \n",
    "    \n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def show_tabular_overview(run):\n",
    "    try:\n",
    "        if run == True:\n",
    "            content = pn.Column(\n",
    "                pn.widgets.DataFrame(pd.read_json(i_class.analysed_datasets_dict[i_expname.value][\"Overview table\"]), height=200, width=600, disabled=True),\n",
    "                pn.widgets.FileDownload(callback=table_download, filename=\"cluster_distances.csv\"),\n",
    "                i_logOR01_selection,\n",
    "                df01_download_widget,\n",
    "                pn.widgets.FileDownload(callback=json_download, filename=\"AnalysedDatasets.json\")\n",
    "            )\n",
    "            return pn.Row(content, width=1000)\n",
    "        else:\n",
    "            content = \"Please, upload a file first and press ‘Analyse clusters’\"\n",
    "            return pn.Row(content, width=1000)\n",
    "    except Exception:\n",
    "        content = traceback.format_exc()\n",
    "        return pn.Row(content, width=1000)\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def table_download(run):\n",
    "    df = i_class.results_overview_table()\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def json_download(run):\n",
    "    sio = StringIO()\n",
    "    json.dump(\n",
    "        SpatialDataSetComparison.analysed_datasets_dict, \n",
    "        sio, \n",
    "        indent=4, \n",
    "        sort_keys=True\n",
    "    )\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value, i_logOR01_selection.param.value)\n",
    "def df01_download_widget(run, logOR01_selection):\n",
    "    if logOR01_selection == \"0/1 normalized data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df01_download, filename = \"01_normalized_data.csv\"), width=650) \n",
    "    if logOR01_selection == \"log transformed data\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=dflog_download, filename = \"log_transformed_data.csv\"), width=650)\n",
    "    else:\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df_filteredRawData_download, filename = \"stringency_filtered_raw_data.csv\"), width=650)\n",
    "\n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def df01_download(run):\n",
    "    df_01 = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_01_stacked)\n",
    "    sio = StringIO()\n",
    "    df_01.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "    \n",
    "    \n",
    "@pn.depends(cache_run.param.value)\n",
    "def dflog_download(run):\n",
    "    df_log = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_log_stacked)\n",
    "    sio = StringIO()\n",
    "    df_log.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio \n",
    "\n",
    "@pn.depends(cache_run.param.value)\n",
    "def df_filteredRawData_download(run):\n",
    "    df = i_class.reframe_df_01ORlog_for_Perseus(i_class.df_stringencyFiltered)\n",
    "    sio = StringIO()\n",
    "    df.to_csv(sio)\n",
    "    sio.seek(0)\n",
    "    return sio\n",
    "\n",
    "\n",
    "loading_status_comparison = pn.Row()\n",
    "idle_comparison = pn.indicators.LoadingSpinner(value=False, width=100, height=100, color=\"primary\")\n",
    "loading_comparison = pn.indicators.LoadingSpinner(value=True, width=100, height=100, color=\"primary\")\n",
    "\n",
    "cache_uploaded_json = pn.widgets.Checkbox(value=False)\n",
    "cache_run_json = pn.widgets.Checkbox(value=False)\n",
    "button_comparison = pn.widgets.Button(name=\"Compare experiments\", width=50)\n",
    "i_jsonFile = pn.widgets.FileInput(name=\"Upload JSON file for comparison\")\n",
    "i_organism_comparison = pn.widgets.Select(options=list(SpatialDataSet.markerproteins_set.keys()), name=\"Organism\", width=300) #\n",
    "#i_clusterwidget_comparison = pn.widgets.Select(options=list(SpatialDataSet.markerproteins.keys()), name=\"Cluster of interest\", width=300)\n",
    "i_clusterwidget_comparison = pn.widgets.Select(options=[\"Proteasome\", \"Lysosome\"], name=\"Cluster of interest\", width=300)\n",
    "i_clusters_for_ranking = pn.widgets.CrossSelector(name=\"Select clusters to be considered for ranking calculation\", #value=list(SpatialDataSet.markerproteins.keys()), \n",
    "                                                options=[\"Proteasome\", \"Lysosome\"], size=8)\n",
    "i_multi_choice = pn.widgets.CrossSelector(name=\"Select experiments for comparison\", value=[\"a\", \"b\"], options=[\"a\", \"b\", \"c\"])\n",
    "i_ref_exp = pn.widgets.Select(name=\"Select experiments as reference\", options=[\"a\", \"b\", \"c\"])\n",
    "i_scatter_metric = pn.widgets.Select(name=\"Distance metric\",\n",
    "                                     options=[\"euclidean distance\", \"manhattan distance\",\n",
    "                                              \"1 - cosine correlation\", \"1 - pearson correlation\"])\n",
    "i_scatter_consolidation = pn.widgets.Select(name=\"Consolidation of replicate distances\",\n",
    "                                            options=[\"median\",\"average\",\"sum\"])\n",
    "\n",
    "dashboard_json = pn.Column(\"Please, upload a file first and press 'Compare clusters'\", name=\"Comparison\", css_classes=[\"content-width\"])\n",
    "comparison_status = pn.Pane(\"No datasets were compared yet\")\n",
    "i_collapse_maps = pn.widgets.Checkbox(value=False, name=\"Collapse maps\")\n",
    "i_collapse_cluster = pn.widgets.Checkbox(value=True, name=\"Collapse cluster\")\n",
    "i_markerset_or_cluster = pn.widgets.Checkbox(value=False, name=\"Display only protein clusters\")\n",
    "#i_ranking_boxPlot = pn.widgets.Checkbox(value=False, name=\"Display box plot\")\n",
    "i_ranking_boxPlot = pn.widgets.RadioBoxGroup(name=\"Types of ranking\", options=[\"Box plot\", \"Bar plot - median\", \"Bar plot - sum\"], inline=True)\n",
    "#i_toggle_sumORmedian = pn.widgets.Toggle(name=\"Sum or Median\", button_type=\"primary\")\n",
    "i_ExpOverview = pn.Row(pn.Pane(\"\", width=1000))\n",
    "i_include_dataset = pn.widgets.Checkbox(value=False, name=\"Include data analysed under 'Analysis' tab\")\n",
    "wdgts_comparison = [button_comparison,i_organism_comparison, i_include_dataset] \n",
    "json_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "@pn.depends(cache_run.param.value, i_jsonFile.param.value)#cache_run.param.value\n",
    "def open_jsonFile(run, jsonFile):#run\n",
    "    cache_run_json.value = False\n",
    "    if run == False and jsonFile is None:\n",
    "        filereading_status_json = \"No file is uploaded\"\n",
    "        cache_uploaded_json.value = False\n",
    "        return filereading_status_json\n",
    "    else:\n",
    "        try:\n",
    "            cache_uploaded_json.value = False\n",
    "            if run == True:\n",
    "                global json_dict\n",
    "                json_dict.update(SpatialDataSet.analysed_datasets_dict)\n",
    "            elif jsonFile is not None:\n",
    "                if i_include_dataset.value == False:\n",
    "                    json_dict = json.load(BytesIO(jsonFile))\n",
    "                else:\n",
    "                    #global json_dict\n",
    "                    json_dict.update(json.load(BytesIO(jsonFile))) #i_class.\n",
    "            try:\n",
    "                dashboard_comparison.objects = dashboard_comparison.objects[0:4]\n",
    "            except:\n",
    "                pass\n",
    "            if hasattr(json_dict, \"keys\") == False: #i_class.\n",
    "                return \"Your json-File does not fulfill the requirements\"\n",
    "            else:\n",
    "                i_multi_choice.options = []\n",
    "                filereading_status_json = list(json_dict.keys())# list(set(list(SpatialDataSet.analysed_datasets_dict.keys()) + )) #i_class.\n",
    "                cache_uploaded_json.value = True\n",
    "                for wdgt in wdgts_comparison:\n",
    "                    wdgt.disabled = False\n",
    "                return pn.Column(pn.Row(\"You will compare following experiments:\\n {}, and {}\".format(\", \".join(filereading_status_json[:-1]), filereading_status_json[-1])),\n",
    "                                 pn.Row(i_include_dataset),\n",
    "                                 pn.Row(i_organism_comparison),\n",
    "                                 pn.Row(button_comparison),\n",
    "                                 )\n",
    "        \n",
    "        except Exception: \n",
    "            filereading_status_json = traceback.format_exc()\n",
    "            cache_uploaded_json.value = False\n",
    "            return filereading_status_json                 \n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusterwidget_comparison.param.value, cache_run_json.param.value, i_xAxis_PCA_comp.param.value, i_yAxis_PCA_comp.param.value, \n",
    "            i_markerset_or_cluster.param.value)\n",
    "def update_visualization_map_comparison(multi_choice, clusterwidget_comparison, run_json, xAxis_PCA_comp, yAxis_PCA_comp, markerset_or_cluster):\n",
    "    try:\n",
    "        if run_json == True:\n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\")\n",
    "                                )\n",
    "            else:\n",
    "                pass\n",
    "            pca_global_comparison = i_class_comp.plot_global_pca_comparison(cluster_of_interest_comparison=clusterwidget_comparison, x_PCA=xAxis_PCA_comp, y_PCA=yAxis_PCA_comp, \n",
    "                                                                       markerset_or_cluster=markerset_or_cluster, multi_choice=multi_choice)\n",
    "            if markerset_or_cluster == False:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(i_clusterwidget_comparison),\n",
    "                                 pn.Row(i_markerset_or_cluster),\n",
    "                                 pn.Row(pca_global_comparison),\n",
    "                                 pn.Row(i_xAxis_PCA_comp, i_yAxis_PCA_comp)  \n",
    "                                )\n",
    "            else:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(i_markerset_or_cluster),\n",
    "                                 pn.Row(pca_global_comparison),\n",
    "                                 pn.Row(i_xAxis_PCA_comp, i_yAxis_PCA_comp)   \n",
    "                                )\n",
    "        else:\n",
    "            pca_global_comparison = \"Run analysis first!\"\n",
    "            return pca_global_comparison\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_clusterwidget_comparison.param.value, i_collapse_maps.param.value, i_collapse_cluster.param.value, i_clusters_for_ranking.param.value, i_ref_exp.param.value,\n",
    "            i_ranking_boxPlot.param.value, cache_run_json.param.value) #i_toggle_sumORmedian.param.value, toggle_sumORmedian\n",
    "def update_distance_and_pca(multi_choice, clusterwidget_comparison, collapse_maps, collapse_cluster, clusters_for_ranking, ref_exp, ranking_boxPlot, run_json):\n",
    "    try:\n",
    "        if run_json == True:\n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\")\n",
    "                                )\n",
    "            else:\n",
    "                pass\n",
    "            #i_ref_exp.options = multi_choice\n",
    "            update_ref_exp(i_ref_exp)\n",
    "            if clusters_for_ranking == []:\n",
    "                clusters_for_ranking = [clusterwidget_comparison]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            distance_ranking_comparison = i_class_comp.distance_ranking_barplot_comparison(collapse_cluster=collapse_cluster, multi_choice=multi_choice, \n",
    "                                                                                           clusters_for_ranking=clusters_for_ranking, ranking_boxPlot=ranking_boxPlot)\n",
    "            i_class_comp.cache_cluster_quantified = True\n",
    "            distance_comparison = i_class_comp.distance_boxplot_comparison(collapse_maps=collapse_maps, cluster_of_interest_comparison=clusterwidget_comparison, multi_choice=multi_choice)\n",
    "            if i_class_comp.cache_cluster_quantified == False:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(i_clusterwidget_comparison),\n",
    "                                 pn.Row(\"Cluster was not quantified in any experiment\")\n",
    "                                )\n",
    "            else:\n",
    "                pca_comparison = i_class_comp.plot_pca_comparison(collapse_maps=collapse_maps, cluster_of_interest_comparison=clusterwidget_comparison, multi_choice=multi_choice)\n",
    "                if clusters_for_ranking == []:\n",
    "                    return pn.Column(\n",
    "                                     pn.Row(i_clusters_for_ranking),\n",
    "                                     pn.Row(\"Select at least one cluster\"),         \n",
    "                                     pn.Row(i_clusterwidget_comparison),\n",
    "                                     pn.Row(i_collapse_maps),\n",
    "                                     pn.Row(pca_comparison),\n",
    "                                     pn.Row(distance_comparison),\n",
    "                                     )\n",
    "                else:\n",
    "                    if i_collapse_cluster.value == True:\n",
    "                        return pn.Column(\n",
    "                                     pn.Row(i_clusters_for_ranking),\n",
    "                                     pn.Row(i_collapse_cluster, i_ranking_boxPlot),\n",
    "                                     pn.Row(distance_ranking_comparison),\n",
    "                                     #pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster, height=200, width=1800, disabled=True)),\n",
    "                                     pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster2, height=200, width=1800, disabled=True)),\n",
    "                                     pn.Row(i_clusterwidget_comparison),\n",
    "                                     pn.Row(i_collapse_maps),\n",
    "                                     pn.Row(pca_comparison),\n",
    "                                     pn.Row(distance_comparison),\n",
    "\n",
    "                                    )\n",
    "                    else:\n",
    "                        return pn.Column(\n",
    "                                     pn.Row(i_clusters_for_ranking),\n",
    "                                     pn.Row(i_collapse_cluster),\n",
    "                                     pn.Row(distance_ranking_comparison),\n",
    "                                     #pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster, height=200, width=1800, disabled=True)), \n",
    "                                     pn.Row(pn.widgets.DataFrame(i_class_comp.df_quantified_cluster2, height=200, width=1800, disabled=True)),\n",
    "                                     pn.Row(i_clusterwidget_comparison),\n",
    "                                     pn.Row(i_collapse_maps),\n",
    "                                     pn.Row(pca_comparison),\n",
    "                                     pn.Row(distance_comparison),\n",
    "\n",
    "                                    )\n",
    "        else:\n",
    "            pca_comparison = \"Run analysis first!\"\n",
    "            return pca_comparison\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "    \n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_npr_ngg_nprDc(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                fig_quantity_pg, fig_quantity_pr = i_class_comp.quantity_pr_pg_barplot_comparison(multi_choice=multi_choice)\n",
    "                coverage_barplot = i_class_comp.coverage_comparison(multi_choice=multi_choice)\n",
    "                return pn.Column(\n",
    "                                 pn.Row(fig_quantity_pg), \n",
    "                                 pn.Row(fig_quantity_pr),\n",
    "                                 pn.Row(coverage_barplot)\n",
    "                                )\n",
    "        else:\n",
    "            completeness_barplot = \"Run analysis first!\"\n",
    "            return completeness_barplot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status \n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_venn(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            venn_plot = []\n",
    "            if len(multi_choice)<=1:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(pn.Pane(\"Please select 2 or more experiments for comparison\"), width=1000))\n",
    "            else:\n",
    "                venn_plot, figure_UpSetPlot = i_class_comp.venn_sections(multi_choice_venn = multi_choice)\n",
    "                return pn.Column(\n",
    "                                 pn.Pane(venn_plot),\n",
    "                                 pn.Row(figure_UpSetPlot,width=1000)\n",
    "                                )\n",
    "        else:\n",
    "            venn_plot = \"Run analysis first!\"\n",
    "            return venn_plot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status    \n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, cache_run_json.param.value)\n",
    "def update_SVM_Analysis(multi_choice, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                if i_class_comp.cache_stored_SVM == False:\n",
    "                    return pn.Column(\n",
    "                                 pn.Row(\"No Missclassifiaction Matrix is stored\"))\n",
    "                else:\n",
    "                    fig_markerPredictionAccuracy, fig_clusterPerformance = i_class_comp.svm_plotting(multi_choice)\n",
    "                    return pn.Column(\n",
    "                                 pn.Row(fig_markerPredictionAccuracy),\n",
    "                                 pn.Row(fig_clusterPerformance))\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status \n",
    "                \n",
    "@pn.depends(i_multi_choice.param.value, i_ref_exp.param.value, i_collapse_cluster.param.value, cache_run_json.param.value)\n",
    "def update_dynamic_range_comparison(multi_choice, ref_exp, collapse_cluster, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                dynamic_range_barplot = i_class_comp.dynamic_range_comparison(collapse_cluster=collapse_cluster, multi_choice=multi_choice, ref_exp=ref_exp)\n",
    "                return pn.Column(\n",
    "                                 pn.Row(dynamic_range_barplot),\n",
    "                                 pn.Row(i_collapse_cluster),\n",
    "                                 pn.Row(i_ref_exp)\n",
    "                                )\n",
    "        else:\n",
    "            dynamic_range_barplot = \"Run analysis first!\"\n",
    "            return dynamic_range_barplot\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, i_scatter_metric.param.value, i_scatter_consolidation.param.value, cache_run_json.param.value)\n",
    "def update_global_scatter_comparison(multi_choice, metric, consolidation, run_json):\n",
    "    try:\n",
    "        if run_json == True: \n",
    "            if multi_choice == []:\n",
    "                return pn.Column(#pn.Row(i_multi_choice),\n",
    "                                 pn.Row(\"Please select experiments for comparison\"))\n",
    "            else:\n",
    "                scatter_histogram = i_class_comp.calculate_global_scatter(multi_choice=multi_choice,\n",
    "                                                                              metric=metric, consolidation=consolidation)\n",
    "                return pn.Column(pn.Row(i_scatter_metric),\n",
    "                                 pn.Row(i_scatter_consolidation),\n",
    "                                 pn.Row(scatter_histogram)\n",
    "                                )\n",
    "        else:\n",
    "            return \"Run analysis first!\"\n",
    "    except Exception:\n",
    "        update_status = traceback.format_exc()\n",
    "        return update_status\n",
    "\n",
    "\n",
    "def update_multi_choice(i_multi_choice, i_clusterwidget, i_clusters_for_ranking):\n",
    "    i_multi_choice.options = list(json_dict.keys())\n",
    "    i_clusterwidget.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.options = list(i_class_comp.markerproteins.keys())\n",
    "    i_clusters_for_ranking.value = list(i_class_comp.markerproteins.keys())\n",
    "    i_multi_choice.value = list(json_dict.keys())\n",
    "\n",
    "    \n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ref_exp(multi_choice):\n",
    "    i_ref_exp.options = i_multi_choice.value\n",
    "    #return i_ref_exp\n",
    "\n",
    "\n",
    "@pn.depends(i_multi_choice.param.value, watch=True)\n",
    "def update_ExpOverview(multi_choice):\n",
    "    dict_analysis_parameters={}\n",
    "    for exp_name in multi_choice:\n",
    "        dict_analysis_parameters[exp_name] = json_dict[exp_name][\"Analysis parameters\"]\n",
    "    i_ExpOverview[0] = pn.widgets.DataFrame(pd.DataFrame.from_dict(dict_analysis_parameters))\n",
    "    i_ExpOverview.value = pd.DataFrame.from_dict(dict_analysis_parameters)\n",
    "    i_ExpOverview.disabled = True\n",
    "    i_ExpOverview.height = 200\n",
    "\n",
    "    \n",
    "def execution_comparison(event):\n",
    "    if cache_uploaded_json.value == False:\n",
    "        comparison_status.object = \"Please upload a JSON-file first\"\n",
    "    else:        \n",
    "        #dashboard_comparison.objects[2:] = []\n",
    "        cache_run_json.value = False\n",
    "        for wdgt in wdgts_comparison:\n",
    "            wdgt.disabled = True\n",
    "        try:\n",
    "            loading_status_comparison.objects = [loading_comparison]\n",
    "            comparison_status.object = \"Analysis in progress\"\n",
    "            protein_cluster = SpatialDataSet.markerproteins_set[i_organism_comparison.value].keys()\n",
    "            update_ref_exp(i_ref_exp)\n",
    "            global i_class_comp\n",
    "            comparison_status.object = \"Initialization\"\n",
    "            i_class_comp = SpatialDataSetComparison(clusters_for_ranking=protein_cluster, ref_exp=i_ref_exp.value, organism=i_organism_comparison.value)\n",
    "            i_class_comp.json_dict = json_dict\n",
    "            comparison_status.object = \"Reading\"\n",
    "            i_class_comp.read_jsonFile()\n",
    "            update_multi_choice(i_multi_choice, i_clusterwidget_comparison, i_clusters_for_ranking)\n",
    "            comparison_status.object = \"SVM Processing\"\n",
    "            i_class_comp.svm_processing()\n",
    "            comparison_status.object = \"PCA\"\n",
    "            i_class_comp.perform_pca_comparison()\n",
    "            dashboard_comparison.append(pn.Row(i_multi_choice))\n",
    "            dashboard_comparison.append(pn.Row(i_ExpOverview))\n",
    "            dashboard_comparison.append(comparison_tabs)\n",
    "            loading_status_comparison.objects = [idle_comparison]\n",
    "            comparison_status.object = \"Comparison finished!\"\n",
    "            cache_run_json.value = True\n",
    "        except Exception:\n",
    "            loading_status_comparison.objects = [\"\"]\n",
    "            for wdgt in wdgts_comparison:\n",
    "                wdgt.disabled = False\n",
    "            comparison_status.object = traceback.format_exc()\n",
    "            cache_run_json.value = False\n",
    "button_comparison.on_click(execution_comparison)\n",
    "\n",
    "\n",
    "i_jsonFile_amendments_intended = pn.widgets.FileInput(name=\"Upload JSON file to be amended\")\n",
    "i_json_ExpSelector = pn.widgets.CrossSelector(name=\"Select experiments, that will be removed from JSON file\", width=1000)\n",
    "cache_uploaded_json_amendment = pn.widgets.Checkbox(value=False)\n",
    "cache_run_json_amendment = pn.widgets.Checkbox(value=False)\n",
    "button_reset = pn.widgets.Button(name=\"Reset\", width=650)\n",
    "download_status = pn.Pane(\"Upload a JSON file first\", width=1000)\n",
    "i_df_ExpComment = pn.widgets.DataFrame()\n",
    "wdgt_json = [button_reset]\n",
    "json_dict_amendments_intended = {}\n",
    "#make a cache, and say, if this hasnt been executed so far, please reset it\n",
    "dict_new_expNames = {}\n",
    "dict_new_comments = {}\n",
    "i_exp_SVM = pn.widgets.Select(name=\"Select experiments as reference\", options=[\"a\", \"b\", \"c\"])\n",
    "button_SVM_analysis = pn.widgets.Button(name=\"Analyse misclassification matrix\", width=50)\n",
    "i_SVM_table = pn.widgets.input.TextAreaInput(name=\"Misclassification matrix from Perseus\", placeholder=\"Copy matrix here...\")\n",
    "cache_uploaded_SVM = pn.widgets.Checkbox(value=False)\n",
    "analysis_status_SVM = pn.Row(pn.Pane(\"No SVM analysis run yet\", width=1000))\n",
    "i_all_or_marker = pn.widgets.Select(name=\"Select dataset for Perseus\", options=[\"0/1 normalized data, all experiments\", \n",
    "                                                                               \"0/1 normalized data, markerset only, all experiments\"])\n",
    "    \n",
    "@pn.depends(i_jsonFile_amendments_intended.param.value)#cache_run.param.value\n",
    "def open_jsonFile_amendment(jsonFile_amendments):#run\n",
    "    cache_run_json_amendment.value = False\n",
    "    if jsonFile_amendments is None:\n",
    "        cache_uploaded_json_amendment.value = False\n",
    "    else:\n",
    "        dashboard_manageDatasets.objects[2:] = []\n",
    "        #dashboard_manageDatasets.objects = []\n",
    "        dashboard_MissclassificationMatrix.objects = []\n",
    "        dashboard_amendment.objects = []\n",
    "        cache_uploaded_json_amendment.value = False\n",
    "        try:\n",
    "            \n",
    "            json_dict_cache = json.load(BytesIO(i_jsonFile_amendments_intended.value))\n",
    "            if hasattr(json_dict_cache, \"keys\") == False: \n",
    "                    return \"Your json-File does not fulfill the requirements\"\n",
    "            else:\n",
    "                global json_dict_amendments_intended\n",
    "                try:\n",
    "                    json_dict_amendments_intended.update(json_dict_cache)\n",
    "                except Exception:\n",
    "                    json_dict_amendments_intended = json_dict_cache\n",
    "                i_json_ExpSelector.options = list(json_dict_amendments_intended.keys())\n",
    "                cache_uploaded_json_amendment.value = True\n",
    "                for wdgt in wdgt_json:\n",
    "                    wdgt.disabled = False\n",
    "                download_status.object = \"Upload successful! Select experiments now.\"\n",
    "                dashboard_manageDatasets.append(amendment_tabs)\n",
    "                dashboard_amendment.append(pn.Column(i_df_ExpComment, download_status))\n",
    "                analysis_status_SVM[0] = pn.Pane(\"Upload successful! Select experiments now.\")\n",
    "                dashboard_MissclassificationMatrix.append(analysis_status_SVM)\n",
    "                return pn.Column(\n",
    "                                 i_json_ExpSelector,\n",
    "                                 pn.widgets.FileDownload(callback=json_amendment_download, filename=\"AnalysedDatasets2.0.json\", width=650),\n",
    "                                 i_all_or_marker,           \n",
    "                                 df01_json_download_widget,#pn.Column(pn.widgets.FileDownload(callback=df01_fromJson_download, filename = \"all_01_normalized_data.csv\", width=650)),\n",
    "                                 button_reset,\n",
    "                                 )\n",
    "        except Exception: \n",
    "            filereading_status_json = traceback.format_exc()\n",
    "            cache_uploaded_json_amendment.value = False\n",
    "            return filereading_status_json\n",
    "\n",
    "    \n",
    "@pn.depends(i_exp_SVM.param.value, watch=True)\n",
    "def update_SVM_Matrix(exp_SVM):\n",
    "    try:\n",
    "        i_SVM_table.value = json_dict_amendments_intended[exp_SVM][\"Misclassification Matrix\"]\n",
    "    except:\n",
    "        i_SVM_table.value = ''\n",
    "    \n",
    "\n",
    "@pn.depends(i_json_ExpSelector.param.value, watch=True)\n",
    "def update_exp_for_SVM(json_ExpSelector):\n",
    "    if json_ExpSelector == []:\n",
    "        dashboard_MissclassificationMatrix.objects = []\n",
    "        analysis_status_SVM[0] = pn.Pane(\"Select experiments first\")\n",
    "        dashboard_MissclassificationMatrix.append(analysis_status_SVM)\n",
    "    else:\n",
    "        #i_SVM_table.value = \"\"       \n",
    "        dashboard_MissclassificationMatrix.objects = []\n",
    "        i_exp_SVM.options = json_ExpSelector\n",
    "        analysis_status_SVM[0] = pn.Pane(\"Please paste a SVM Matrix first\")\n",
    "        dashboard_MissclassificationMatrix.append(pn.Row(i_exp_SVM, i_SVM_table))\n",
    "        dashboard_MissclassificationMatrix.append(read_SVM_matrix)\n",
    "        dashboard_MissclassificationMatrix.append(analysis_status_SVM)\n",
    "    \n",
    "        \n",
    "@pn.depends(i_SVM_table.param.value)\n",
    "def read_SVM_matrix(SVM_table):   \n",
    "    if SVM_table == \"\":\n",
    "        SVM_reading_status = \"No misclassification matrix is uploaded\"\n",
    "        cache_uploaded_SVM.value = False\n",
    "        analysis_status_SVM[0] = pn.Pane(\"Please paste a SVM Matrix first\")\n",
    "    else:\n",
    "        cache_uploaded_SVM.value = False\n",
    "        try:\n",
    "            try:\n",
    "                df_SVM = pd.read_json(json_dict_amendments_intended[i_exp_SVM.value][\"Misclassification Matrix\"])\n",
    "            except KeyError:\n",
    "                df_SVM = pd.read_table(StringIO(SVM_table), sep=\"\\t\")\n",
    "                json_dict_amendments_intended[i_exp_SVM.value][\"Misclassification Matrix\"] = df_SVM.to_json()\n",
    "            #SVM_dict, SVM_dict_total = i_class.svm_processing()\n",
    "            \n",
    "                #\"Cluster performance\" : SVM_dict.copy(), \n",
    "            #                                                                                \"Total cluster performance\" : SVM_dict_total.copy()}\n",
    "            ##analysis_status_SVM[0] = pn.Pane(\"No SVM analysis run yet\")\n",
    "            SVM_reading_status = svm_heatmap(df_SVM)\n",
    "            cache_uploaded_SVM.value = True\n",
    "            #button_SVM_analysis.disabled = False\n",
    "            return pn.Column(#pn.Row(button_SVM_analysis),\n",
    "                             pn.Row(SVM_reading_status, height=600),\n",
    "                            )\n",
    "        except Exception: \n",
    "            SVM_reading_status = \"Paste the SVM matrix from Perseus only!\"\n",
    "            #SVM_reading_status = traceback.format_exc()\n",
    "            cache_uploaded_SVM.value = False\n",
    "            analysis_status_SVM[0] = pn.Pane(traceback.format_exc())    #pn.Pane(\"\")\n",
    "            return SVM_reading_status \n",
    "\n",
    "\n",
    "@pn.depends(cache_run_json_amendment.param.value)\n",
    "def df01_fromJson_download(run):\n",
    "    if i_json_ExpSelector.value == []:\n",
    "        download_status.object = \"No experiments are selected\"\n",
    "        return\n",
    "    else:\n",
    "        df = reframe_df_01_fromJson_for_Perseus(json_dict_amendments_intended)\n",
    "        sio = StringIO()\n",
    "        df.to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        return sio \n",
    "\n",
    "    \n",
    "@pn.depends(cache_run_json_amendment.param.value)\n",
    "def df01_marker_fromJson_download(run):\n",
    "    if i_json_ExpSelector.value == []:\n",
    "        download_status.object = \"No experiments are selected\"\n",
    "        return\n",
    "    else:\n",
    "        df = reframe_df_01_fromJson_for_Perseus(json_dict_amendments_intended)\n",
    "        df = df.loc[df.index.get_level_values(\"Compartment\")!= \"undefined\"]\n",
    "        sio = StringIO()\n",
    "        df.to_csv(sio)\n",
    "        sio.seek(0)\n",
    "        return sio \n",
    "    \n",
    "    \n",
    "@pn.depends(cache_run_json_amendment.param.value, i_all_or_marker.param.value)\n",
    "def df01_json_download_widget(run, all_or_marker):\n",
    "    if all_or_marker == \"0/1 normalized data, all experiments\":\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df01_fromJson_download, filename = \"all_01_normalized_data.csv\"), width=650) \n",
    "    else:\n",
    "        return pn.Column(pn.widgets.FileDownload(callback=df01_marker_fromJson_download, filename = \"marker_01_normalized_data.csv\"), width=650)\n",
    "    \n",
    "    \n",
    "def json_amendment_download():\n",
    "    if i_json_ExpSelector.value == []:\n",
    "        download_status.object = \"No experiments are selected\"\n",
    "        return\n",
    "    else:\n",
    "        json_new = json_dict_amendments_intended.copy()\n",
    "        exp_names_del = [elem for elem in i_json_ExpSelector.options if not elem in i_json_ExpSelector.value]        \n",
    "        for key in exp_names_del:\n",
    "            del json_new[key]\n",
    "        checked_exp = set()\n",
    "        redundant_expNames = set(new_Exp for new_Exp in dict_new_expNames.values() if new_Exp in checked_exp or checked_exp.add(new_Exp))\n",
    "        if redundant_expNames != set():\n",
    "            download_status.object = \"Experiments are not allowed to be labelled identically\"\n",
    "            return\n",
    "        else:\n",
    "            for exp_name in json_new:\n",
    "                json_new[exp_name][\"Analysis parameters\"][\"comment\"] = dict_new_comments[exp_name] #dict_new_expNames\n",
    "            json_new = {dict_new_expNames[oldK]: value for oldK, value in json_new.items()}\n",
    "            sio = StringIO()\n",
    "            json.dump(\n",
    "                json_new, \n",
    "                sio, \n",
    "                indent=4, \n",
    "                sort_keys=True\n",
    "            )\n",
    "            sio.seek(0)\n",
    "            download_status.object = \"Download sucessful\"\n",
    "            return sio\n",
    "\n",
    "        \n",
    "def reset_json_amendment(event):\n",
    "    global json_dict_amendments_intended\n",
    "    json_dict_amendments_intended = {}\n",
    "    i_json_ExpSelector.options = []\n",
    "    i_json_ExpSelector.value = []\n",
    "    i_df_ExpComment.value = pd.DataFrame()\n",
    "    for wdgt in wdgt_json:\n",
    "        wdgt.disabled = True\n",
    "    download_status.object = \"Reset sucessful\"\n",
    "button_reset.on_click(reset_json_amendment)\n",
    "\n",
    "\n",
    "@pn.depends(i_json_ExpSelector.param.value, watch=True)\n",
    "def update_renameExp(json_ExpSelector):\n",
    "    dict_ExpComments = {}\n",
    "    for exp_name in json_ExpSelector:\n",
    "        dict_ExpComments[exp_name] = json_dict_amendments_intended[exp_name][\"Analysis parameters\"][\"comment\"]\n",
    "    df_ExpComments = pd.DataFrame(dict_ExpComments.items(), columns=[\"Experiment name - old\", \"Comment\"])#pd.DataFrame.from_dict(dict_ExpComments)\n",
    "    df_ExpComments.insert(0, \"Experiment name - new\", df_ExpComments[\"Experiment name - old\"])\n",
    "    df_ExpComments.set_index(\"Experiment name - old\", inplace=True)\n",
    "    df_ExpComments.replace({\"Experiment name - new\": dict_new_expNames}, inplace=True)\n",
    "    exp_previous = list(dict_new_comments.keys())\n",
    "    for exp in exp_previous:\n",
    "        if exp not in json_ExpSelector:\n",
    "            del dict_new_comments[exp]\n",
    "    df_ExpComments.loc[dict_new_comments.keys(),\"Comment\"] = list(dict_new_comments.values())\n",
    "    i_df_ExpComment.value = df_ExpComments\n",
    "    i_df_ExpComment.height=len(json_ExpSelector)*50\n",
    "    #return i_df_ExpComment\n",
    "\n",
    "@pn.depends(i_df_ExpComment.param.value, watch=True)\n",
    "def update_newExpNames(df_ExpComment):\n",
    "    try:        \n",
    "        global dict_new_expNames \n",
    "        changed_expName = set(list(dict_new_expNames.values())+list(df_ExpComment[\"Experiment name - new\"]))-set(dict_new_expNames.values())\n",
    "        dict_new_expNames = dict(zip(df_ExpComment.index, df_ExpComment[\"Experiment name - new\"]))\n",
    "        global dict_new_comments\n",
    "        dict_new_comments_cache = dict_new_comments.copy()\n",
    "        changed_comment = set(list(dict_new_comments.values())+list(df_ExpComment[\"Comment\"]))-set(dict_new_comments.values())\n",
    "        dict_new_comments = dict(zip(df_ExpComment.index, df_ExpComment[\"Comment\"]))\n",
    "        if changed_expName == set() and changed_comment == set():\n",
    "            tracked_change = \"No changes saved\"\n",
    "        elif changed_expName != set() and changed_comment != set():\n",
    "            tracked_change = \"Upload sucessful\"\n",
    "        elif changed_expName != set():\n",
    "            new_exp = list(changed_expName)[0]\n",
    "            old_exp = list(dict_new_expNames.keys())[list(dict_new_expNames.values()).index(new_exp)]\n",
    "            tracked_change = \"Experiment name was changed from {} to {}\".format(old_exp, new_exp)\n",
    "        else:# changed_comment != set():\n",
    "            new_comment = list(changed_comment)[0]\n",
    "            exp = list(dict_new_comments.keys())[list(dict_new_comments.values()).index(new_comment)]\n",
    "            old_comment = dict_new_comments_cache[exp]\n",
    "            tracked_change = \"Comment of the experiment {} was changed from {} to {}\".format(exp, old_comment, new_comment)\n",
    "        download_status.object = tracked_change\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "dasboard_home = pn.Column(i_file, read_file,analysis_status, loading_status, name=\"Home\", css_classes=[\"content-width\"])\n",
    "dashboard_analysis = pn.Column(\"Please, upload a file first and press 'Analyse clusters'\", name=\"Analysis\", css_classes=[\"content-width\"])\n",
    "dashboard_MissclassificationMatrix = pn.Column(\"Please, upload a file first and press 'Analyse clusters'\", name=\"SVM Analysis\", css_classes=[\"content-width\"])\n",
    "dashboard_amendment = pn.Column(\"Please, upload a json file first\", name=\"Renaming\", css_classes=[\"content-width\"])\n",
    "dashboard_comparison = pn.Column(i_jsonFile, open_jsonFile, comparison_status, loading_status_comparison)\n",
    "dashboard_manageDatasets = pn.Column(i_jsonFile_amendments_intended, open_jsonFile_amendment)\n",
    "    \n",
    "analysis_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "analysis_tabs.append((\"Data overview\", update_data_overview))\n",
    "analysis_tabs.append((\"Cluster Overview\", update_cluster_overview))\n",
    "analysis_tabs.append((\"Cluster Details\", update_cluster_details))\n",
    "analysis_tabs.append((\"Depth and Coverage\", update_quantity))\n",
    "analysis_tabs.append((\"Dynamic Range\", update_dynamic_range))\n",
    "analysis_tabs.append((\"Download\", show_tabular_overview))\n",
    "\n",
    "comparison_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "comparison_tabs.append((\"Data Overview\", update_visualization_map_comparison))\n",
    "comparison_tabs.append((\"Global Scatter\", update_global_scatter_comparison))\n",
    "comparison_tabs.append((\"Biological Precision\", update_distance_and_pca))\n",
    "comparison_tabs.append((\"Depth and Coverage\", update_npr_ngg_nprDc))\n",
    "comparison_tabs.append((\"Unique and shared protein groups\", update_venn))\n",
    "comparison_tabs.append((\"Dynamic Range\", update_dynamic_range_comparison))\n",
    "comparison_tabs.append((\"SVM Analysis\", update_SVM_Analysis))\n",
    "\n",
    "amendment_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "amendment_tabs.append((\"Change Experiment name and comment\", dashboard_amendment))\n",
    "amendment_tabs.append((\"SVM Upload\", dashboard_MissclassificationMatrix))\n",
    "\n",
    "app_tabs = pn.Tabs(margin=10, css_classes=[\"content-width\"], dynamic=True)\n",
    "app_tabs.append((\"Home\", dasboard_home))\n",
    "app_tabs.append((\"Analysis\", dashboard_analysis))\n",
    "app_tabs.append((\"Data comparison\", dashboard_comparison))\n",
    "app_tabs.append((\"Manage Datasets\", dashboard_manageDatasets))\n",
    "\n",
    "app_tabs.append((\"About\", pn.Row(\"Explanation of what's going on here\", width=2000)))\n",
    "\n",
    "#i_search = pn.widgets.TextInput(name=\"Search\")\n",
    "app_center = pn.Column(pn.Row(pn.Pane(\"# QC tool for Spatial Proteomics\", width = 600),\n",
    "                              pn.layout.HSpacer(),\n",
    "                              #i_search,\n",
    "                              #width=1600, \n",
    "                              margin=10),\n",
    "                       app_tabs,\n",
    "                       #pn.Spacer(background=\"#DDDDDD\", height=100, margin=0)\n",
    "                      )\n",
    "app = pn.GridSpec()#sizing_mode=\"stretch_both\", margin=0)\n",
    "app[0,0] = pn.Spacer(background=\"white\", margin=0) #\"#DDDDDD\"\n",
    "app[0,9] = pn.Spacer(background=\"white\", margin=0) #\"#DDDDDD\"\n",
    "app[0,1:8] = app_center\n",
    "\n",
    "pwd = pn.widgets.PasswordInput(name=\"Please enter password for access.\")\n",
    "app_container = pn.Column(pwd)\n",
    "\n",
    "def check_pwd(event, app=app):\n",
    "    pwd = event.new\n",
    "    if pwd == \"pwd\":\n",
    "        app_container[0]=app\n",
    "pwd.param.watch(check_pwd, \"value\")\n",
    "\n",
    "try:\n",
    "    server.stop()\n",
    "except Exception:\n",
    "    print(\"First server startup\")\n",
    "server = app.show(port=5065, websocket_max_message_size=2000000000)\n",
    "#app.servable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
